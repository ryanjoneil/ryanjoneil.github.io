<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>üßê Data Fitting 1 - Linear Data Fitting | adventures in optimization</title>
<meta name="keywords" content="data fitting, classification, linear programming, python, scip, modeling">
<meta name="description" content="An introduction to data fitting and classification using linear optimization in Python">
<meta name="author" content="Ryan O&#39;Neil">
<link rel="canonical" href="https://ryanjoneil.github.io/posts/2010-11-23-data-fitting-1-linear-data-fitting/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.7c3f0ab5ecc8326dc20e6644afa4081b33304fef3299b2c1179eaee195843a6a.css" integrity="sha256-fD8KtezIMm3CDmZEr6QIGzMwT&#43;8ymbLBF56u4ZWEOmo=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://ryanjoneil.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://ryanjoneil.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://ryanjoneil.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://ryanjoneil.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://ryanjoneil.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link
    rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css"
    integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn"
    crossorigin="anonymous"/>
<script
    src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"
    integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx"
    crossorigin="anonymous">
</script>

<script
    src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"
    integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05"
    crossorigin="anonymous"
    onload="renderMathInElement(document.body);">
</script>

<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
        ]
    });
});
</script>

<meta property="og:title" content="üßê Data Fitting 1 - Linear Data Fitting" />
<meta property="og:description" content="An introduction to data fitting and classification using linear optimization in Python" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ryanjoneil.github.io/posts/2010-11-23-data-fitting-1-linear-data-fitting/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2010-11-23T00:00:00+00:00" />
<meta property="article:modified_time" content="2010-11-23T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="üßê Data Fitting 1 - Linear Data Fitting"/>
<meta name="twitter:description" content="An introduction to data fitting and classification using linear optimization in Python"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://ryanjoneil.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "üßê Data Fitting 1 - Linear Data Fitting",
      "item": "https://ryanjoneil.github.io/posts/2010-11-23-data-fitting-1-linear-data-fitting/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "üßê Data Fitting 1 - Linear Data Fitting",
  "name": "üßê Data Fitting 1 - Linear Data Fitting",
  "description": "An introduction to data fitting and classification using linear optimization in Python",
  "keywords": [
    "data fitting", "classification", "linear programming", "python", "scip", "modeling"
  ],
  "articleBody": "Note: This post was updated to work with Python 3 and PySCIPOpt. The original version used Python 2 and python-zibopt.\nData fitting is one of those tasks that everyone should have at least some exposure to. Certainly developers and analysts will benefit from a working knowledge of its fundamentals and their implementations. However, in my own reading I‚Äôve found it difficult to locate good examples that are simple enough to pick up quickly and come with accompanying source code.\nThis article commences an ongoing series introducing basic data fitting techniques. With any luck they won‚Äôt be overly complex, while still being useful enough to get the point across with a real example and real data. We‚Äôll start with a binary classification problem: presented with a series of records, each containing a set number of input values describing it, determine whether or not each record exhibits some property.\nModel We‚Äôll use the cancer1.dt data from the proben1 set of test cases, which you can download here. Each record starts with 9 data points containing physical characteristics of a tumor. The second to last data point contains 1 if a tumor is benign and 0 if it is malignant. We seek to find a linear function we can run on an arbitrary record that will return a value greater than zero if that record‚Äôs tumor is predicted to be benign and less than zero if it is predicted to be malignant. We will train our linear model on the first 350 records, and test it for accuracy on the remaining rows.\nThis is similar to the data fitting problem found in Chvatal. Our inputs consist of a matrix of observed data, $A$, and a vector of classifications, $b$. In order to classify a record, we require another vector $x$ such that the dot product of $x$ and that record will be either greater or less than zero depending on its predicted classification.\nA couple points to note before we start:\nMost observed data are noisy. This means it may be impossible to locate a hyperplane that cleanly separates given records of one type from another. In this case, we must resort to finding a function that minimizes our predictive error. For the purposes of this example, we‚Äôll minimize the sum of the absolute values of the observed and predicted values. That is, we seek $x$ such that we find $min \\sum_i{|a_i^T x-b_i|}$.\nThe slope-intercept form of a line, $f(x)=m^T x+b$, contains an offset. It should be obvious that this is necessary in our model so that our function isn‚Äôt required to pass through the origin. Thus, we‚Äôll be adding an extra variable with the coefficient of 1 to represent our offset value.\nIn order to model this, we use two linear constraints for each absolute value. We minimize the sum of these. Our Linear Programming model thus looks like:\n$$ \\begin{align*} \\min\\quad \u0026 z = x_0 + \\sum_i{v_i}\\\\ \\text{s.t.}\\quad\u0026 v_i \\geq x_0 + a_i^\\intercal x - 1 \u0026\\quad\\forall\u0026\\quad\\text{benign tumors}\\\\ \u0026 v_i \\geq 1 - x_0 - a_i^\\intercal x \u0026\\quad\\forall\u0026\\quad\\text{benign tumors}\\\\ \u0026 v_i \\geq x_0 + a_i^\\intercal x - (-1) \u0026\\quad\\forall\u0026\\quad\\text{malignant tumors}\\\\ \u0026 v_i \\geq -1 - x_0 - a_i^\\intercal x \u0026\\quad\\forall\u0026\\quad\\text{malignant tumors} \\end{align*} $$\nCode In order to do this in Python, we use SCIP and SoPlex. We start by setting constants for benign and malignant outputs and providing a function to read in the training and testing data sets.\n# Preferred output values for tumor categories BENIGN = 1 MALIGNANT = -1 def read_proben1_cancer_data(filename, train_size): '''Loads a proben1 cancer file into train \u0026 test sets''' # Number of input data points per record DATA_POINTS = 9 train_data = [] test_data = [] with open(filename) as infile: # Read in the first train_size lines to a training data list, and the # others to testing data. This allows us to test how general our model # is on something other than the input data. for line in infile.readlines()[7:]: # skip header line = line.split() # Records = offset (x0) + remaining data points input = [float(x) for x in line[:DATA_POINTS]] output = BENIGN if line[-2] == '1' else MALIGNANT record = {'input': input, 'output': output} # Determine what data set to put this in if len(train_data) \u003e= train_size: test_data.append(record) else: train_data.append(record) return train_data, test_data The next function implements the LP model described above using SoPlex and SCIP. It minimizes the sum of residuals for each training record. This amounts to summing the absolute value of the difference between predicted and observed output data. The following function takes in input and observed output data and returns a list of coefficients. Our resulting model consists of taking the dot product of an input record and these coefficients. If the result is greater than or equal to zero, that record is predicted to be a benign tumor, otherwise it is predicted to be malignant.\nfrom pyscipopt import Model def train_linear_model(train_data): ''' Accepts a set of input training data with known output values. Returns a list of coefficients to apply to arbitrary records for purposes of binary categorization. ''' # Make sure we have at least one training record. assert len(train_data) \u003e 0 num_variables = len(train_data[0]['input']) # Variables are coefficients in front of the data points. It is important # that these be unrestricted in sign so they can take negative values. m = Model() x = [m.addVar(f'x{i}', lb=None) for i in range(num_variables)] # Residual for each data row residuals = [m.addVar(lb=None, ub=None) for _ in train_data] for r, d in zip(residuals, train_data): # r will be the absolute value of the difference between observed and # predicted values. We can model absolute values such as r \u003e= |foo| as: # # r \u003e= foo # r \u003e= -foo m.addCons(sum(x * y for x, y in zip(x, d['input'])) + r \u003e= d['output']) m.addCons(sum(x * y for x, y in zip(x, d['input'])) - r \u003c= d['output']) # Find and return coefficients that min sum of residuals. m.setObjective(sum(residuals)) m.setMinimize() m.optimize() solution = m.getBestSol() return [solution[xi] for xi in x] We also provide a convenience function for counting the number of correct predictions by our resulting model against either the test or training data sets.\ndef count_correct(data_set, coefficients): '''Returns the number of correct predictions.''' correct = 0 for d in data_set: result = sum(x*y for x, y in zip(coefficients, d['input'])) # Do we predict the same as the output? if (result \u003e= 0) == (d['output'] \u003e= 0): correct += 1 return correct Finally we write a main method to read in the data, build our linear model, and test its efficacy.\nfrom pprint import pprint if __name__ == '__main__': # Specs for this input file INPUT_FILE_NAME = 'cancer1.dt' TRAIN_SIZE = 350 train_data, test_data = read_proben1_cancer_data( INPUT_FILE_NAME, TRAIN_SIZE ) # Add the offset variable to each of our data records for data_set in [train_data, test_data]: for row in data_set: row['input'] = [1] + row['input'] coefficients = train_linear_model(train_data) print('coefficients:') pprint(coefficients) # Print % of correct predictions for each data set correct = count_correct(train_data, coefficients) print( '%s / %s = %.02f%% correct on training set' % ( correct, len(train_data), 100 * float(correct) / len(train_data) ) ) correct = count_correct(test_data, coefficients) print( '%s / %s = %.02f%% correct on testing set' % ( correct, len(test_data), 100 * float(correct) / len(test_data) ) ) Results The result of running this model against the cancer1.dt data set is:\ncoefficients: [1.4072882449702786, -0.14014055927954652, -0.6239513714263405, -0.26727681774258882, 0.067107753841131157, -0.28300216102808429, -1.0355594670918404, -0.22774451038152174, -0.69871243677663608, -0.072575089848659444] 328 / 350 = 93.71% correct on training set 336 / 349 = 96.28% correct on testing set The accuracy is pretty good here against the both the training and testing sets, so this particular model generalizes well. This is about the simplest model we can implement for data fitting, and we‚Äôll get to more complicated ones later, but it‚Äôs nice to see we can do so well so quickly. The coefficients correspond to using a function of this form, rounding off to three decimal places:\n$$ \\begin{align*} f(x) =\\ \u00261.407 - 0.140 x_1 - 0.624 x_2 - 0.267 x_3 + 0.067 x_4 - \\\\ \u00260.283 x_5 - 1.037 x_6 - 0.228 x_7 - 0.699 x_8 - 0.073 x_9 \\end{align*} $$\nResources cancer1.dt data file from proben1 Full source listing ",
  "wordCount" : "1363",
  "inLanguage": "en",
  "datePublished": "2010-11-23T00:00:00Z",
  "dateModified": "2010-11-23T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Ryan O'Neil"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://ryanjoneil.github.io/posts/2010-11-23-data-fitting-1-linear-data-fitting/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "adventures in optimization",
    "logo": {
      "@type": "ImageObject",
      "url": "https://ryanjoneil.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://ryanjoneil.github.io/" accesskey="h" title="adventures in optimization (Alt + H)">adventures in optimization</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://ryanjoneil.github.io/about" title="about">
                    <span>about</span>
                </a>
            </li>
            <li>
                <a href="https://ryanjoneil.github.io/coding" title="coding">
                    <span>coding</span>
                </a>
            </li>
            <li>
                <a href="https://ryanjoneil.github.io/posts" title="posting">
                    <span>posting</span>
                </a>
            </li>
            <li>
                <a href="https://ryanjoneil.github.io/speaking" title="speaking">
                    <span>speaking</span>
                </a>
            </li>
            <li>
                <a href="https://ryanjoneil.github.io/writing" title="writing">
                    <span>writing</span>
                </a>
            </li>
            <li>
                <a href="https://ryanjoneil.github.io/search" title="üîç">
                    <span>üîç</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      üßê Data Fitting 1 - Linear Data Fitting
    </h1>
    <div class="post-description">
      An introduction to data fitting and classification using linear optimization in Python
    </div>
    <div class="post-meta"><span title='2010-11-23 00:00:00 +0000 UTC'>November 23, 2010</span>&nbsp;¬∑&nbsp;Ryan O&#39;Neil

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#model" aria-label="Model">Model</a></li>
                <li>
                    <a href="#code" aria-label="Code">Code</a></li>
                <li>
                    <a href="#results" aria-label="Results">Results</a></li>
                <li>
                    <a href="#resources" aria-label="Resources">Resources</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p><em>Note: This post was updated to work with Python 3 and <a href="https://github.com/scipopt/PySCIPOpt">PySCIPOpt</a>. The original version used Python 2 and <a href="https://pythonhosted.org/python-zibopt/">python-zibopt</a>.</em></p>
<p>Data fitting is one of those tasks that everyone should have at least some exposure to. Certainly developers and analysts will benefit from a working knowledge of its fundamentals and their implementations. However, in my own reading I&rsquo;ve found it difficult to locate good examples that are simple enough to pick up quickly and come with accompanying source code.</p>
<p>This article commences an ongoing series introducing basic data fitting techniques. With any luck they won&rsquo;t be overly complex, while still being useful enough to get the point across with a real example and real data. We&rsquo;ll start with a binary classification problem: presented with a series of records, each containing a set number of input values describing it, determine whether or not each record exhibits some property.</p>
<h2 id="model">Model<a hidden class="anchor" aria-hidden="true" href="#model">#</a></h2>
<p>We&rsquo;ll use the <code>cancer1.dt</code> data from the <code>proben1</code> set of test cases, which you can download <a href="/files/2010-11-23-data-fitting-1-linear-data-fitting/cancer1.dt">here</a>. Each record starts with 9 data points containing physical characteristics of a tumor. The second to last data point contains 1 if a tumor is benign and 0 if it is malignant. We seek to find a linear function we can run on an arbitrary record that will return a value greater than zero if that record&rsquo;s tumor is predicted to be benign and less than zero if it is predicted to be malignant. We will train our linear model on the first 350 records, and test it for accuracy on the remaining rows.</p>
<p>This is similar to the data fitting problem found in <a href="https://www.thriftbooks.com/w/linear-programming-series-of-books-in-the-mathematical-sciences_vasek-chvatal/249798/#edition=2416723&amp;idiq=15706498">Chvatal</a>. Our inputs consist of a matrix of observed data, $A$, and a vector of classifications, $b$. In order to classify a record, we require another vector $x$ such that the dot product of $x$ and that record will be either greater or less than zero depending on its predicted classification.</p>
<p>A couple points to note before we start:</p>
<ul>
<li>
<p>Most observed data are noisy. This means it may be impossible to locate a hyperplane that cleanly separates given records of one type from another. In this case, we must resort to finding a function that minimizes our predictive error. For the purposes of this example, we&rsquo;ll minimize the sum of the absolute values of the observed and predicted values. That is, we seek $x$ such that we find $min \sum_i{|a_i^T x-b_i|}$.</p>
</li>
<li>
<p>The <a href="https://www.purplemath.com/modules/strtlneq.htm">slope-intercept</a> form of a line, $f(x)=m^T x+b$, contains an offset. It should be obvious that this is necessary in our model so that our function isn&rsquo;t required to pass through the origin. Thus, we&rsquo;ll be adding an extra variable with the coefficient of 1 to represent our offset value.</p>
</li>
<li>
<p>In order to model this, we use two linear constraints for each absolute value. We minimize the sum of these. Our Linear Programming model thus looks like:</p>
</li>
</ul>
<p>$$
\begin{align*}
\min\quad       &amp; z = x_0 + \sum_i{v_i}\\
\text{s.t.}\quad&amp; v_i \geq x_0 + a_i^\intercal x - 1    &amp;\quad\forall&amp;\quad\text{benign tumors}\\
&amp; v_i \geq 1 - x_0 - a_i^\intercal x    &amp;\quad\forall&amp;\quad\text{benign tumors}\\
&amp; v_i \geq x_0 + a_i^\intercal x - (-1) &amp;\quad\forall&amp;\quad\text{malignant tumors}\\
&amp; v_i \geq -1 - x_0 - a_i^\intercal x   &amp;\quad\forall&amp;\quad\text{malignant tumors}
\end{align*}
$$</p>
<h2 id="code">Code<a hidden class="anchor" aria-hidden="true" href="#code">#</a></h2>
<p>In order to do this in Python, we use <a href="https://www.scipopt.org/">SCIP</a> and <a href="https://soplex.zib.de/">SoPlex</a>. We start by setting constants for benign and malignant outputs and providing a function to read in the training and testing data sets.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Preferred output values for tumor categories</span>
</span></span><span style="display:flex;"><span>BENIGN <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>MALIGNANT <span style="color:#f92672">=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">read_proben1_cancer_data</span>(filename, train_size):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;&#39;&#39;Loads a proben1 cancer file into train &amp; test sets&#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Number of input data points per record</span>
</span></span><span style="display:flex;"><span>    DATA_POINTS <span style="color:#f92672">=</span> <span style="color:#ae81ff">9</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    train_data <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    test_data <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> open(filename) <span style="color:#66d9ef">as</span> infile:
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Read in the first train_size lines to a training data list, and the</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># others to testing data. This allows us to test how general our model</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># is on something other than the input data.</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> line <span style="color:#f92672">in</span> infile<span style="color:#f92672">.</span>readlines()[<span style="color:#ae81ff">7</span>:]: <span style="color:#75715e"># skip header</span>
</span></span><span style="display:flex;"><span>            line <span style="color:#f92672">=</span> line<span style="color:#f92672">.</span>split()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Records = offset (x0) + remaining data points</span>
</span></span><span style="display:flex;"><span>            input <span style="color:#f92672">=</span> [float(x) <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> line[:DATA_POINTS]]
</span></span><span style="display:flex;"><span>            output <span style="color:#f92672">=</span> BENIGN <span style="color:#66d9ef">if</span> line[<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>] <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;1&#39;</span> <span style="color:#66d9ef">else</span> MALIGNANT
</span></span><span style="display:flex;"><span>            record <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#39;input&#39;</span>: input, <span style="color:#e6db74">&#39;output&#39;</span>: output}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Determine what data set to put this in</span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> len(train_data) <span style="color:#f92672">&gt;=</span> train_size:
</span></span><span style="display:flex;"><span>                test_data<span style="color:#f92672">.</span>append(record)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>                train_data<span style="color:#f92672">.</span>append(record)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> train_data, test_data
</span></span></code></pre></div><p>The next function implements the LP model described above using SoPlex and SCIP. It minimizes the sum of residuals for each training record. This amounts to summing the absolute value of the difference between predicted and observed output data. The following function takes in input and observed output data and returns a list of coefficients. Our resulting model consists of taking the <a href="https://en.wikipedia.org/wiki/Dot_product">dot product</a> of an input record and these coefficients. If the result is greater than or equal to zero, that record is predicted to be a benign tumor, otherwise it is predicted to be malignant.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> pyscipopt <span style="color:#f92672">import</span> Model
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">train_linear_model</span>(train_data):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;&#39;&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Accepts a set of input training data with known output
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    values.  Returns a list of coefficients to apply to
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    arbitrary records for purposes of binary categorization.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Make sure we have at least one training record.</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">assert</span> len(train_data) <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>    num_variables <span style="color:#f92672">=</span> len(train_data[<span style="color:#ae81ff">0</span>][<span style="color:#e6db74">&#39;input&#39;</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Variables are coefficients in front of the data points. It is important</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># that these be unrestricted in sign so they can take negative values.</span>
</span></span><span style="display:flex;"><span>    m <span style="color:#f92672">=</span> Model()
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> [m<span style="color:#f92672">.</span>addVar(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;x</span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>, lb<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>) <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(num_variables)]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Residual for each data row</span>
</span></span><span style="display:flex;"><span>    residuals <span style="color:#f92672">=</span> [m<span style="color:#f92672">.</span>addVar(lb<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>, ub<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>) <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> train_data]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> r, d <span style="color:#f92672">in</span> zip(residuals, train_data):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># r will be the absolute value of the difference between observed and</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># predicted values. We can model absolute values such as r &gt;= |foo| as:</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">#   r &gt;=  foo</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">#   r &gt;= -foo</span>
</span></span><span style="display:flex;"><span>        m<span style="color:#f92672">.</span>addCons(sum(x <span style="color:#f92672">*</span> y <span style="color:#66d9ef">for</span> x, y <span style="color:#f92672">in</span> zip(x, d[<span style="color:#e6db74">&#39;input&#39;</span>])) <span style="color:#f92672">+</span> r <span style="color:#f92672">&gt;=</span> d[<span style="color:#e6db74">&#39;output&#39;</span>])
</span></span><span style="display:flex;"><span>        m<span style="color:#f92672">.</span>addCons(sum(x <span style="color:#f92672">*</span> y <span style="color:#66d9ef">for</span> x, y <span style="color:#f92672">in</span> zip(x, d[<span style="color:#e6db74">&#39;input&#39;</span>])) <span style="color:#f92672">-</span> r <span style="color:#f92672">&lt;=</span> d[<span style="color:#e6db74">&#39;output&#39;</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Find and return coefficients that min sum of residuals.</span>
</span></span><span style="display:flex;"><span>    m<span style="color:#f92672">.</span>setObjective(sum(residuals))
</span></span><span style="display:flex;"><span>    m<span style="color:#f92672">.</span>setMinimize()
</span></span><span style="display:flex;"><span>    m<span style="color:#f92672">.</span>optimize()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    solution <span style="color:#f92672">=</span> m<span style="color:#f92672">.</span>getBestSol()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> [solution[xi] <span style="color:#66d9ef">for</span> xi <span style="color:#f92672">in</span> x]
</span></span></code></pre></div><p>We also provide a convenience function for counting the number of correct predictions by our resulting model against either the test or training data sets.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">count_correct</span>(data_set, coefficients):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;&#39;&#39;Returns the number of correct predictions.&#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>    correct <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> d <span style="color:#f92672">in</span> data_set:
</span></span><span style="display:flex;"><span>        result <span style="color:#f92672">=</span> sum(x<span style="color:#f92672">*</span>y <span style="color:#66d9ef">for</span> x, y <span style="color:#f92672">in</span> zip(coefficients, d[<span style="color:#e6db74">&#39;input&#39;</span>]))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Do we predict the same as the output?</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> (result <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">0</span>) <span style="color:#f92672">==</span> (d[<span style="color:#e6db74">&#39;output&#39;</span>] <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">0</span>):
</span></span><span style="display:flex;"><span>            correct <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> correct
</span></span></code></pre></div><p>Finally we write a main method to read in the data, build our linear model, and test its efficacy.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> pprint <span style="color:#f92672">import</span> pprint
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;__main__&#39;</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Specs for this input file</span>
</span></span><span style="display:flex;"><span>    INPUT_FILE_NAME <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;cancer1.dt&#39;</span>
</span></span><span style="display:flex;"><span>    TRAIN_SIZE <span style="color:#f92672">=</span> <span style="color:#ae81ff">350</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    train_data, test_data <span style="color:#f92672">=</span> read_proben1_cancer_data(
</span></span><span style="display:flex;"><span>        INPUT_FILE_NAME,
</span></span><span style="display:flex;"><span>        TRAIN_SIZE
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Add the offset variable to each of our data records</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> data_set <span style="color:#f92672">in</span> [train_data, test_data]:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> row <span style="color:#f92672">in</span> data_set:
</span></span><span style="display:flex;"><span>            row[<span style="color:#e6db74">&#39;input&#39;</span>] <span style="color:#f92672">=</span> [<span style="color:#ae81ff">1</span>] <span style="color:#f92672">+</span> row[<span style="color:#e6db74">&#39;input&#39;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    coefficients <span style="color:#f92672">=</span> train_linear_model(train_data)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#39;coefficients:&#39;</span>)
</span></span><span style="display:flex;"><span>    pprint(coefficients)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Print % of correct predictions for each data set</span>
</span></span><span style="display:flex;"><span>    correct <span style="color:#f92672">=</span> count_correct(train_data, coefficients)
</span></span><span style="display:flex;"><span>    print(
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#39;</span><span style="color:#e6db74">%s</span><span style="color:#e6db74"> / </span><span style="color:#e6db74">%s</span><span style="color:#e6db74"> = </span><span style="color:#e6db74">%.02f%%</span><span style="color:#e6db74"> correct on training set&#39;</span> <span style="color:#f92672">%</span> (
</span></span><span style="display:flex;"><span>            correct, len(train_data),
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">100</span> <span style="color:#f92672">*</span> float(correct) <span style="color:#f92672">/</span> len(train_data)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    correct <span style="color:#f92672">=</span> count_correct(test_data, coefficients)
</span></span><span style="display:flex;"><span>    print(
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#39;</span><span style="color:#e6db74">%s</span><span style="color:#e6db74"> / </span><span style="color:#e6db74">%s</span><span style="color:#e6db74"> = </span><span style="color:#e6db74">%.02f%%</span><span style="color:#e6db74"> correct on testing set&#39;</span> <span style="color:#f92672">%</span> (
</span></span><span style="display:flex;"><span>            correct, len(test_data),
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">100</span> <span style="color:#f92672">*</span> float(correct) <span style="color:#f92672">/</span> len(test_data)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>    )
</span></span></code></pre></div><h2 id="results">Results<a hidden class="anchor" aria-hidden="true" href="#results">#</a></h2>
<p>The result of running this model against the <code>cancer1.dt</code> data set is:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-txt" data-lang="txt"><span style="display:flex;"><span>coefficients:
</span></span><span style="display:flex;"><span>[1.4072882449702786,
</span></span><span style="display:flex;"><span> -0.14014055927954652,
</span></span><span style="display:flex;"><span> -0.6239513714263405,
</span></span><span style="display:flex;"><span> -0.26727681774258882,
</span></span><span style="display:flex;"><span> 0.067107753841131157,
</span></span><span style="display:flex;"><span> -0.28300216102808429,
</span></span><span style="display:flex;"><span> -1.0355594670918404,
</span></span><span style="display:flex;"><span> -0.22774451038152174,
</span></span><span style="display:flex;"><span> -0.69871243677663608,
</span></span><span style="display:flex;"><span> -0.072575089848659444]
</span></span><span style="display:flex;"><span>328 / 350 = 93.71% correct on training set
</span></span><span style="display:flex;"><span>336 / 349 = 96.28% correct on testing set
</span></span></code></pre></div><p>The accuracy is pretty good here against the both the training and testing sets, so this particular model generalizes well.  This is about the simplest model we can implement for data fitting, and we&rsquo;ll get to more complicated ones later, but it&rsquo;s nice to see we can do so well so quickly.  The coefficients correspond to using a function of this form, rounding off to three decimal places:</p>
<p>$$
\begin{align*}
f(x) =\ &amp;1.407 - 0.140 x_1 - 0.624 x_2 - 0.267 x_3 + 0.067 x_4 - \\
&amp;0.283 x_5 - 1.037 x_6 - 0.228 x_7 - 0.699 x_8 - 0.073 x_9
\end{align*}
$$</p>
<h2 id="resources">Resources<a hidden class="anchor" aria-hidden="true" href="#resources">#</a></h2>
<ul>
<li><a href="/files/2010-11-23-data-fitting-1-linear-data-fitting/cancer1.dt"><code>cancer1.dt</code></a> data file from <code>proben1</code></li>
<li>Full <a href="/files/2010-11-23-data-fitting-1-linear-data-fitting/fit-linear.py">source listing</a></li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://ryanjoneil.github.io/tags/data-fitting/">data fitting</a></li>
      <li><a href="https://ryanjoneil.github.io/tags/classification/">classification</a></li>
      <li><a href="https://ryanjoneil.github.io/tags/linear-programming/">linear programming</a></li>
      <li><a href="https://ryanjoneil.github.io/tags/python/">python</a></li>
      <li><a href="https://ryanjoneil.github.io/tags/scip/">scip</a></li>
      <li><a href="https://ryanjoneil.github.io/tags/modeling/">modeling</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://ryanjoneil.github.io/posts/2010-11-30-off-the-cuff-voter-fraud-detection/">
    <span class="title">¬´ Prev</span>
    <br>
    <span>üó≥ Off the Cuff Voter Fraud Detection</span>
  </a>
  <a class="next" href="https://ryanjoneil.github.io/posts/2009-10-08-monte-carlo-simulation-in-python/">
    <span class="title">Next ¬ª</span>
    <br>
    <span>üêç Monte Carlo Simulation in Python</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://ryanjoneil.github.io/">adventures in optimization</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
