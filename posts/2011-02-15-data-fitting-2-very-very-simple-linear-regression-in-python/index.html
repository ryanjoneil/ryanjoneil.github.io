<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>üßê Data Fitting 2 - Very, Very Simple Linear Regression in Python | adventures in optimization</title>
<meta name="keywords" content="data fitting, regression, python, modeling">
<meta name="description" content="Predict how much people like cats and dogs based on their ice cream preferences. Also, Python and numpy.">
<meta name="author" content="Ryan O&#39;Neil">
<link rel="canonical" href="https://ryanjoneil.github.io/posts/2011-02-15-data-fitting-2-very-very-simple-linear-regression-in-python/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.7c3f0ab5ecc8326dc20e6644afa4081b33304fef3299b2c1179eaee195843a6a.css" integrity="sha256-fD8KtezIMm3CDmZEr6QIGzMwT&#43;8ymbLBF56u4ZWEOmo=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://ryanjoneil.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://ryanjoneil.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://ryanjoneil.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://ryanjoneil.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://ryanjoneil.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link
    rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css"
    integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn"
    crossorigin="anonymous"/>
<script
    src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"
    integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx"
    crossorigin="anonymous">
</script>

<script
    src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"
    integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05"
    crossorigin="anonymous"
    onload="renderMathInElement(document.body);">
</script>

<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
        ]
    });
});
</script>

<meta property="og:title" content="üßê Data Fitting 2 - Very, Very Simple Linear Regression in Python" />
<meta property="og:description" content="Predict how much people like cats and dogs based on their ice cream preferences. Also, Python and numpy." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ryanjoneil.github.io/posts/2011-02-15-data-fitting-2-very-very-simple-linear-regression-in-python/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2011-02-15T00:00:00+00:00" />
<meta property="article:modified_time" content="2011-02-15T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="üßê Data Fitting 2 - Very, Very Simple Linear Regression in Python"/>
<meta name="twitter:description" content="Predict how much people like cats and dogs based on their ice cream preferences. Also, Python and numpy."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://ryanjoneil.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "üßê Data Fitting 2 - Very, Very Simple Linear Regression in Python",
      "item": "https://ryanjoneil.github.io/posts/2011-02-15-data-fitting-2-very-very-simple-linear-regression-in-python/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "üßê Data Fitting 2 - Very, Very Simple Linear Regression in Python",
  "name": "üßê Data Fitting 2 - Very, Very Simple Linear Regression in Python",
  "description": "Predict how much people like cats and dogs based on their ice cream preferences. Also, Python and numpy.",
  "keywords": [
    "data fitting", "regression", "python", "modeling"
  ],
  "articleBody": "This post is based on a memo I sent to some former colleagues at the Post. I‚Äôve edited it for use here since it fits well as the second in a series on simple data fitting techniques. If you‚Äôre among the many enlightened individuals already using regression analysis, then this post is probably not for you. If you aren‚Äôt, then hopefully this provides everything you need to develop rudimentary predictive models that yield surprising levels of accuracy.\nData For purposes of a simple working example, we have collected six records of input data over three dimensions with the goal of predicting two outputs. The input data are:\n$$ \\begin{align*} x_1 \u0026= \\text{How much a respondent likes vanilla [0-10]}\\\\ x_2 \u0026= \\text{How much a respondent likes strawberry [0-10]}\\\\ x_3 \u0026= \\text{How much a respondent likes chocolate [0-10]} \\end{align*} $$\nOutput data consist of:\n$$ \\begin{align*} b_1 \u0026= \\text{How much a respondent likes dogs [0-10]}\\\\ b_2 \u0026= \\text{How much a respondent likes cats [0-10]} \\end{align*} $$\nBelow are anonymous data collected from a random sample of people.\nrespondent vanilla ‚ù§Ô∏è strawberry ‚ù§Ô∏è chocolate ‚ù§Ô∏è dog ‚ù§Ô∏è cat ‚ù§Ô∏è Alyssa P Hacker 9 4 9 9 8 Ben Bitdiddle 8 6 4 10 4 Cy D. Fect 9 4 8 2 6 Eva Lu Ator 3 7 9 4 6 Lem E. Tweakit 6 8 5 2 5 Louis Reasoner 4 5 3 10 3 Our input is in three dimensions. Each output requires its own model, so we‚Äôll have one for dogs and one for cats. We‚Äôre looking for functions, dog(x) and cat(x), that can predict $b_1$ and $b_2$ based on given values of $x_1$, $x_2$, and $x_3$.\nModel 1 For both models we want to find parameters that minimize their squared residuals (read: errors). There‚Äôs a number of names for this. Optimization folks like to think of it as unconstrained quadratic optimization, but it‚Äôs more common to call it least squares or linear regression. It‚Äôs not necessary to entirely understand why for our purposes, but the function that minimizes these errors is:\n$$\\beta = ({A^t}A)^{-1}{A^t}b$$\nThis is implemented for you in the numpy.linalg Python package, which we‚Äôll use for examples. Much more information than you probably want can be found here.\nBelow is a first stab at a Python version. It runs least squares against our input and output data exactly as they are. You can see the matrix $A$ and outputs $b_1$ and $b_2$ (dog and cat love, respectively) are represented just as they are in the table.\n# Version 1: No offset, no squared inputs import numpy A = numpy.vstack([ [9, 4, 9], [8, 6, 4], [9, 4, 8], [3, 7, 9], [6, 8, 5], [4, 5, 3] ]) b1 = numpy.array([9, 10, 2, 4, 2, 10]) b2 = numpy.array([9, 4, 6, 6, 5, 3]) print('dog ‚ù§Ô∏è:', numpy.linalg.lstsq(A, b1, rcond=None)[0]) print('cat ‚ù§Ô∏è:', numpy.linalg.lstsq(A, b2, rcond=None)[0]) # Output: # dog ‚ù§Ô∏è: [0.72548294 0.53045642 -0.29952361] # cat ‚ù§Ô∏è: [2.36110929e-01 2.61934385e-05 6.26892476e-01] The resulting model is:\ndog(x) = 0.72548294 * x1 + 0.53045642 * x2 - 0.29952361 * x3 cat(x) = 2.36110929e-01 * x1 + 2.61934385e-05 * x2 + 6.26892476e-01 * x3 The coefficients before our variables correspond to beta in the formula above. Errors between observed and predicted data, shown below, are calculated and summed. For these six records, dog(x) has a total error of 20.76 and cat(x) has 3.74. Not great.\nrespondent predicted b1 b1 error predicted b2 b2 error Alyssa P Hacker 5.96 3.04 7.77 1.23 Ben Bitdiddle 7.79 2.21 4.40 0.40 Cy D. Fect 6.25 4.25 7.14 1.14 Eva Lu Ator 3.19 0.81 6.35 0.35 Lem E. Tweakit 7.10 5.10 4.55 0.45 Louis Reasoner 4.66 5.34 2.83 0.17 Total error: 20.76 3.74 Model 2 One problem with this model is that dog(x) and cat(x) are forced to pass through the origin. (Why is that?) We can improve it somewhat if we add an offset. This amounts to prepending 1 to every row in $A$ and adding a constant to the resulting functions. You can see the very slight difference between the code for this model and that of the previous:\n# Version 2: Offset, no squared inputs import numpy A = numpy.vstack([ [1, 9, 4, 9], [1, 8, 6, 4], [1, 9, 4, 8], [1, 3, 7, 9], [1, 6, 8, 5], [1, 4, 5, 3] ]) print('dog ‚ù§Ô∏è:', numpy.linalg.lstsq(A, b1, rcond=None)[0]) print('cat ‚ù§Ô∏è:', numpy.linalg.lstsq(A, b2, rcond=None)[0]) # Output: # dog ‚ù§Ô∏è: [20.92975427 -0.27831197 -1.43135684 -0.76469017] # cat ‚ù§Ô∏è: [-0.31744124 0.25133547 0.02978098 0.63394765] This yields the seconds version of our models:\ndog(x) = 20.92975427 - 0.27831197 * x1 - 1.43135684 * x2 - 0.76469017 * x3 cat(x) = -0.31744124 + 0.25133547 * x1 + 0.02978098 * x2 + 0.63394765 * x3 These models provide errors of 13.87 and 3.79. A little better on the dog side, but still not quite usable.\nrespondent predicted b1 b1 error predicted b2 b2 error Alyssa P Hacker 5.82 3.18 7.77 1.23 Ben Bitdiddle 7.06 2.94 4.41 0.41 Cy D. Fect 6.58 4.58 7.14 1.14 Eva Lu Ator 3.19 0.81 6.35 0.35 Lem E. Tweakit 3.99 1.99 4.60 0.40 Louis Reasoner 10.37 0.37 2.74 0.26 Total error: 13.87 3.79 Model 3 The problem is that dog(x) and cat(x) are linear functions. Most observed data don‚Äôt conform to straight lines. Take a moment and draw the line $f(x) = x$ and the curve $f(x) = x^2$. The former makes a poor approximation of the latter.\nMost of the time, people just use squares of the input data to add curvature to their models. We do this in our next version of the code by just adding squares of the input row values to our $A$ matrix. Everything else is the same. (In reality, you can add any function of the input data you feel best models the data, if you understand it well enough.)\n# Version 3: Offset with squared inputs import numpy A = numpy.vstack([ [1, 9, 9**2, 4, 4**2, 9, 9**2], [1, 8, 8**2, 6, 6**2, 4, 4**2], [1, 9, 9**2, 4, 4**2, 8, 8**2], [1, 3, 3**2, 7, 7**2, 9, 9**2], [1, 6, 6**2, 8, 8**2, 5, 5**2], [1, 4, 4**2, 5, 5**2, 3, 3**2] ]) b1 = numpy.array([9, 10, 2, 4, 2, 10]) b2 = numpy.array([9, 4, 6, 6, 5, 3]) print('dog ‚ù§Ô∏è:', numpy.linalg.lstsq(A, b1, rcond=None)[0]) print('cat ‚ù§Ô∏è:', numpy.linalg.lstsq(A, b2, rcond=None)[0]) # dog ‚ù§Ô∏è: [1.29368307 7.03633306 -0.44795498 9.98093332 # -0.75689575 -19.00757486 1.52985734] # cat ‚ù§Ô∏è: [0.47945896 5.30866067 -0.39644128 -1.28704188 # 0.12634295 -4.32392606 0.43081918] This gives us our final version of the model:\ndog(x) = 1.29368307 + 7.03633306 * x1 - 0.44795498 * x1**2 + 9.98093332 * x2 - 0.75689575 * x2**2 - 19.00757486 * x3 + 1.52985734 * x3**2 cat(x) = 0.47945896 + 5.30866067 * x1 - 0.39644128 * x1**2 - 1.28704188 * x2 + 0.12634295 * x2**2 - 4.32392606 * x3 + 0.43081918 * x3**2 Adding curvature to our model eliminates all perceived error, at least within 1e-16. This may seem unbelievable, but when you consider that we only have six input records, it isn‚Äôt really.\nrespondent predicted b1 b1 error predicted b2 b2 error Alyssa P Hacker 9 0 9 0 Ben Bitdiddle 10 0 4 0 Cy D. Fect 2 0 6 0 Eva Lu Ator 4 0 6 0 Lem E. Tweakit 2 0 5 0 Louis Reasoner 10 0 3 0 Total error: 0 0 It should be fairly obvious how one can take this and extrapolate to much larger models. I hope this is useful and that least squares becomes an important part of your lives.\n",
  "wordCount" : "1252",
  "inLanguage": "en",
  "datePublished": "2011-02-15T00:00:00Z",
  "dateModified": "2011-02-15T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Ryan O'Neil"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://ryanjoneil.github.io/posts/2011-02-15-data-fitting-2-very-very-simple-linear-regression-in-python/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "adventures in optimization",
    "logo": {
      "@type": "ImageObject",
      "url": "https://ryanjoneil.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://ryanjoneil.github.io/" accesskey="h" title="adventures in optimization (Alt + H)">adventures in optimization</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://ryanjoneil.github.io/about" title="about">
                    <span>about</span>
                </a>
            </li>
            <li>
                <a href="https://ryanjoneil.github.io/coding" title="coding">
                    <span>coding</span>
                </a>
            </li>
            <li>
                <a href="https://ryanjoneil.github.io/posts" title="posting">
                    <span>posting</span>
                </a>
            </li>
            <li>
                <a href="https://ryanjoneil.github.io/speaking" title="speaking">
                    <span>speaking</span>
                </a>
            </li>
            <li>
                <a href="https://ryanjoneil.github.io/writing" title="writing">
                    <span>writing</span>
                </a>
            </li>
            <li>
                <a href="https://ryanjoneil.github.io/search" title="üîç">
                    <span>üîç</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      üßê Data Fitting 2 - Very, Very Simple Linear Regression in Python
    </h1>
    <div class="post-description">
      Predict how much people like cats and dogs based on their ice cream preferences. Also, Python and numpy.
    </div>
    <div class="post-meta"><span title='2011-02-15 00:00:00 +0000 UTC'>February 15, 2011</span>&nbsp;¬∑&nbsp;Ryan O&#39;Neil

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#data" aria-label="Data">Data</a></li>
                <li>
                    <a href="#model-1" aria-label="Model 1">Model 1</a></li>
                <li>
                    <a href="#model-2" aria-label="Model 2">Model 2</a></li>
                <li>
                    <a href="#model-3" aria-label="Model 3">Model 3</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>This post is based on a memo I sent to some former colleagues at the Post. I&rsquo;ve edited it for use here since it fits well as the second in a series on simple data fitting techniques. If you&rsquo;re among the many enlightened individuals already using regression analysis, then this post is probably not for you. If you aren&rsquo;t, then hopefully this provides everything you need to develop rudimentary predictive models that yield surprising levels of accuracy.</p>
<h2 id="data">Data<a hidden class="anchor" aria-hidden="true" href="#data">#</a></h2>
<p>For purposes of a simple working example, we have collected six records of input data over three dimensions with the goal of predicting two outputs. The input data are:</p>
<p>$$
\begin{align*}
x_1 &amp;= \text{How much a respondent likes vanilla [0-10]}\\
x_2 &amp;= \text{How much a respondent likes strawberry [0-10]}\\
x_3 &amp;= \text{How much a respondent likes chocolate [0-10]}
\end{align*}
$$</p>
<p>Output data consist of:</p>
<p>$$
\begin{align*}
b_1 &amp;= \text{How much a respondent likes dogs [0-10]}\\
b_2 &amp;= \text{How much a respondent likes cats [0-10]}
\end{align*}
$$</p>
<p>Below are anonymous data collected from a random sample of people.</p>
<table>
<thead>
<tr>
<th>respondent</th>
<th style="text-align:right">vanilla ‚ù§Ô∏è</th>
<th style="text-align:right">strawberry ‚ù§Ô∏è</th>
<th style="text-align:right">chocolate ‚ù§Ô∏è</th>
<th style="text-align:right">dog ‚ù§Ô∏è</th>
<th style="text-align:right">cat ‚ù§Ô∏è</th>
</tr>
</thead>
<tbody>
<tr>
<td>Alyssa P Hacker</td>
<td style="text-align:right">9</td>
<td style="text-align:right">4</td>
<td style="text-align:right">9</td>
<td style="text-align:right">9</td>
<td style="text-align:right">8</td>
</tr>
<tr>
<td>Ben Bitdiddle</td>
<td style="text-align:right">8</td>
<td style="text-align:right">6</td>
<td style="text-align:right">4</td>
<td style="text-align:right">10</td>
<td style="text-align:right">4</td>
</tr>
<tr>
<td>Cy D. Fect</td>
<td style="text-align:right">9</td>
<td style="text-align:right">4</td>
<td style="text-align:right">8</td>
<td style="text-align:right">2</td>
<td style="text-align:right">6</td>
</tr>
<tr>
<td>Eva Lu Ator</td>
<td style="text-align:right">3</td>
<td style="text-align:right">7</td>
<td style="text-align:right">9</td>
<td style="text-align:right">4</td>
<td style="text-align:right">6</td>
</tr>
<tr>
<td>Lem E. Tweakit</td>
<td style="text-align:right">6</td>
<td style="text-align:right">8</td>
<td style="text-align:right">5</td>
<td style="text-align:right">2</td>
<td style="text-align:right">5</td>
</tr>
<tr>
<td>Louis Reasoner</td>
<td style="text-align:right">4</td>
<td style="text-align:right">5</td>
<td style="text-align:right">3</td>
<td style="text-align:right">10</td>
<td style="text-align:right">3</td>
</tr>
</tbody>
</table>
<p>Our input is in three dimensions. Each output requires its own model, so we&rsquo;ll have one for dogs and one for cats. We&rsquo;re looking for functions, <code>dog(x)</code> and <code>cat(x)</code>, that can predict $b_1$ and $b_2$ based on given values of $x_1$, $x_2$, and $x_3$.</p>
<h2 id="model-1">Model 1<a hidden class="anchor" aria-hidden="true" href="#model-1">#</a></h2>
<p>For both models we want to find parameters that minimize their squared residuals (read: errors). There&rsquo;s a number of names for this. Optimization folks like to think of it as unconstrained quadratic optimization, but it&rsquo;s more common to call it least squares or linear regression. It&rsquo;s not necessary to entirely understand why for our purposes, but the function that minimizes these errors is:</p>
<p>$$\beta = ({A^t}A)^{-1}{A^t}b$$</p>
<p>This is implemented for you in the <code>numpy.linalg</code> Python package, which we&rsquo;ll use for examples. Much more information than you probably want can be found <a href="http://en.wikipedia.org/wiki/Least_squares">here</a>.</p>
<p>Below is a first stab at a Python version. It runs least squares against our input and output data exactly as they are. You can see the matrix $A$ and outputs $b_1$ and $b_2$ (dog and cat love, respectively) are represented just as they are in the table.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Version 1: No offset, no squared inputs</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>A <span style="color:#f92672">=</span> numpy<span style="color:#f92672">.</span>vstack([
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">9</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">4</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">8</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">9</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">5</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">3</span>]
</span></span><span style="display:flex;"><span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>b1 <span style="color:#f92672">=</span> numpy<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">10</span>])
</span></span><span style="display:flex;"><span>b2 <span style="color:#f92672">=</span> numpy<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">3</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;dog ‚ù§Ô∏è:&#39;</span>, numpy<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>lstsq(A, b1, rcond<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>)[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;cat ‚ù§Ô∏è:&#39;</span>, numpy<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>lstsq(A, b2, rcond<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>)[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Output:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># dog ‚ù§Ô∏è: [0.72548294      0.53045642     -0.29952361]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># cat ‚ù§Ô∏è: [2.36110929e-01  2.61934385e-05  6.26892476e-01]</span>
</span></span></code></pre></div><p>The resulting model is:</p>
<pre tabindex="0"><code>dog(x) = 0.72548294 * x1 + 0.53045642 * x2 - 0.29952361 * x3
cat(x) = 2.36110929e-01 * x1 + 2.61934385e-05 * x2 + 6.26892476e-01 * x3
</code></pre><p>The coefficients before our variables correspond to beta in the formula above. Errors between observed and predicted data, shown below, are calculated and summed. For these six records, <code>dog(x)</code> has a total error of 20.76 and <code>cat(x)</code> has 3.74. Not great.</p>
<table>
<thead>
<tr>
<th>respondent</th>
<th style="text-align:right">predicted b1</th>
<th style="text-align:right">b1 error</th>
<th style="text-align:right">predicted b2</th>
<th style="text-align:right">b2 error</th>
</tr>
</thead>
<tbody>
<tr>
<td>Alyssa P Hacker</td>
<td style="text-align:right">5.96</td>
<td style="text-align:right">3.04</td>
<td style="text-align:right">7.77</td>
<td style="text-align:right">1.23</td>
</tr>
<tr>
<td>Ben Bitdiddle</td>
<td style="text-align:right">7.79</td>
<td style="text-align:right">2.21</td>
<td style="text-align:right">4.40</td>
<td style="text-align:right">0.40</td>
</tr>
<tr>
<td>Cy D. Fect</td>
<td style="text-align:right">6.25</td>
<td style="text-align:right">4.25</td>
<td style="text-align:right">7.14</td>
<td style="text-align:right">1.14</td>
</tr>
<tr>
<td>Eva Lu Ator</td>
<td style="text-align:right">3.19</td>
<td style="text-align:right">0.81</td>
<td style="text-align:right">6.35</td>
<td style="text-align:right">0.35</td>
</tr>
<tr>
<td>Lem E. Tweakit</td>
<td style="text-align:right">7.10</td>
<td style="text-align:right">5.10</td>
<td style="text-align:right">4.55</td>
<td style="text-align:right">0.45</td>
</tr>
<tr>
<td>Louis Reasoner</td>
<td style="text-align:right">4.66</td>
<td style="text-align:right">5.34</td>
<td style="text-align:right">2.83</td>
<td style="text-align:right">0.17</td>
</tr>
<tr>
<td>Total error:</td>
<td style="text-align:right"></td>
<td style="text-align:right">20.76</td>
<td style="text-align:right"></td>
<td style="text-align:right">3.74</td>
</tr>
</tbody>
</table>
<h2 id="model-2">Model 2<a hidden class="anchor" aria-hidden="true" href="#model-2">#</a></h2>
<p>One problem with this model is that <code>dog(x)</code> and <code>cat(x)</code> are forced to pass through the origin. <em>(Why is that?)</em> We can improve it somewhat if we add an offset. This amounts to prepending 1 to every row in $A$ and adding a constant to the resulting functions. You can see the very slight difference between the code for this model and that of the previous:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Version 2: Offset, no squared inputs</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>A <span style="color:#f92672">=</span> numpy<span style="color:#f92672">.</span>vstack([
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">9</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">4</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">8</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">9</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">5</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">3</span>]
</span></span><span style="display:flex;"><span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;dog ‚ù§Ô∏è:&#39;</span>, numpy<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>lstsq(A, b1, rcond<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>)[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;cat ‚ù§Ô∏è:&#39;</span>, numpy<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>lstsq(A, b2, rcond<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>)[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Output:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># dog ‚ù§Ô∏è: [20.92975427  -0.27831197  -1.43135684  -0.76469017]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># cat ‚ù§Ô∏è: [-0.31744124   0.25133547   0.02978098   0.63394765]</span>
</span></span></code></pre></div><p>This yields the seconds version of our models:</p>
<pre tabindex="0"><code>dog(x) = 20.92975427 - 0.27831197 * x1 - 1.43135684 * x2 - 0.76469017 * x3
cat(x) = -0.31744124 + 0.25133547 * x1 + 0.02978098 * x2 + 0.63394765 * x3
</code></pre><p>These models provide errors of 13.87 and 3.79.  A little better on the dog side, but still not quite usable.</p>
<table>
<thead>
<tr>
<th>respondent</th>
<th style="text-align:right">predicted b1</th>
<th style="text-align:right">b1 error</th>
<th style="text-align:right">predicted b2</th>
<th style="text-align:right">b2 error</th>
</tr>
</thead>
<tbody>
<tr>
<td>Alyssa P Hacker</td>
<td style="text-align:right">5.82</td>
<td style="text-align:right">3.18</td>
<td style="text-align:right">7.77</td>
<td style="text-align:right">1.23</td>
</tr>
<tr>
<td>Ben Bitdiddle</td>
<td style="text-align:right">7.06</td>
<td style="text-align:right">2.94</td>
<td style="text-align:right">4.41</td>
<td style="text-align:right">0.41</td>
</tr>
<tr>
<td>Cy D. Fect</td>
<td style="text-align:right">6.58</td>
<td style="text-align:right">4.58</td>
<td style="text-align:right">7.14</td>
<td style="text-align:right">1.14</td>
</tr>
<tr>
<td>Eva Lu Ator</td>
<td style="text-align:right">3.19</td>
<td style="text-align:right">0.81</td>
<td style="text-align:right">6.35</td>
<td style="text-align:right">0.35</td>
</tr>
<tr>
<td>Lem E. Tweakit</td>
<td style="text-align:right">3.99</td>
<td style="text-align:right">1.99</td>
<td style="text-align:right">4.60</td>
<td style="text-align:right">0.40</td>
</tr>
<tr>
<td>Louis Reasoner</td>
<td style="text-align:right">10.37</td>
<td style="text-align:right">0.37</td>
<td style="text-align:right">2.74</td>
<td style="text-align:right">0.26</td>
</tr>
<tr>
<td>Total error:</td>
<td style="text-align:right"></td>
<td style="text-align:right">13.87</td>
<td style="text-align:right"></td>
<td style="text-align:right">3.79</td>
</tr>
</tbody>
</table>
<h2 id="model-3">Model 3<a hidden class="anchor" aria-hidden="true" href="#model-3">#</a></h2>
<p>The problem is that <code>dog(x)</code> and <code>cat(x)</code> are linear functions. Most observed data don&rsquo;t conform to straight lines. Take a moment and draw the line $f(x) = x$ and the curve $f(x) = x^2$. The former makes a poor approximation of the latter.</p>
<p>Most of the time, people just use squares of the input data to add curvature to their models. We do this in our next version of the code by just adding squares of the input row values to our $A$ matrix. Everything else is the same. (In reality, you can add any function of the input data you feel best models the data, if you understand it well enough.)</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Version 3: Offset with squared inputs</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>A <span style="color:#f92672">=</span> numpy<span style="color:#f92672">.</span>vstack([
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">9</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">4</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">9</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">8</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">6</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">4</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">9</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">4</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">8</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">7</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">9</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">6</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">8</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">4</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>]
</span></span><span style="display:flex;"><span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>b1 <span style="color:#f92672">=</span> numpy<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">10</span>])
</span></span><span style="display:flex;"><span>b2 <span style="color:#f92672">=</span> numpy<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">3</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;dog ‚ù§Ô∏è:&#39;</span>, numpy<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>lstsq(A, b1, rcond<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>)[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;cat ‚ù§Ô∏è:&#39;</span>, numpy<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>lstsq(A, b2, rcond<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>)[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># dog ‚ù§Ô∏è: [1.29368307  7.03633306  -0.44795498  9.98093332</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#  -0.75689575  -19.00757486  1.52985734]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># cat ‚ù§Ô∏è: [0.47945896  5.30866067  -0.39644128 -1.28704188</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   0.12634295   -4.32392606  0.43081918]</span>
</span></span></code></pre></div><p>This gives us our final version of the model:</p>
<pre tabindex="0"><code>dog(x) = 1.29368307 + 7.03633306 * x1 - 0.44795498 * x1**2 + 9.98093332 * x2 - 0.75689575 * x2**2 - 19.00757486 * x3 + 1.52985734 * x3**2
cat(x) = 0.47945896 + 5.30866067 * x1 - 0.39644128 * x1**2 - 1.28704188 * x2 + 0.12634295 * x2**2 - 4.32392606 * x3 + 0.43081918 * x3**2
</code></pre><p>Adding curvature to our model eliminates all perceived error, at least within 1e-16. This may seem unbelievable, but when you consider that we only have six input records, it isn&rsquo;t really.</p>
<table>
<thead>
<tr>
<th>respondent</th>
<th style="text-align:right">predicted b1</th>
<th style="text-align:right">b1 error</th>
<th style="text-align:right">predicted b2</th>
<th style="text-align:right">b2 error</th>
</tr>
</thead>
<tbody>
<tr>
<td>Alyssa P Hacker</td>
<td style="text-align:right">9</td>
<td style="text-align:right">0</td>
<td style="text-align:right">9</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td>Ben Bitdiddle</td>
<td style="text-align:right">10</td>
<td style="text-align:right">0</td>
<td style="text-align:right">4</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td>Cy D. Fect</td>
<td style="text-align:right">2</td>
<td style="text-align:right">0</td>
<td style="text-align:right">6</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td>Eva Lu Ator</td>
<td style="text-align:right">4</td>
<td style="text-align:right">0</td>
<td style="text-align:right">6</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td>Lem E. Tweakit</td>
<td style="text-align:right">2</td>
<td style="text-align:right">0</td>
<td style="text-align:right">5</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td>Louis Reasoner</td>
<td style="text-align:right">10</td>
<td style="text-align:right">0</td>
<td style="text-align:right">3</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td>Total error:</td>
<td style="text-align:right"></td>
<td style="text-align:right">0</td>
<td style="text-align:right"></td>
<td style="text-align:right">0</td>
</tr>
</tbody>
</table>
<p>It should be fairly obvious how one can take this and extrapolate to much larger models. I hope this is useful and that least squares becomes an important part of your lives.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://ryanjoneil.github.io/tags/data-fitting/">data fitting</a></li>
      <li><a href="https://ryanjoneil.github.io/tags/regression/">regression</a></li>
      <li><a href="https://ryanjoneil.github.io/tags/python/">python</a></li>
      <li><a href="https://ryanjoneil.github.io/tags/modeling/">modeling</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://ryanjoneil.github.io/posts/2011-02-16-data-fitting-2a-very-very-simple-linear-regression-in-r/">
    <span class="title">¬´ Prev</span>
    <br>
    <span>üßê Data Fitting 2a - Very, Very Simple Linear Regression in R</span>
  </a>
  <a class="next" href="https://ryanjoneil.github.io/posts/2010-11-30-off-the-cuff-voter-fraud-detection/">
    <span class="title">Next ¬ª</span>
    <br>
    <span>üó≥ Off the Cuff Voter Fraud Detection</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://ryanjoneil.github.io/">adventures in optimization</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
