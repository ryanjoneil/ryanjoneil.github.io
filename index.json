[{"content":"I\u0026rsquo;ve been a mostly absent blogger for the past few years. I could make excuses. They might sound like, \u0026ldquo;I was busy finishing my dissertation!\u0026rdquo; or \u0026ldquo;I founded a company and have a toddler!\u0026rdquo; or \u0026ldquo;The static site generator I used was abandoned!\u0026rdquo; Whatever they might be, these excuses would certainly end in exclamation points.\nBut, ultimately, for several years it just felt like blogging was dead. Its space was usurped by Tweets, LinkedIn hustle posts, long form Medium content aimed at attracting talent, and other content trends. RSS feeds dried up bit by bit. That beautiful structure somewhere between a college essay and an academic preprint mostly ceased to be. Sad times, indeed.\nThat trend seems to be reversing. I don\u0026rsquo;t know whether it\u0026rsquo;s the result of nudges from Substack, or that all the introverts in tech finally gave up baking pandemic, but there is a lot of good content out there again! Fire up your RSS aggregators and get reading.\nThis post is my own reboot of \u0026ldquo;adventures in optimization,\u0026rdquo; a blog I\u0026rsquo;ve written intermittently since 2009. Unfortunately, it will take some time to move over all my old posts to Hugo. I\u0026rsquo;ll do that slowly as I create new ones. For now, I\u0026rsquo;ve ported over a couple early posts and put together a list of the active blogs I\u0026rsquo;m gleefully catching up on.\nSee you soon!\nConstraint Applications Blog by Helmut Simonis Erling\u0026rsquo;s blog eugeneyan Harlan D. Harris Lil\u0026rsquo;Log Nathan Brixius Punk Rock Operations Research Shtetl-Optimized Timefold ","permalink":"https://ryanjoneil.github.io/posts/2023-09-07-blogging-is-back/","summary":"I\u0026rsquo;ve been a mostly absent blogger for the past few years. I could make excuses. They might sound like, \u0026ldquo;I was busy finishing my dissertation!\u0026rdquo; or \u0026ldquo;I founded a company and have a toddler!\u0026rdquo; or \u0026ldquo;The static site generator I used was abandoned!\u0026rdquo; Whatever they might be, these excuses would certainly end in exclamation points.\nBut, ultimately, for several years it just felt like blogging was dead. Its space was usurped by Tweets, LinkedIn hustle posts, long form Medium content aimed at attracting talent, and other content trends.","title":"üöÄ Blogging is back, baby!"},{"content":"This is a running list of talks I\u0026rsquo;ve given and am scheduled to give. It probably isn\u0026rsquo;t exhaustive. Some of them have slides or videos available.\nThe future\u0026hellip; September 20, 2023 - DecisionCAMP 2023 - Decision model, meet the real world: Testing optimization models for use in production environments (abstract, slides)\nOctober 17, 2023 - INFORMS Annual Meeting - Adapting to Change in On-Demand Delivery: Unpacking a Suite of Testing Methodologies (abstract)\nThe past\u0026hellip; August 27, 2023 - DPSOLVE 2023 - Implementing Decision Diagrams in Production Systems (slides)\nMay 11, 2023 - Nextmv Videos - Several people are optimizing: Collaborative workflows for decision model operations (video)\nApril 18, 2023 - INFORMS Business Analytics Conference - Decision Model, Meet Production: A Collaborative Workflow for Optimizing More Operations (abstract)\nMarch 1, 2023 - Nextmv Videos - What are decision diagrams? How are they used to model and solve optimization problems? (video)\nJanuary 18, 2023 - Nextmv Videos - In conversation with Karla Hoffman (video)\nOctober 5, 2020 - INFORMS Philadelphia Chapter - Real-Time Routing for On-Demand Delivery (video)\nOctober 22, 2019 - INFORMS Annual Meeting - Decision Diagrams for Real-Time Routing (slides)\nJuly 6, 2017 - PyData Seattle 2017 - Practical Optimization for Stats Nerds (abstract, video)\nMarch 5, 2017 - Data Science DC - Practical Optimization for Stats Nerds (slides)\nDecember 4, 2015 - PyData NYC 2015 - Optimize your Docker Infrastructure with Python (slides, video)\nJuly 17, 2014 - IFORS 2014 - A MIP-Based Dual Bounding Technique for the Irregular Nesting Problem (abstract, slides)\nFebruary 19, 2010 - PyCon 2010 - Optimal Resource Allocation using Python (video)\n","permalink":"https://ryanjoneil.github.io/speaking/","summary":"This is a running list of talks I\u0026rsquo;ve given and am scheduled to give. It probably isn\u0026rsquo;t exhaustive. Some of them have slides or videos available.\nThe future\u0026hellip; September 20, 2023 - DecisionCAMP 2023 - Decision model, meet the real world: Testing optimization models for use in production environments (abstract, slides)\nOctober 17, 2023 - INFORMS Annual Meeting - Adapting to Change in On-Demand Delivery: Unpacking a Suite of Testing Methodologies (abstract)","title":"üí¨ Some talk"},{"content":"I\u0026rsquo;m an desultory blogger and intermittent academic. Nonetheless, I like the research and writing process. Most of my current and old posts live here. Some of my other writings and collaborations are listed below.\nPapers, patents \u0026amp; preprints\u0026hellip; June 2023 - USPTO - Runners for optimization solvers and simulators describes technology for creating and executing Decision Diagram-based optimization solvers and state-based simulators in cloud environments. (patent)\nSeptember 2020 - Operations Research Forum - MIPLIBing: Seamless Benchmarking of Mathematical Optimization Problems and Metadata Extensions presents a Python library that automatically downloads queried subsets from the current versions of MIPLIB, MINLPLib, and QPLIB, provides a centralized local cache across projects, and tracks the best solution values and bounds on record for each problem. (preprint)\nMay 2019 - Operations Research Letters - Decision diagrams for solving traveling salesman problems with pickup and delivery in real time explores the use of Multivalued Decision Diagrams and Assignment Problem inference duals for real-time optimization of TSPPDs. (preprint)\nOctober 2018 - Optimization Online - Integer Models for the Asymmetric Traveling Salesman Problem with Pickup and Delivery proposes a new ATSPPD model, new valid inequalities for the Sarin-Sherali-Bhootra ATSPPD, and studies the impact of relaxing complicating constraints in these. (preprint)\nSeptember 2018 - Optimization Online - Exact Methods for Solving Traveling Salesman Problems with Pickup and Delivery in Real Time examines exact methods for solving TSPPDs with consolidation in real-time applications. It considers enumerative, Mixed Integer Programming, Constraint Programming, and hybrid optimization approaches under various time budgets. (preprint)\nMarch 2018 - Optimization Online -The Meal Delivery Routing Problem introduces the MDRP to formalize and study an important emerging class of dynamic delivery operations. It also develops optimization-based algorithms tailored to solve the courier assignment (dynamic vehicle routing) and capacity management (offline shift scheduling) problems encountered in meal delivery operations. (preprint)\nOther blogs\u0026hellip; April 20, 2022 - Nextmv Blog - You need a solver. What is a solver?\nMarch 2, 2021 - Nextmv Blog - Binaries are beautiful\nMarch 2, 2020 - Nextmv Blog - How Hop Hops\nSeptember 13, 2018 - Grubhub Bytes - Decisions are first class citizens: an introduction to Decision Engineering\nJanuary 5, 2015 - The Yhat Blog - Currency Portfolio Optimization Using ScienceOps\nNovember 10, 2014 - The Yhat Blog - How Yhat Does Cloud Balancing: A Case Study\n","permalink":"https://ryanjoneil.github.io/writing/","summary":"I\u0026rsquo;m an desultory blogger and intermittent academic. Nonetheless, I like the research and writing process. Most of my current and old posts live here. Some of my other writings and collaborations are listed below.\nPapers, patents \u0026amp; preprints\u0026hellip; June 2023 - USPTO - Runners for optimization solvers and simulators describes technology for creating and executing Decision Diagram-based optimization solvers and state-based simulators in cloud environments. (patent)\nSeptember 2020 - Operations Research Forum - MIPLIBing: Seamless Benchmarking of Mathematical Optimization Problems and Metadata Extensions presents a Python library that automatically downloads queried subsets from the current versions of MIPLIB, MINLPLib, and QPLIB, provides a centralized local cache across projects, and tracks the best solution values and bounds on record for each problem.","title":"üìù Some text"},{"content":" I use data and models to simplify complex decisions.\nBy day, I am an operations research analyst, Go coder, and co-founder of Nextmv. I\u0026rsquo;m interested in hybrid optimization, decision diagrams, and mixed integer programming. My applications skew heavily toward logistics for delivery platforms, with detours into cutting and packing.\nFor the past several years, I\u0026rsquo;ve worked in real-time optimization for on-demand delivery, scheduling, forecasting, and simulation. I did a MS in Operations Research by night at [George Mason University], then a PhD in the same field under the advisement of Karla Hoffman.\nBy night, I am am amateur cellist, and a cat and early music enthusiast.\n","permalink":"https://ryanjoneil.github.io/about/","summary":"I use data and models to simplify complex decisions.\nBy day, I am an operations research analyst, Go coder, and co-founder of Nextmv. I\u0026rsquo;m interested in hybrid optimization, decision diagrams, and mixed integer programming. My applications skew heavily toward logistics for delivery platforms, with detours into cutting and packing.\nFor the past several years, I\u0026rsquo;ve worked in real-time optimization for on-demand delivery, scheduling, forecasting, and simulation. I did a MS in Operations Research by night at [George Mason University], then a PhD in the same field under the advisement of Karla Hoffman.","title":"üññ Hi, I'm Ryan."},{"content":"Most of my work is proprietary, but some of it is open. Here are a few projects I\u0026rsquo;ve built okr made significant contributions. I\u0026rsquo;ve also made significant contributions to projects such as PuLP, MIPLIBing, and MDRPlib.\nAlive\u0026hellip; The Ruby Algebraic Modeling System is a simple modeling tool for formulating and solving MILPs in Ruby.\nap.cpp is an incremental primal-dual assignment problem solver written in C++. It can vastly improve propagation in hybrid optimization models that use AP relaxations. I use it within custom propagators in Gecode and in Decision Diagrams for solving the Traveling Salesman Problem with side constraints.\nap is a Go version of ap.cpp.\nTSPPD Hybrid Optimization Code and TSPPD Decision Diagram Code are both used in my dissertation. The former contains C++14 code for hybrid CP and MIP models for solving TSPPDs. The latter uses a hybridized Decision Diagram implementation with an Assignment Problem inference dual inside a branch-and-bound.\nTSPPDlib is a standard test set for TSPPDs. The instances are based on observed meal delivery data at Grubhub.\nDefunct\u0026hellip; python-zibopt was a Python interface to the SCIP Optimization Suite. This was no longer necessary once PySCIPOpt emerged.\nChute was a simple, lightweight tool for running discrete event simulations in Python.\nPyGEP was a simple library suitable for academic study of GEP (Gene Expression Programming) in Python 2.\n","permalink":"https://ryanjoneil.github.io/coding/","summary":"Most of my work is proprietary, but some of it is open. Here are a few projects I\u0026rsquo;ve built okr made significant contributions. I\u0026rsquo;ve also made significant contributions to projects such as PuLP, MIPLIBing, and MDRPlib.\nAlive\u0026hellip; The Ruby Algebraic Modeling System is a simple modeling tool for formulating and solving MILPs in Ruby.\nap.cpp is an incremental primal-dual assignment problem solver written in C++. It can vastly improve propagation in hybrid optimization models that use AP relaxations.","title":"üßë‚ÄçüíªÔ∏è Some code"},{"content":"Note: This posted was updated to work with Python 3 and PySCIPOpt. The original version used Python 2 and python-zibopt.\nIntroduction Data fitting is one of those tasks that everyone should have at least some exposure to. Certainly developers and analysts will benefit from a working knowledge of its fundamentals and their implementations. However, in my own reading I\u0026rsquo;ve found it difficult to locate good examples that are simple enough to pick up quickly and come with accompanying source code.\nThis article commences an ongoing series introducing basic data fitting techniques. With any luck they won\u0026rsquo;t be overly complex, while still being useful enough to get the point across with a real example and real data. We\u0026rsquo;ll start with a binary classification problem: presented with a series of records, each containing a set number of input values describing it, determine whether or not each record exhibits some property.\nModel We\u0026rsquo;ll use the cancer1.dt data from the proben1 set of test cases, which you can download here. Each record starts with 9 data points containing physical characteristics of a tumor. The second to last data point contains 1 if a tumor is benign and 0 if it is malignant. We seek to find a linear function we can run on an arbitrary record that will return a value greater than zero if that record\u0026rsquo;s tumor is predicted to be benign and less than zero if it is predicted to be malignant. We will train our linear model on the first 350 records, and test it for accuracy on the remaining rows.\nThis is similar to the data fitting problem found in Chvatal. Our inputs consist of a matrix of observed data, $A$, and a vector of classifications, $b$. In order to classify a record, we require another vector $x$ such that the dot product of $x$ and that record will be either greater or less than zero depending on its predicted classification.\nA couple points to note before we start:\nMost observed data are noisy. This means it may be impossible to locate a hyperplane that cleanly separates given records of one type from another. In this case, we must resort to finding a function that minimizes our predictive error. For the purposes of this example, we\u0026rsquo;ll minimize the sum of the absolute values of the observed and predicted values. That is, we seek $x$ such that we find $min \\sum_i{|a_i^T x-b_i|}$.\nThe slope-intercept form of a line, $f(x)=m^T x+b$, contains an offset. It should be obvious that this is necessary in our model so that our function isn\u0026rsquo;t required to pass through the origin. Thus, we\u0026rsquo;ll be adding an extra variable with the coefficient of 1 to represent our offset value.\nIn order to model this, we use two linear constraints for each absolute value. We minimize the sum of these. Our Linear Programming model thus looks like:\n$$ \\begin{align*} \\min\\quad \u0026amp; z = x_0 + \\sum_i{v_i}\\\\ \\text{s.t.}\\quad\u0026amp; v_i \\geq x_0 + a_i^\\intercal x - 1 \u0026amp;\\quad\\forall\u0026amp;\\quad\\text{benign tumors}\\\\ \u0026amp; v_i \\geq 1 - x_0 - a_i^\\intercal x \u0026amp;\\quad\\forall\u0026amp;\\quad\\text{benign tumors}\\\\ \u0026amp; v_i \\geq x_0 + a_i^\\intercal x - (-1) \u0026amp;\\quad\\forall\u0026amp;\\quad\\text{malignant tumors}\\\\ \u0026amp; v_i \\geq -1 - x_0 - a_i^\\intercal x \u0026amp;\\quad\\forall\u0026amp;\\quad\\text{malignant tumors} \\end{align*} $$\nCode In order to do this in Python, we use SCIP and SoPlex. We start by setting constants for benign and malignant outputs and providing a function to read in the training and testing data sets.\n# Preferred output values for tumor categories BENIGN = 1 MALIGNANT = -1 def read_proben1_cancer_data(filename, train_size): \u0026#39;\u0026#39;\u0026#39;Loads a proben1 cancer file into train \u0026amp; test sets\u0026#39;\u0026#39;\u0026#39; # Number of input data points per record DATA_POINTS = 9 train_data = [] test_data = [] with open(filename) as infile: # Read in the first train_size lines to a training data list, and the # others to testing data. This allows us to test how general our model # is on something other than the input data. for line in infile.readlines()[7:]: # skip header line = line.split() # Records = offset (x0) + remaining data points input = [float(x) for x in line[:DATA_POINTS]] output = BENIGN if line[-2] == \u0026#39;1\u0026#39; else MALIGNANT record = {\u0026#39;input\u0026#39;: input, \u0026#39;output\u0026#39;: output} # Determine what data set to put this in if len(train_data) \u0026gt;= train_size: test_data.append(record) else: train_data.append(record) return train_data, test_data The next function implements the LP model described above using SoPlex and SCIP. It minimizes the sum of residuals for each training record. This amounts to summing the absolute value of the difference between predicted and observed output data. The following function takes in input and observed output data and returns a list of coefficients. Our resulting model consists of taking the dot product of an input record and these coefficients. If the result is greater than or equal to zero, that record is predicted to be a benign tumor, otherwise it is predicted to be malignant.\nfrom pyscipopt import Model def train_linear_model(train_data): \u0026#39;\u0026#39;\u0026#39; Accepts a set of input training data with known output values. Returns a list of coefficients to apply to arbitrary records for purposes of binary categorization. \u0026#39;\u0026#39;\u0026#39; # Make sure we have at least one training record. assert len(train_data) \u0026gt; 0 num_variables = len(train_data[0][\u0026#39;input\u0026#39;]) # Variables are coefficients in front of the data points. It is important # that these be unrestricted in sign so they can take negative values. m = Model() x = [m.addVar(f\u0026#39;x{i}\u0026#39;, lb=None) for i in range(num_variables)] # Residual for each data row residuals = [m.addVar(lb=None, ub=None) for _ in train_data] for r, d in zip(residuals, train_data): # r will be the absolute value of the difference between observed and # predicted values. We can model absolute values such as r \u0026gt;= |foo| as: # # r \u0026gt;= foo # r \u0026gt;= -foo m.addCons(sum(x * y for x, y in zip(x, d[\u0026#39;input\u0026#39;])) + r \u0026gt;= d[\u0026#39;output\u0026#39;]) m.addCons(sum(x * y for x, y in zip(x, d[\u0026#39;input\u0026#39;])) - r \u0026lt;= d[\u0026#39;output\u0026#39;]) # Find and return coefficients that min sum of residuals. m.setObjective(sum(residuals)) m.setMinimize() m.optimize() solution = m.getBestSol() return [solution[xi] for xi in x] We also provide a convenience function for counting the number of correct predictions by our resulting model against either the test or training data sets.\ndef count_correct(data_set, coefficients): \u0026#39;\u0026#39;\u0026#39;Returns the number of correct predictions.\u0026#39;\u0026#39;\u0026#39; correct = 0 for d in data_set: result = sum(x*y for x, y in zip(coefficients, d[\u0026#39;input\u0026#39;])) # Do we predict the same as the output? if (result \u0026gt;= 0) == (d[\u0026#39;output\u0026#39;] \u0026gt;= 0): correct += 1 return correct Finally we write a main method to read in the data, build our linear model, and test its efficacy.\nfrom pprint import pprint if __name__ == \u0026#39;__main__\u0026#39;: # Specs for this input file INPUT_FILE_NAME = \u0026#39;cancer1.dt\u0026#39; TRAIN_SIZE = 350 train_data, test_data = read_proben1_cancer_data( INPUT_FILE_NAME, TRAIN_SIZE ) # Add the offset variable to each of our data records for data_set in [train_data, test_data]: for row in data_set: row[\u0026#39;input\u0026#39;] = [1] + row[\u0026#39;input\u0026#39;] coefficients = train_linear_model(train_data) print(\u0026#39;coefficients:\u0026#39;) pprint(coefficients) # Print % of correct predictions for each data set correct = count_correct(train_data, coefficients) print( \u0026#39;%s / %s = %.02f%% correct on training set\u0026#39; % ( correct, len(train_data), 100 * float(correct) / len(train_data) ) ) correct = count_correct(test_data, coefficients) print( \u0026#39;%s / %s = %.02f%% correct on testing set\u0026#39; % ( correct, len(test_data), 100 * float(correct) / len(test_data) ) ) Results The result of running this model against the cancer1.dt data set is:\ncoefficients: [1.4072882449702786, -0.14014055927954652, -0.6239513714263405, -0.26727681774258882, 0.067107753841131157, -0.28300216102808429, -1.0355594670918404, -0.22774451038152174, -0.69871243677663608, -0.072575089848659444] 328 / 350 = 93.71% correct on training set 336 / 349 = 96.28% correct on testing set The accuracy is pretty good here against the both the training and testing sets, so this particular model generalizes well. This is about the simplest model we can implement for data fitting, and we\u0026rsquo;ll get to more complicated ones later, but it\u0026rsquo;s nice to see we can do so well so quickly. The coefficients correspond to using a function of this form, rounding off to three decimal places:\n$$ \\begin{align*} f(x) =\\ \u0026amp;1.407 - 0.140 x_1 - 0.624 x_2 - 0.267 x_3 + 0.067 x_4 - \\\\ \u0026amp;0.283 x_5 - 1.037 x_6 - 0.228 x_7 - 0.699 x_8 - 0.073 x_9 \\end{align*} $$\nResources cancer1.dt data file from proben1 Full source listing ","permalink":"https://ryanjoneil.github.io/posts/2010-11-23-data-fitting-1-linear-data-fitting/","summary":"Note: This posted was updated to work with Python 3 and PySCIPOpt. The original version used Python 2 and python-zibopt.\nIntroduction Data fitting is one of those tasks that everyone should have at least some exposure to. Certainly developers and analysts will benefit from a working knowledge of its fundamentals and their implementations. However, in my own reading I\u0026rsquo;ve found it difficult to locate good examples that are simple enough to pick up quickly and come with accompanying source code.","title":"üßê Data Fitting 1 - Linear Data Fitting"},{"content":"Note: This posted was updated to work with Python 3.\nOne of the most useful tools one learns in an Operations Research curriculum is Monte Carlo Simulation. Its utility lies in its simplicity: one can learn vital information about nearly any process, be it deterministic or stochastic, without wading through the grunt work of finding an analytical solution. It can be used for off-the-cuff estimates or as a proper scientific tool. All one needs to know is how to simulate a given process and its appropriate probability distributions and parameters if that process is stochastic.\nHere\u0026rsquo;s how it works:\nConstruct a simulation that, given input values, returns a value of interest. This could be a pure quantity, like time spent waiting for a bus, or a boolean indicating whether or not a particular event occurs. Run the simulation a, usually large, number of times, each time with randomly generated input variables. Record its output values. Compute sample mean and variance of the output values. In the case of time spent waiting for a bus, the sample mean and variance are estimators of mean and variance for one\u0026rsquo;s wait time. In the boolean case, these represent probability that the given event will occur.\nOne can think of Monte Carlo Simulation as throwing darts. Say you want to find the area under a curve without integrating. All you must do is draw the curve on a wall and throw darts at it randomly. After you\u0026rsquo;ve thrown enough darts, the area under the curve can be approximated using the percentage of darts that end up under the curve times the total area.\nThis technique is often performed using a spreadsheet, but that can be a bit clunky and may make more complex simulations difficult. I\u0026rsquo;d like to spend a minute showing how it can be done in Python. Consider the following scenario:\nPassengers for a train arrive according to a Poisson process with a mean of 100 per hour. The next train arrives exponentially with a rate of 5 per hour. How many passers will be aboard the train?\nWe can simulate this using the fact that a Poisson process can be represented as a string of events occurring with exponential inter-arrival times. We use the sim() function below to generate the number of passengers for random instances of the problem. We then compute sample mean and variance for these values.\nimport random PASSENGERS = 100.0 TRAINS = 5.0 ITERATIONS = 10000 def sim(): passengers = 0.0 # Determine when the train arrives train = random.expovariate(TRAINS) # Count the number of passenger arrivals before the train now = 0.0 while True: now += random.expovariate(PASSENGERS) if now \u0026gt;= train: break passengers += 1.0 return passengers if __name__ == \u0026#39;__main__\u0026#39;: output = [sim() for _ in range(ITERATIONS)] total = sum(output) mean = total / len(output) sum_sqrs = sum(x*x for x in output) variance = (sum_sqrs - total * mean) / (len(output) - 1) print(\u0026#39;E[X] = %.02f\u0026#39; % mean) print(\u0026#39;Var(X) = %.02f\u0026#39; % variance) ","permalink":"https://ryanjoneil.github.io/posts/2009-10-08-monte-carlo-simulation-in-python/","summary":"Note: This posted was updated to work with Python 3.\nOne of the most useful tools one learns in an Operations Research curriculum is Monte Carlo Simulation. Its utility lies in its simplicity: one can learn vital information about nearly any process, be it deterministic or stochastic, without wading through the grunt work of finding an analytical solution. It can be used for off-the-cuff estimates or as a proper scientific tool.","title":"üêç Monte Carlo Simulation in Python"},{"content":"One of the difficulties we encounter in solving the Traveling Salesman Problem (TSP) is that, for even a small numer of cities, a complete description of the problem requires a factorial number of constraints. This is apparent in the standard formulation used to teach the TSP to OR students. Consider a set of $n$ cities with the distance from city $i$ to city $j$ denoted $d_{ij}$. We attempt to minimize the total distance of a tour entering and leaving each city exactly once. $x_{ij} = 1$ if the edge from city $i$ to city $j$ is included in the tour, $0$ otherwise:\n$$ \\small \\begin{align*} \\min\\quad \u0026amp; z = \\sum_i \\sum_{j\\ne i} d_{ij} x_{ij}\\\\ \\text{s.t.}\\quad\u0026amp; \\sum_{j\\ne i} x_{ij} = 1 \u0026amp;\\quad\\forall\u0026amp;\\ i \u0026amp; \\text{leave each city once}\\\\ \u0026amp; \\sum_{i\\ne j} x_{ij} = 1 \u0026amp;\\quad\\forall\u0026amp;\\ j \u0026amp; \\text{enter each city once}\\\\ \u0026amp; x_{ij} \\in \\{0,1\\} \u0026amp;\\quad\\forall\u0026amp;\\ i,j \\end{align*} $$\nThis appears like a reasonable formulation until we solve it and see that our solution contains disconnected subtours. Suppose we have four cities, labeled $A$ through $D$. Connecting $A$ to $B$, $B$ to $A$, $C$ to $D$ and $D$ to $C$ provides a feasible solution to our formulation, but does not constitute a cycle. Here is a more concrete example of two disconnected subtours $\\{(1,5),(5,1)\\}$ and $\\{(2,3),(3,4),(4,2)\\}$ over five cities:\nampl: display x; x [*,*] : 1 2 3 4 5 := 1 0 0 0 0 1 2 0 0 1 0 0 3 0 0 0 1 0 4 0 1 0 0 0 5 1 0 0 0 0 ; Realizing we just solved the Assignment Problem, we now add subtour elimination constraints. These require that any proper, non-null subset of our $n$ cities is connected by at most $n-1$ active edges:\n$$ \\sum_{i \\in S} \\sum_{j \\in S} x_{ij} \\leq |S|-1 \\quad\\forall\\ S \\subset {1, \u0026hellip;, n}, S \\ne O $$\nIndexing subtour elimination constraints over a power set of the cities completes the formulation. However, this requires an additional $\\sum_{k=2}^{n-1} \\begin{pmatrix} n \\\\ k \\end{pmatrix}$ rows tacked on the end of our matrix and is clearly infeasible for large $n$. The most current computers can handle using this approach is around 19 cities. It remains an instructive tool for understanding the combinatorial explosion that occurs in problems like TSP and is worth translating into a modeling language. So how does one model it on a computer?\nUnfortunately, AMPL, the gold standard in mathematical modeling languages, is unable to index over sets. Creating a power set in AMPL requires going through a few contortions. The following code demonstrates power and index sets over four cities:\nset cities := 1 .. 4 ordered; param n := card(cities); set indices := 0 .. (2^n - 1); set power {i in indices} := {c in cities: (i div 2^(ord(c) - 1)) mod 2 = 1}; display cities; display n; display indices; display power; This yields the following output:\nset cities := 1 2 3 4; n = 4 set indices := 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15; set power[0] := ; # empty set power[1] := 1; set power[2] := 2; set power[3] := 1 2; set power[4] := 3; set power[5] := 1 3; set power[6] := 2 3; set power[7] := 1 2 3; set power[8] := 4; set power[9] := 1 4; set power[10] := 2 4; set power[11] := 1 2 4; set power[12] := 3 4; set power[13] := 1 3 4; set power[14] := 2 3 4; set power[15] := 1 2 3 4; Note how the index set contains an index for each row in our power set. We can now generate the subtour elimination constraints:\nvar x {cities cross cities} binary; s.t. subtours {i in indices: card(power[i]) \u0026gt; 1 and card(power[i]) \u0026lt; card(cities)}: sum {(c,k) in power[i] cross power[i]: k != c} x[c,k] \u0026lt;= card(power[i]) - 1; expand subtours; subject to subtours[3]: x[1,2] + x[2,1] \u0026lt;= 1; subject to subtours[5]: x[1,3] + x[3,1] \u0026lt;= 1; subject to subtours[6]: x[2,3] + x[3,2] \u0026lt;= 1; subject to subtours[7]: x[1,2] + x[1,3] + x[2,1] + x[2,3] + x[3,1] + x[3,2] \u0026lt;= 2; subject to subtours[9]: x[1,4] + x[4,1] \u0026lt;= 1; subject to subtours[10]: x[2,4] + x[4,2] \u0026lt;= 1; subject to subtours[11]: x[1,2] + x[1,4] + x[2,1] + x[2,4] + x[4,1] + x[4,2] \u0026lt;= 2; subject to subtours[12]: x[3,4] + x[4,3] \u0026lt;= 1; subject to subtours[13]: x[1,3] + x[1,4] + x[3,1] + x[3,4] + x[4,1] + x[4,3] \u0026lt;= 2; subject to subtours[14]: x[2,3] + x[2,4] + x[3,2] + x[3,4] + x[4,2] + x[4,3] \u0026lt;= 2; While this does work, the code for generating the power set looks like voodoo. Understanding it required piece-by-piece decomposition, an exercise I suggest you go through yourself if you have a copy of AMPL and 15 minutes to spare:\nset foo {c in cities} := {ord(c)}; set bar {c in cities} := {2^(ord(c) - 1)}; set baz {i in indices} := {c in cities: i div 2^(ord(c) - 1)}; set qux {i in indices} := {c in cities: (i div 2^(ord(c) - 1)) mod 2 = 1}; display foo; display bar; display baz; display qux; This may be an instance where open source leads commercial software. The good folks who produce the SCIP Optimization Suite provide an AMPL-like language called ZIMPL with a few additional useful features. One of these is power sets. Compared to the code above, doesn\u0026rsquo;t this look refreshing?\nset cities := {1 to 4}; set power[] := powerset(cities); set indices := indexset(power); ","permalink":"https://ryanjoneil.github.io/posts/2009-02-27-on-the-beauty-of-power-sets/","summary":"One of the difficulties we encounter in solving the Traveling Salesman Problem (TSP) is that, for even a small numer of cities, a complete description of the problem requires a factorial number of constraints. This is apparent in the standard formulation used to teach the TSP to OR students. Consider a set of $n$ cities with the distance from city $i$ to city $j$ denoted $d_{ij}$. We attempt to minimize the total distance of a tour entering and leaving each city exactly once.","title":"‚ö°Ô∏è On the Beauty of Power Sets"},{"content":"Uncapacitated Lot Sizing (ULS) is a classic OR problem that seeks to minimize the cost of satisfying known demand for a product over time. Demand is subject to varying costs for production, set-up, and storage of the product. Technically, it is a mixed binary integer linear program \u0026ndash; the key point separating it from the world of linear optimization being that production cannot occur during any period without paying that period\u0026rsquo;s fixed costs for set-up. Thus it has linear nonnegative variables for production and storage amounts during each period, and a binary variable for each period that determines whether or not production can actually occur.\nFor $n$ periods with per-period fixed set-up cost $f_t$, unit production cost $p_t$, unit storage cost $h_t$,and demand $d_t$, we define decision variables related to production and storage quantities:\n$$ \\small \\begin{align*} x_t \u0026amp;= \\text{units produced in period}\\ t\\\\ s_t \u0026amp;= \\text{stock at the end of period}\\ t\\\\ y_t \u0026amp;= 1\\ \\text{if production occurs in period}\\ t, 0\\ \\text{otherwise} \\end{align*} $$\nOne can minimize overall cost for satisfying all demand on time using the following model per Wolsey (1998), defined slightly differently here:\n$$ \\small \\begin{align*} \\min\\quad \u0026amp; z = \\sum_t{p_t x_t} + \\sum_t{h_t s_t} + \\sum_t{f_t y_t}\\\\ \\text{s.t.}\\quad\u0026amp; s_1 = d_1 + s_1\\\\ \u0026amp; s_{t-1} + x_t = d_t + s_t \u0026amp;\\quad\\forall\u0026amp;\\ t \u0026gt; 1\\\\ \u0026amp; x_t \\leq M y_t \u0026amp;\\quad\\forall\u0026amp;\\ t\\\\ \u0026amp; s_t, x_t \\geq 0 \u0026amp;\\quad\\forall\u0026amp;\\ t\\\\ \u0026amp; y_t \\in {0,1} \u0026amp;\\quad\\forall\u0026amp;\\ t \\end{align*} $$\nAccording to Wolsey, page 11, given that $s_t = \\sum_{i=1}^t (x_i - d_i)$ and defining new constants $K = \\sum_{t=1}^n h_t(\\sum_{i=1}^t d_i)$ and $c_t = p_t + \\sum_{i=t}^n h_i$, the objective function can be rewritten as $z = \\sum_t c_t x_t + \\sum _t f_t y_t - K$. The book lacks a proof of this and it seems a bit non-obvious, so I attempt an explanation in somewhat painstaking detail here.\n$$ \\small \\begin{align*} \u0026amp;\\text{Proof}:\\\\ \u0026amp; \u0026amp; \\sum_t p_t x_t + \\sum_t h_t s_t + \\sum_t f_t y_t \u0026amp;= \\sum_t c_t x_t + \\sum _t f_t y_t - K\\\\ \u0026amp;\\text{1. Remove} \\sum_t f_t y_t:\\\\ \u0026amp; \u0026amp; \\sum_t p_t x_t + \\sum_t h_t s_t \u0026amp;= \\sum_t c_t x_t - K\\\\ \u0026amp;\\text{2. Expand}\\ K\\ \\text{and}\\ c_t:\\\\ \u0026amp; \u0026amp; \\sum_t p_t x_t + \\sum_t h_t s_t \u0026amp;= \\sum_t (p_t + \\sum_{i=t}^n h_i) x_t - \\sum_t h_t (\\sum_{i=1}^t d_i)\\\\ \u0026amp;\\text{3. Remove}\\ \\sum_t p_t x_t:\\\\ \u0026amp; \u0026amp; \\sum_t h_t s_t \u0026amp;= \\sum_t x_t (\\sum_{i=t}^n h_i) - \\sum_t h_t (\\sum_{i=1}^t d_i)\\\\ \u0026amp;\\text{4. Expand}\\ s_t:\\\\ \u0026amp; \u0026amp; \\sum_t h_t (\\sum_{i=1}^t x_i) - \\sum_t h_t (\\sum_{i=1}^t d_i) \u0026amp;= \\sum_t x_t (\\sum_{i=t}^n h_i) - \\sum_t h_t (\\sum_{i=1}^t d_i)\\\\ \u0026amp;\\text{5. Remove}\\ \\sum_t h_t (\\sum_{i=1}^t d_i):\\\\ \u0026amp; \u0026amp; \\sum_t h_t (\\sum_{i=1}^t x_i) \u0026amp;= \\sum_t x_t (\\sum_{i=t}^n h_i) \\end{align*} $$\nThe result from step 5 becomes obvious upon expanding its left and right-hand terms:\n$$ h_1 x_1 + h_2 (x_1 + x_2) + \\cdots + h_n (x_1 + \\cdots + x_n) =\\\\ x_1 (h_1 + \\cdots + h_n) + x2 (h_2 + \\cdots + h_n) + \\cdots + x_n h_n $$\nIn matrix notation with $h$ and $x$ as column vectors in $\\bf R^n$ and $L$ and $U$ being $n \\times n$ lower and upper triangular identity matrices, respectively, this can be written as:\n$$ \\small \\begin{pmatrix} h_1 \\cdots h_n \\end{pmatrix} \\begin{pmatrix} 1 \\cdots 0 \\\\ \\vdots \\ddots \\vdots \\\\ 1 \\cdots 1 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{pmatrix} = \\begin{pmatrix} x_1 \\cdots x_n \\end{pmatrix} \\begin{pmatrix} 1 \\cdots 1 \\\\ \\vdots \\ddots \\vdots \\\\ 0 \\cdots 1 \\end{pmatrix} \\begin{pmatrix} h_1 \\\\ \\vdots \\\\ h_n \\end{pmatrix} $$\nor $h^T L x = x^T U h$.\n","permalink":"https://ryanjoneil.github.io/posts/2009-02-20-uncapacitated-lot-sizing/","summary":"Uncapacitated Lot Sizing (ULS) is a classic OR problem that seeks to minimize the cost of satisfying known demand for a product over time. Demand is subject to varying costs for production, set-up, and storage of the product. Technically, it is a mixed binary integer linear program \u0026ndash; the key point separating it from the world of linear optimization being that production cannot occur during any period without paying that period\u0026rsquo;s fixed costs for set-up.","title":"üìê Uncapacitated Lot Sizing"}]