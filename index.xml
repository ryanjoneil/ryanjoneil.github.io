<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>adventures in optimization</title>
    <link>https://ryanjoneil.github.io/</link>
    <description>Recent content on adventures in optimization</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 13 Sep 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://ryanjoneil.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>üñç Visualizing Decision Diagrams</title>
      <link>https://ryanjoneil.github.io/posts/2023-09-13-visualizing-decision-diagrams/</link>
      <pubDate>Wed, 13 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://ryanjoneil.github.io/posts/2023-09-13-visualizing-decision-diagrams/</guid>
      <description>A look at a few tools for visualizing decision diagrams</description>
      <content:encoded><![CDATA[<p>I attended <a href="https://sites.google.com/view/dpsolve2023/">DPSOLVE 2023</a> recently and found lots of good inspiration for the next version of Nextmv&rsquo;s Decision Diagram (DD) solver, <a href="https://www.nextmv.io/blog/how-hop-hops">Hop</a>. It&rsquo;s a few years old now, and we learned a lot applying it in the field. Hop formed the basis for our first routing models. While those models moved to a different structure in our <a href="https://www.nextmv.io/docs/vehicle-routing/get-started">latest routing code</a>, the <a href="https://www.nextmv.io/docs/vehicle-routing/legacy/routing">first version</a> broke ground combining DDs with Adaptive Large Neighborhood Search (ALNS), and its use continues to grow organically.</p>
<p>A feature I&rsquo;d love for Hop is the ability to visualize DDs and monitor the search. That could work interactively, like Gecode&rsquo;s <a href="https://researchmgt.monash.edu/ws/portalfiles/portal/257776846/3566886_oa.pdf">GIST</a>, or passively during the search process. This requires automatic generation of images representing potentially large diagrams. So I spent a few hours looking at graph rendering options for DDs.</p>
<h2 id="manual-rendering">Manual rendering</h2>
<p>We&rsquo;ll start with examples of visualizations built by hand. These form a good standard for how we want DDs to look if we automate rendering. We&rsquo;ll start with some examples from academic literature, look at some we&rsquo;ve used in <a href="https://nextmv.io">Nextmv</a> presentations, and show an interesting example that embeds in <a href="https://gohugo.io/">Hugo</a>, the popular static site generator I use for this blog.</p>
<p>All the literature on using Decision Diagrams (DD) for optimization that I&rsquo;m aware of depicts DDs as top-down, layered, directed graphs (digraphs). Some of the diagrams we come across appear to be coded and rendered, while some are fussily created by hand with a diagramming tool.</p>
<h3 id="academia">Academia</h3>
<p>I believe most of of the examples we find in academic literature are coded by hand and rendered using the LaTeX <a href="https://tikz.net/">TikZ</a> package. Below is one of the first diagrams that newcomers to DDs encounter. It&rsquo;s from <a href="https://link.springer.com/book/10.1007/978-3-319-42849-9">Decision Diagrams for Optimization</a> by Bergman et al, 2016.</p>
<p><img loading="lazy" src="/files/2023-09-13-visualizing-decision-diagrams/ddo-exact-relaxed.png" alt="Exact &amp;amp; relaxed BDDs"  />
</p>
<p>It doesn&rsquo;t matter here what model this represents. It&rsquo;s a Binary Decision Diagram (BDD), which means that each variable can be $0$ or $1$. The BDD on the left is exact, while the BDD on the right is a relaxed version of the same.</p>
<p>There&rsquo;s quite a bit going on, so it&rsquo;s worth an explanation. Let&rsquo;s look at the &ldquo;exact&rdquo; BDD on the left first.</p>
<ul>
<li>Horizontal layers group arcs with a binary variable (e.g. $x_1$, $x_2$).</li>
<li>Arcs assign either the value $0$ or $1$ to their layer&rsquo;s variable. Dotted lines assign $0$ while solid lines assign $1$.</li>
<li>Arc labels specify their costs. The BDD searches for a longest (or shortest) path from the root node $r$ to the terminal node $t$.</li>
</ul>
<p>The &ldquo;relaxed&rdquo; BDD on the right <em>overapproximates</em> both the objective value and the set of feasible solutions of the exact BDD on the left.</p>
<ul>
<li>The diagram is limited to a fixed width (2, in this case) at each layer.</li>
<li>The achieve this, the DD merges exact nodes together.</li>
<li>Thus, on the left of the relaxed BDD, there is a single node in which $x_2$ can be $0$ or $1$.</li>
</ul>
<p>Here&rsquo;s another example of an exact BDD from the same book.</p>
<p><img loading="lazy" src="/files/2023-09-13-visualizing-decision-diagrams/ddo-exact.png" alt="Exact BDD"  />
</p>
<p>In this diagram, each node has a state. For example, the state of $r$ is $\{1,2,3,4,5\}$. If we start at the root node $r$ and assign $x_1 = 0$, we end up at node $u_1$ with state $\{2,3,4,5\}$.</p>
<p>Most other academic literature about DDs uses images similar to these.</p>
<h3 id="nextmv">Nextmv</h3>
<p>We&rsquo;ve rendered a number of DDs over the years at <a href="https://nextmv.io">Nextmv</a>. Most of these images demonstrate a concept instead of a particular model. We usually create them by hand in a diagramming tool like <a href="https://whimsical.com/">Whimsical</a>, <a href="https://www.lucidchart.com/">Lucidchart</a>, or <a href="https://excalidraw.com/">Excalidraw</a>. I built the diagrams below by hand in Whimsical. I think the result is nice, if time consuming and fussy.</p>
<p>This is a representation of an exact DD. It doesn&rsquo;t indicate whether this is a BDD or a Multivalued Decision Diagram (MDD). It doesn&rsquo;t have any labels or variable names. It just shows what a DD search might look like in the abstract.</p>
<p><img loading="lazy" src="/files/2023-09-13-visualizing-decision-diagrams/nextmv-dd-exact.png" alt="Exact DD"  />
</p>
<p>The restricted DD below is more involved. It addition to horizontal layers, it divides nodes into explored and deferred groups. Most of the examples I&rsquo;ve seen mix different types of nodes, like exact and relaxed. I really like differentiating node types like this.</p>
<p>In this representation, deferred nodes are in Hop&rsquo;s queue for later exploration. Thus they don&rsquo;t connect to any child nodes yet. This is the kind of thing I&rsquo;d like to generate with real diagrams during search so I can examine the state of the solver.</p>
<p><img loading="lazy" src="/files/2023-09-13-visualizing-decision-diagrams/nextmv-dd-restricted.png" alt="Restricted DD"  />
</p>
<p>My favorite of my DD renderings so far is the next one. This shows a single-vehicle pickup-and-delivery problem. The arc labels are stops (e.g. üê∂, üê±). The path the üöó follows to the terminal node is the route. The gray boxes group together nodes to merge based on state to reduce isomorphisms out of the diagram.</p>
<p><img loading="lazy" src="/files/2023-09-13-visualizing-decision-diagrams/nextmv-pdp-reduced.png" alt="Reduced MDD"  />
</p>
<p>We also have some images like those in our <a href="https://www.nextmv.io/blog/introducing-expanders-in-hop">post on expanders</a> by hand. As you can see, coding these by hand gets tedious.</p>
<h3 id="goat">GoAT</h3>
<p>TikZ is a program that renders manually coded graphics, while Whimsical is a WYSIWYG diagram editor. I like the Whimsical images a lot better &ndash; they feel cleaner and easier to understand.</p>
<p>Hugo <a href="https://gohugo.io/content-management/diagrams/#goat-diagrams-ascii">supports GoAT diagrams</a> by default, so I tried that out too. Here is an arbitrary MDD with two layers. The $[[1,2],4]$ node is a relaxed node; it doesn&rsquo;t really matter here what the label means.</p>



<div class="goat svg-container ">
  
    <svg
      xmlns="http://www.w3.org/2000/svg"
      font-family="Menlo,Lucida Console,monospace"
      
        viewBox="0 0 424 329"
      >
      <g transform='translate(8,16)'>
<path d='M 160,16 L 248,16' fill='none' stroke='currentColor'></path>
<path d='M 280,16 L 368,16' fill='none' stroke='currentColor'></path>
<path d='M 232,80 L 296,80' fill='none' stroke='currentColor'></path>
<path d='M 232,112 L 264,112' fill='none' stroke='currentColor'></path>
<path d='M 264,112 L 296,112' fill='none' stroke='currentColor'></path>
<path d='M 168,144 L 264,144' fill='none' stroke='currentColor'></path>
<path d='M 144,192 L 152,192' fill='none' stroke='currentColor'></path>
<path d='M 256,192 L 264,192' fill='none' stroke='currentColor'></path>
<path d='M 376,192 L 392,192' fill='none' stroke='currentColor'></path>
<path d='M 144,224 L 152,224' fill='none' stroke='currentColor'></path>
<path d='M 256,224 L 264,224' fill='none' stroke='currentColor'></path>
<path d='M 376,224 L 384,224' fill='none' stroke='currentColor'></path>
<path d='M 384,224 L 392,224' fill='none' stroke='currentColor'></path>
<path d='M 168,288 L 240,288' fill='none' stroke='currentColor'></path>
<path d='M 288,288 L 368,288' fill='none' stroke='currentColor'></path>
<path d='M 144,32 L 144,64' fill='none' stroke='currentColor'></path>
<path d='M 152,160 L 152,176' fill='none' stroke='currentColor'></path>
<path d='M 152,224 L 152,272' fill='none' stroke='currentColor'></path>
<path d='M 264,16 L 264,32' fill='none' stroke='currentColor'></path>
<path d='M 264,32 L 264,64' fill='none' stroke='currentColor'></path>
<path d='M 264,112 L 264,144' fill='none' stroke='currentColor'></path>
<path d='M 264,144 L 264,176' fill='none' stroke='currentColor'></path>
<path d='M 264,224 L 264,256' fill='none' stroke='currentColor'></path>
<path d='M 384,32 L 384,64' fill='none' stroke='currentColor'></path>
<path d='M 384,112 L 384,176' fill='none' stroke='currentColor'></path>
<path d='M 384,224 L 384,272' fill='none' stroke='currentColor'></path>
<path d='M 144,64 L 144,72' fill='none' stroke='currentColor'></path>
<polygon points='160.000000,64.000000 148.000000,58.400002 148.000000,69.599998' fill='currentColor' transform='rotate(90.000000, 144.000000, 64.000000)'></polygon>
<path d='M 152,176 L 152,184' fill='none' stroke='currentColor'></path>
<polygon points='168.000000,176.000000 156.000000,170.399994 156.000000,181.600006' fill='currentColor' transform='rotate(90.000000, 152.000000, 176.000000)'></polygon>
<polygon points='248.000000,288.000000 236.000000,282.399994 236.000000,293.600006' fill='currentColor' transform='rotate(0.000000, 240.000000, 288.000000)'></polygon>
<path d='M 264,64 L 264,72' fill='none' stroke='currentColor'></path>
<polygon points='280.000000,64.000000 268.000000,58.400002 268.000000,69.599998' fill='currentColor' transform='rotate(90.000000, 264.000000, 64.000000)'></polygon>
<path d='M 264,176 L 264,184' fill='none' stroke='currentColor'></path>
<polygon points='280.000000,176.000000 268.000000,170.399994 268.000000,181.600006' fill='currentColor' transform='rotate(90.000000, 264.000000, 176.000000)'></polygon>
<path d='M 264,256 L 264,264' fill='none' stroke='currentColor'></path>
<polygon points='280.000000,256.000000 268.000000,250.399994 268.000000,261.600006' fill='currentColor' transform='rotate(90.000000, 264.000000, 256.000000)'></polygon>
<polygon points='296.000000,288.000000 284.000000,282.399994 284.000000,293.600006' fill='currentColor' transform='rotate(180.000000, 288.000000, 288.000000)'></polygon>
<path d='M 384,64 L 384,72' fill='none' stroke='currentColor'></path>
<polygon points='400.000000,64.000000 388.000000,58.400002 388.000000,69.599998' fill='currentColor' transform='rotate(90.000000, 384.000000, 64.000000)'></polygon>
<path d='M 384,176 L 384,184' fill='none' stroke='currentColor'></path>
<polygon points='400.000000,176.000000 388.000000,170.399994 388.000000,181.600006' fill='currentColor' transform='rotate(90.000000, 384.000000, 176.000000)'></polygon>
<path d='M 264,0 A 16,16 0 0,0 248,16' fill='none' stroke='currentColor'></path>
<path d='M 264,0 A 16,16 0 0,1 280,16' fill='none' stroke='currentColor'></path>
<path d='M 160,16 A 16,16 0 0,0 144,32' fill='none' stroke='currentColor'></path>
<path d='M 368,16 A 16,16 0 0,1 384,32' fill='none' stroke='currentColor'></path>
<path d='M 248,16 A 16,16 0 0,0 264,32' fill='none' stroke='currentColor'></path>
<path d='M 280,16 A 16,16 0 0,1 264,32' fill='none' stroke='currentColor'></path>
<path d='M 144,80 A 16,16 0 0,0 128,96' fill='none' stroke='currentColor'></path>
<path d='M 144,80 A 16,16 0 0,1 160,96' fill='none' stroke='currentColor'></path>
<path d='M 232,80 A 16,16 0 0,0 216,96' fill='none' stroke='currentColor'></path>
<path d='M 296,80 A 16,16 0 0,1 312,96' fill='none' stroke='currentColor'></path>
<path d='M 384,80 A 16,16 0 0,0 368,96' fill='none' stroke='currentColor'></path>
<path d='M 384,80 A 16,16 0 0,1 400,96' fill='none' stroke='currentColor'></path>
<path d='M 128,96 A 16,16 0 0,0 144,112' fill='none' stroke='currentColor'></path>
<path d='M 160,96 A 16,16 0 0,1 144,112' fill='none' stroke='currentColor'></path>
<path d='M 216,96 A 16,16 0 0,0 232,112' fill='none' stroke='currentColor'></path>
<path d='M 312,96 A 16,16 0 0,1 296,112' fill='none' stroke='currentColor'></path>
<path d='M 368,96 A 16,16 0 0,0 384,112' fill='none' stroke='currentColor'></path>
<path d='M 400,96 A 16,16 0 0,1 384,112' fill='none' stroke='currentColor'></path>
<path d='M 168,144 A 16,16 0 0,0 152,160' fill='none' stroke='currentColor'></path>
<path d='M 144,192 A 16,16 0 0,0 128,208' fill='none' stroke='currentColor'></path>
<path d='M 152,192 A 16,16 0 0,1 168,208' fill='none' stroke='currentColor'></path>
<path d='M 256,192 A 16,16 0 0,0 240,208' fill='none' stroke='currentColor'></path>
<path d='M 264,192 A 16,16 0 0,1 280,208' fill='none' stroke='currentColor'></path>
<path d='M 376,192 A 16,16 0 0,0 360,208' fill='none' stroke='currentColor'></path>
<path d='M 392,192 A 16,16 0 0,1 408,208' fill='none' stroke='currentColor'></path>
<path d='M 128,208 A 16,16 0 0,0 144,224' fill='none' stroke='currentColor'></path>
<path d='M 168,208 A 16,16 0 0,1 152,224' fill='none' stroke='currentColor'></path>
<path d='M 240,208 A 16,16 0 0,0 256,224' fill='none' stroke='currentColor'></path>
<path d='M 280,208 A 16,16 0 0,1 264,224' fill='none' stroke='currentColor'></path>
<path d='M 360,208 A 16,16 0 0,0 376,224' fill='none' stroke='currentColor'></path>
<path d='M 408,208 A 16,16 0 0,1 392,224' fill='none' stroke='currentColor'></path>
<path d='M 264,272 A 16,16 0 0,0 248,288' fill='none' stroke='currentColor'></path>
<path d='M 264,272 A 16,16 0 0,1 280,288' fill='none' stroke='currentColor'></path>
<path d='M 152,272 A 16,16 0 0,0 168,288' fill='none' stroke='currentColor'></path>
<path d='M 384,272 A 16,16 0 0,1 368,288' fill='none' stroke='currentColor'></path>
<path d='M 248,288 A 16,16 0 0,0 264,304' fill='none' stroke='currentColor'></path>
<path d='M 280,288 A 16,16 0 0,1 264,304' fill='none' stroke='currentColor'></path>
<circle cx='264' cy='16' r='6' stroke='currentColor' fill='#fff'></circle>
<circle cx='264' cy='288' r='6' stroke='currentColor' fill='currentColor'></circle>
<text text-anchor='middle' x='64' y='100' fill='currentColor' style='font-size:1em'>x</text>
<text text-anchor='middle' x='64' y='212' fill='currentColor' style='font-size:1em'>x</text>
<text text-anchor='middle' x='72' y='100' fill='currentColor' style='font-size:1em'>1</text>
<text text-anchor='middle' x='72' y='212' fill='currentColor' style='font-size:1em'>2</text>
<text text-anchor='middle' x='144' y='100' fill='currentColor' style='font-size:1em'>0</text>
<text text-anchor='middle' x='144' y='212' fill='currentColor' style='font-size:1em'>1</text>
<text text-anchor='middle' x='152' y='212' fill='currentColor' style='font-size:1em'>0</text>
<text text-anchor='middle' x='232' y='100' fill='currentColor' style='font-size:1em'>[</text>
<text text-anchor='middle' x='240' y='100' fill='currentColor' style='font-size:1em'>[</text>
<text text-anchor='middle' x='248' y='100' fill='currentColor' style='font-size:1em'>1</text>
<text text-anchor='middle' x='256' y='100' fill='currentColor' style='font-size:1em'>,</text>
<text text-anchor='middle' x='256' y='212' fill='currentColor' style='font-size:1em'>2</text>
<text text-anchor='middle' x='264' y='100' fill='currentColor' style='font-size:1em'>2</text>
<text text-anchor='middle' x='264' y='212' fill='currentColor' style='font-size:1em'>0</text>
<text text-anchor='middle' x='272' y='100' fill='currentColor' style='font-size:1em'>]</text>
<text text-anchor='middle' x='280' y='100' fill='currentColor' style='font-size:1em'>,</text>
<text text-anchor='middle' x='288' y='100' fill='currentColor' style='font-size:1em'>4</text>
<text text-anchor='middle' x='296' y='100' fill='currentColor' style='font-size:1em'>]</text>
<text text-anchor='middle' x='376' y='212' fill='currentColor' style='font-size:1em'>1</text>
<text text-anchor='middle' x='384' y='100' fill='currentColor' style='font-size:1em'>3</text>
<text text-anchor='middle' x='384' y='212' fill='currentColor' style='font-size:1em'>0</text>
<text text-anchor='middle' x='392' y='212' fill='currentColor' style='font-size:1em'>0</text>
</g>

    </svg>
  
</div>
<p>I like the way GoAT renders this diagram. It&rsquo;s very readable. Unfortunately, it isn&rsquo;t easy to automate. Creating a GoAT diagram is like using ASCII as a WYSIWYG diagramming tool, as you can see from the code for that image.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-txt" data-lang="txt"><span style="display:flex;"><span>                                .-.
</span></span><span style="display:flex;"><span>                   .-----------+ o +-----------.
</span></span><span style="display:flex;"><span>                  |             &#39;+&#39;             |
</span></span><span style="display:flex;"><span>                  |              |              |
</span></span><span style="display:flex;"><span>                  v              v              v
</span></span><span style="display:flex;"><span>                 .-.        .---------.        .-.
</span></span><span style="display:flex;"><span>        x1      | 0 |      | [[1,2],4] |      | 3 |
</span></span><span style="display:flex;"><span>                 &#39;-&#39;        &#39;----+----&#39;        &#39;+&#39;
</span></span><span style="display:flex;"><span>                                 |              |
</span></span><span style="display:flex;"><span>                    .------------+              |
</span></span><span style="display:flex;"><span>                   |             |              |
</span></span><span style="display:flex;"><span>                   v             v              v
</span></span><span style="display:flex;"><span>                 .--.          .--.           .---.
</span></span><span style="display:flex;"><span>        x2      | 10 |        | 20 |         | 100 |
</span></span><span style="display:flex;"><span>                 &#39;-+&#39;          &#39;-+&#39;           &#39;-+-&#39;
</span></span><span style="display:flex;"><span>                   |             |              |
</span></span><span style="display:flex;"><span>                   |             v              |
</span></span><span style="display:flex;"><span>                   |            .-.             |
</span></span><span style="display:flex;"><span>                    &#39;---------&gt;| * |&lt;----------&#39;
</span></span><span style="display:flex;"><span>                                &#39;-&#39;
</span></span></code></pre></div><h2 id="automated-rendering">Automated rendering</h2>
<p>Now we&rsquo;ll look at a couple options for automatically generating visualizations of DDs. These convert descriptions of graphs into images.</p>
<h3 id="graphviz">Graphviz</h3>
<p><a href="https://graphviz.org/">Graphviz</a> is the tried and true graph visualizer. It&rsquo;s used in the <a href="https://go.dev/blog/pprof">Go <code>pprof</code></a> library for examining CPU and memory profiles, and lots of other places.</p>
<p>Graphviz accepts a language called <a href="https://graphviz.org/doc/info/lang.html">DOT</a>. It uses different <a href="https://graphviz.org/docs/layouts/">layout engines</a> to convert input into a visual representation. The user doesn&rsquo;t have control over node position. That&rsquo;s the job of the layout engine.</p>
<p>Here&rsquo;s the same MDD as written in DOT. The <code>start -&gt; end</code> lines specify arcs in the digraph. The subgraphs organize nodes into layers. We add a dotted border around each layer and a label to say which variable it assigns. There isn&rsquo;t any way of vertically centering and horizontally aligning the layer labels, so I thought it make more sense this way.</p>
<pre tabindex="0"><code class="language-dot" data-lang="dot">digraph G {
    s1 [label = 0]
    s2 [label = &#34;[[1,2],4]&#34;]
    s3 [label = 3]
    s4 [label = 10]
    s5 [label = 20]
    s6 [label = 100]

    r -&gt; s1 [label = 2]
    r -&gt; s2 [label = 4]
    r -&gt; s3 [label = 1]
    s2 -&gt; s4 [label = 10]
    s2 -&gt; s5 [label = 4]
    s3 -&gt; s6 [label = 2]

    subgraph cluster_0 {
        label = &#34;x1&#34;
        labeljust = &#34;l&#34;
        style = &#34;dotted&#34;
        s1
        s2
        s3
    }

    subgraph cluster_1 {
        label = &#34;x2&#34;
        labeljust = &#34;l&#34;
        style = &#34;dotted&#34;
        s4
        s5
        s6
    }

    s4 -&gt; t
    s5 -&gt; t
    s6 -&gt; t
}
</code></pre><p>The result is comprehensible if not very attractive. With some fiddling, it&rsquo;s possible to improve things like the spacing around arc labels. I couldn&rsquo;t figure out how to align the layer labels and boxes. It doesn&rsquo;t seem possible to move the relaxed nodes into their own column either, but that limitation isn&rsquo;t unique to Graphviz.</p>
<p><img loading="lazy" src="/files/2023-09-13-visualizing-decision-diagrams/graphviz-dd.png" alt="Graphviz DD"  />
</p>
<h3 id="mermaid">Mermaid</h3>
<p><a href="https://mermaid.js.org/">Mermaid</a> is a JavaScript library for diagramming and charting. One can use it on the web or, presumably, embed it in an application.</p>
<p>Mermaid is similar to Graphviz in many ways, but it supports more diagram types. The input for that MDD in Mermaid is a bit simpler. Labels go inside arcs (e.g. <code>-- 2 --&gt;</code>), and there are more sensible rendering defaults.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-txt" data-lang="txt"><span style="display:flex;"><span>graph TD
</span></span><span style="display:flex;"><span>    start((( )))
</span></span><span style="display:flex;"><span>    stop((( )))
</span></span><span style="display:flex;"><span>    A(0)
</span></span><span style="display:flex;"><span>    B(&#34;[[1,2],4]&#34;)
</span></span><span style="display:flex;"><span>    C(3)
</span></span><span style="display:flex;"><span>    D(10)
</span></span><span style="display:flex;"><span>    E(20)
</span></span><span style="display:flex;"><span>    F(100)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    start -- 2 --&gt; A
</span></span><span style="display:flex;"><span>    start -- 4 --&gt; B
</span></span><span style="display:flex;"><span>    start -- 1 --&gt; C
</span></span><span style="display:flex;"><span>    B -- 10 --&gt; D
</span></span><span style="display:flex;"><span>    B -- 4 --&gt; E
</span></span><span style="display:flex;"><span>    C -- 2 --&gt; F
</span></span><span style="display:flex;"><span>    D --&gt; stop
</span></span><span style="display:flex;"><span>    E --&gt; stop
</span></span><span style="display:flex;"><span>    F --&gt; stop
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    subgraph &#34;x1 &#34;
</span></span><span style="display:flex;"><span>        A; B; C
</span></span><span style="display:flex;"><span>    end
</span></span><span style="display:flex;"><span>    subgraph &#34;x2&#34;
</span></span><span style="display:flex;"><span>        D; E; F
</span></span><span style="display:flex;"><span>    end
</span></span></code></pre></div><p>The result has a lot of the same limitations as the Graphviz version, but it looks more like the GoAT version. The biggest problem, as we see below, is that it&rsquo;s not possible to left-align the layer labels. They can be obscured by arcs.</p>
<script type="module">
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
    let isDark = document.body.className.includes('dark');
    mermaid.initialize({theme: (isDark) ? 'dark' : 'neutral'});
</script>

<center>
    <div class="mermaid">
        
graph TD
    start((( )))
    stop((( )))
    A(0)
    B("[[1,2],4]")
    C(3)
    D(10)
    E(20)
    F(100)

    start -- 2 --> A
    start -- 4 --> B
    start -- 1 --> C
    B -- 10 --> D
    B -- 4 --> E
    C -- 2 --> F
    D --> stop
    E --> stop
    F --> stop

    subgraph "x1 "
        A; B; C
    end
    subgraph "x2"
        D; E; F
    end

    </div>
</center>
<p>This got me thinking that there isn&rsquo;t a strong reason DDs have to progress downward layer by layer. They could just as easily go from left to right. If we change the opening line from <code>graph TD</code> to <code>graph LR</code>, then we get the following image.</p>
<script type="module">
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
    let isDark = document.body.className.includes('dark');
    mermaid.initialize({theme: (isDark) ? 'dark' : 'neutral'});
</script>

<center>
    <div class="mermaid">
        
graph LR
    start((( )))
    stop((( )))
    A(0)
    B("[[1,2],4]")
    C(3)
    D(10)
    E(20)
    F(100)

    start -- 2 --> A
    start -- 4 --> B
    start -- 1 --> C
    B -- 10 --> D
    B -- 4 --> E
    C -- 2 --> F
    D --> stop
    E --> stop
    F --> stop

    subgraph "x1 "
        A; B; C
    end
    subgraph "x2"
        D; E; F
    end

    </div>
</center>
<p>I think that&rsquo;s pretty nice for a generated image.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>üöÄ Blogging is back, baby!</title>
      <link>https://ryanjoneil.github.io/posts/2023-09-07-blogging-is-back/</link>
      <pubDate>Thu, 07 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://ryanjoneil.github.io/posts/2023-09-07-blogging-is-back/</guid>
      <description>New adventures in optimization</description>
      <content:encoded><![CDATA[<p>I&rsquo;ve been a mostly absent blogger for the past few years. I could make excuses. They might sound like, &ldquo;I was busy finishing my dissertation!&rdquo; or &ldquo;I founded a company and have a toddler!&rdquo; or &ldquo;The <a href="https://github.com/eudicots/Cactus">static site generator</a> I used was abandoned!&rdquo; Whatever they might be, these excuses would certainly end in exclamation points.</p>
<p>But, ultimately, for several years it just felt like blogging was <em>dead</em>. Its space was usurped by Tweets, LinkedIn hustle posts, long form Medium content aimed at attracting talent, and other content trends. RSS feeds dried up bit by bit. That beautiful structure somewhere between a college essay and an academic preprint mostly ceased to be. Sad times, indeed.</p>
<p>That trend seems to be reversing. I don&rsquo;t know whether it&rsquo;s the result of nudges from Substack, or that all the introverts in tech finally gave up baking pandemic, but there is a lot of good content out there again! Fire up your <a href="https://www.inoreader.com/">RSS aggregators</a> and get reading.</p>
<p>This post is my own reboot of &ldquo;adventures in optimization,&rdquo; a blog I&rsquo;ve written intermittently since 2009. Unfortunately, it will take some time to move over all my old posts to Hugo. I&rsquo;ll do that slowly as I create new ones. For now, I&rsquo;ve ported over a couple early posts and put together a list of the active blogs I&rsquo;m gleefully catching up on.</p>
<p>See you soon!</p>
<ul>
<li><a href="https://hsimonis.wordpress.com/">Constraint Applications Blog</a> by Helmut Simonis</li>
<li><a href="https://erlingdandersen.blogspot.com/">Erling&rsquo;s blog</a></li>
<li><a href="https://eugeneyan.com/">eugeneyan</a></li>
<li><a href="https://www.harlan.harris.name/">Harlan D. Harris</a></li>
<li><a href="https://lilianweng.github.io/">Lil&rsquo;Log</a></li>
<li><a href="https://nathanbrixius.wordpress.com/">Nathan Brixius</a></li>
<li><a href="https://punkrockor.com/">Punk Rock Operations Research</a></li>
<li><a href="https://scottaaronson.blog/">Shtetl-Optimized</a></li>
<li><a href="https://timefold.ai/blog/">Timefold</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>üí¨ Some talk</title>
      <link>https://ryanjoneil.github.io/speaking/</link>
      <pubDate>Wed, 06 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://ryanjoneil.github.io/speaking/</guid>
      <description>This is a running list of talks I&amp;rsquo;ve given and am scheduled to give. It probably isn&amp;rsquo;t exhaustive. Some of them have slides or videos available.
Upcoming talks December 6-8, 2023 - PyData Global 2023 - üìÑ abstract, üéü registration
Order up! How do I deliver it? Build on-demand logistics apps with Python, OR-Tools, and DecisionOps
Past talks 2023 November 16, 2023 - Nextmv Videos - üéü registration
Forecast, schedule, route: 3 starter models for on-demand logistics</description>
      <content:encoded><![CDATA[<p>This is a running list of talks I&rsquo;ve given and am scheduled to give. It probably isn&rsquo;t exhaustive. Some of them have slides or videos available.</p>
<h2 id="upcoming-talks">Upcoming talks</h2>
<p>December 6-8, 2023 - <a href="https://pydata.org/global2023/">PyData Global 2023</a> - <a href="https://global2023.pydata.org/cfp/talk/review/QWNCD3B7EP8MR9BZMN7VVG8PG97ZG3L3">üìÑ abstract</a>, <a href="https://pydata.org/global2023/tickets">üéü registration</a><br>
Order up! How do I deliver it? Build on-demand logistics apps with Python, OR-Tools, and DecisionOps</p>
<h2 id="past-talks">Past talks</h2>
<h3 id="2023">2023</h3>
<p>November 16, 2023 - <a href="https://www.nextmv.io/videos">Nextmv Videos</a> - <a href="https://www.nextmv.io/videos/forecast-schedule-route-3-starter-models-for-on-demand-logistics">üéü registration</a><br>
Forecast, schedule, route: 3 starter models for on-demand logistics</p>
<p>October 17, 2023 - <a href="https://meetings.informs.org/wordpress/phoenix2023/">INFORMS Annual Meeting</a> - <a href="https://www.abstractsonline.com/pp8/?__hstc=194041586.cc5f446034c6d268dfea27d9b7ceb6dc.1681140481721.1691273766763.1691449438596.35&amp;__hssc=194041586.3.1691449438596&amp;__hsfp=1912911101&amp;hsCtaTracking=8f511889-324a-41b3-a438-37ad295392e9%7C0c80c5d7-cc8d-4989-9b70-52de4c44b90b#!/10856/presentation/7867">üìÑ abstract</a><br>
Adapting to Change in On-Demand Delivery: Unpacking a Suite of Testing Methodologies</p>
<p>September 20, 2023 - <a href="https://decisioncamp2023.wordpress.com/">DecisionCAMP 2023</a> - <a href="https://decisioncamp2023.wordpress.com/program/#NEXTMV">üìÑ abstract</a>, <a href="/slides/2023-decisioncamp.pdf">üíª slides</a>, <a href="https://www.youtube.com/watch?v=jh767AIuFP0">üé• video</a><br>
Decision model, meet the real world: Testing optimization models for use in production environments</p>
<p>August 27, 2023 - <a href="https://sites.google.com/view/dpsolve2023/">DPSOLVE 2023</a> - <a href="/slides/2023-dpsolve.pdf">üíª slides</a><br>
Implementing Decision Diagrams in Production Systems</p>
<p>May 11, 2023 - <a href="https://www.nextmv.io/videos">Nextmv Videos</a> - <a href="https://www.youtube.com/watch?v=8WkHltfgVPw">üé• video</a><br>
Several people are optimizing: Collaborative workflows for decision model operations</p>
<p>April 18, 2023 - <a href="https://meetings.informs.org/wordpress/analytics2023/">INFORMS Business Analytics Conference</a> - <a href="https://meetings.informs.org/wordpress/analytics2023/tracks/technology-tutorials/">üìÑ abstract</a><br>
Decision Model, Meet Production: A Collaborative Workflow for Optimizing More Operations</p>
<p>March 1, 2023 - <a href="https://www.nextmv.io/videos">Nextmv Videos</a> - <a href="https://www.youtube.com/watch?v=WtdUhYH5Szs">üé• video</a><br>
What are decision diagrams? How are they used to model and solve optimization problems?</p>
<p>January 18, 2023 - <a href="https://www.nextmv.io/videos">Nextmv Videos</a> - <a href="https://www.youtube.com/watch?v=p4R9cglMP4Y">üé• video</a><br>
In conversation with Karla Hoffman</p>
<h3 id="ancient-history">Ancient history</h3>
<p>October 5, 2020 - <a href="https://connect.informs.org/phillychapter/home">INFORMS Philadelphia Chapter</a> - <a href="https://youtu.be/t6rI2JFVqG4">üé• video</a><br>
Real-Time Routing for On-Demand Delivery</p>
<p>October 22, 2019 - INFORMS Annual Meeting - <a href="/slides/2019-informs.pdf">üíª slides</a><br>
Decision Diagrams for Real-Time Routing</p>
<p>July 6, 2017 - <a href="https://pydata.org/seattle2017/">PyData Seattle 2017</a> - <a href="https://pydata.org/seattle2017/schedule/presentation/81/">üìÑ abstract</a>, <a href="https://youtu.be/n9Fz74pFiA4?list=PLGVZCDnMOq0rxoq9Nx0B4tqtr891vaCn7">üé• video</a><br>
Practical Optimization for Stats Nerds</p>
<p>March 5, 2017 - <a href="https://www.meetup.com/Data-Science-DC/events/238107178/">Data Science DC</a> - <a href="https://nbviewer.org/github/ryanjoneil/talks/blob/master/2017/07/pydata-seattle/practical-optimization-for-stats-nerds.slides.html#/">üíª slides</a><br>
Practical Optimization for Stats Nerds</p>
<p>December 4, 2015 - PyData NYC 2015 - <a href="https://nbviewer.org/github/ryanjoneil/talks/blob/master/2015/11/pydata-nyc/optimize-your-docker-infrastructure-with-python.ipynb">üíª slides</a>, <a href="https://www.youtube.com/watch?v=l6NjecDJFb8">üé• video</a><br>
Optimize your Docker Infrastructure with Python</p>
<p>July 17, 2014 - IFORS 2014 - <a href="https://www.euro-online.org/conferences/program/#abstract/13132">üìÑ abstract</a>, <a href="/slides/2014-ifors.pdf">üíª slides</a><br>
A MIP-Based Dual Bounding Technique for the Irregular Nesting Problem</p>
<p>February 19, 2010 - <a href="https://pyvideo.org/events/pycon-us-2010.html">PyCon 2010</a> - <a href="https://pyvideo.org/pycon-us-2010/pycon-2010--optimal-resource-allocation-using-pyt.html">üé• video</a><br>
Optimal Resource Allocation using Python</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>üìù Some text</title>
      <link>https://ryanjoneil.github.io/writing/</link>
      <pubDate>Wed, 06 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://ryanjoneil.github.io/writing/</guid>
      <description>I&amp;rsquo;m an desultory blogger and intermittent academic. Nonetheless, I like the research and writing process. Most of my current and old posts live here. Some of my other writings and collaborations are listed below.
Papers, patents &amp;amp; preprints June 2023 - USPTO - üìÑ patent
Runners for optimization solvers and simulators describes technology for creating and executing Decision Diagram-based optimization solvers and state-based simulators in cloud environments.
September 2020 - Operations Research Forum - üìÑ preprint</description>
      <content:encoded><![CDATA[<p>I&rsquo;m an desultory blogger and intermittent academic. Nonetheless, I like the research and writing process. Most of my current and old posts live <a href="/posts">here</a>. Some of my other writings and collaborations are listed below.</p>
<h2 id="papers-patents--preprints">Papers, patents &amp; preprints</h2>
<p>June 2023 - <a href="https://www.uspto.gov/">USPTO</a> - <a href="https://patentimages.storage.googleapis.com/c6/08/09/060978f00373e0/US11675688.pdf">üìÑ patent</a><br>
<a href="https://patents.google.com/patent/US11675688B2/en">Runners for optimization solvers and simulators</a> describes technology for creating and executing Decision Diagram-based optimization solvers and state-based simulators in cloud environments.</p>
<p>September 2020 - <a href="https://www.springer.com/journal/43069">Operations Research Forum</a> - <a href="https://digitalcommons.bucknell.edu/cgi/viewcontent.cgi?article=2883&amp;context=fac_journ">üìÑ preprint</a><br>
<a href="https://link.springer.com/article/10.1007/s43069-020-00024-1">MIPLIBing: Seamless Benchmarking of Mathematical Optimization Problems and Metadata Extensions</a> presents a Python library that automatically downloads queried subsets from the current versions of MIPLIB, MINLPLib, and QPLIB, provides a centralized local cache across projects, and tracks the best solution values and bounds on record for each problem.</p>
<p>May 2019 - <a href="https://www.sciencedirect.com/journal/operations-research-letters">Operations Research Letters</a> - <a href="http://www.optimization-online.org/DB_FILE/2018/10/6853.pdf">üìÑ preprint</a><br>
<a href="https://www.sciencedirect.com/science/article/abs/pii/S0167637718305364">Decision diagrams for solving traveling salesman problems with pickup and delivery in real time</a> explores the use of Multivalued Decision Diagrams and Assignment Problem inference duals for real-time optimization of TSPPDs.</p>
<p>October 2018 - <a href="https://optimization-online.org/author/roneil1/">Optimization Online</a> - <a href="http://www.optimization-online.org/DB_FILE/2018/10/6838.pdf">üìÑ preprint</a><br>
<a href="https://optimization-online.org/2018/10/6838/">Integer Models for the Asymmetric Traveling Salesman Problem with Pickup and Delivery</a> proposes a new ATSPPD model, new valid inequalities for the Sarin-Sherali-Bhootra ATSPPD, and studies the impact of relaxing complicating constraints in these.</p>
<p>September 2018 - <a href="https://optimization-online.org/author/roneil1/">Optimization Online</a> - <a href="http://www.optimization-online.org/DB_FILE/2017/12/6370.pdf">üìÑ preprint</a><br>
<a href="http://www.optimization-online.org/DB_HTML/2017/12/6370.html">Exact Methods for Solving Traveling Salesman Problems with Pickup and Delivery in Real Time</a> examines exact methods for solving TSPPDs with consolidation in real-time applications. It considers enumerative, Mixed Integer Programming, Constraint Programming, and hybrid optimization approaches under various time budgets.</p>
<p>March 2018 - <a href="https://optimization-online.org/author/roneil1/">Optimization Online</a> - <a href="https://optimization-online.org/wp-content/uploads/2018/04/6571.pdf">üìÑ preprint</a><br>
<a href="https://optimization-online.org/2018/04/6571/">The Meal Delivery Routing Problem</a> introduces the MDRP to formalize and study an important emerging class of dynamic delivery operations. It also develops optimization-based algorithms tailored to solve the courier assignment (dynamic vehicle routing) and capacity management (offline shift scheduling) problems encountered in meal delivery operations.</p>
<h2 id="other-blogs">Other blogs</h2>
<p>April 20, 2022 - <a href="https://www.nextmv.io/blog">Nextmv Blog</a> -
<a href="https://www.nextmv.io/blog/you-need-a-solver-what-is-a-solver">You need a solver. What is a solver?</a></p>
<p>March 2, 2021 - <a href="https://www.nextmv.io/blog">Nextmv Blog</a> -
<a href="https://www.nextmv.io/blog/binaries-are-beautiful">Binaries are beautiful</a></p>
<p>March 2, 2020 - <a href="https://www.nextmv.io/blog">Nextmv Blog</a> -
<a href="https://www.nextmv.io/blog/how-hop-hops">How Hop Hops</a></p>
<p>September 13, 2018 - <a href="https://bytes.grubhub.com/">Grubhub Bytes</a> -
<a href="https://bytes.grubhub.com/decisions-are-first-class-citizens-9994fd8fe802">Decisions are first class citizens: an introduction to Decision Engineering</a></p>
<p>January 5, 2015 - <a href="https://web.archive.org/web/20161107120859/http://blog.yhat.com/">The Yhat Blog</a> -
<a href="https://web.archive.org/web/20160909032804/http://blog.yhat.com/posts/currency-portfolio-optimization-using-scienceops.html">Currency Portfolio Optimization Using ScienceOps</a></p>
<p>November 10, 2014 - <a href="https://web.archive.org/web/20161107120859/http://blog.yhat.com/">The Yhat Blog</a> -
<a href="https://web.archive.org/web/20160308065802/http://blog.yhat.com/posts/how-yhat-does-cloud-balancing.html">How Yhat Does Cloud Balancing: A Case Study</a></p>
]]></content:encoded>
    </item>
    
    <item>
      <title>üññ Hi, I&#39;m Ryan.</title>
      <link>https://ryanjoneil.github.io/about/</link>
      <pubDate>Wed, 06 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://ryanjoneil.github.io/about/</guid>
      <description>I build Optimization AI tools.
By day, I am an operations research analyst, Go coder, and co-founder of Nextmv. I&amp;rsquo;m interested in hybrid optimization, decision diagrams, and mixed integer programming. My applications skew toward logistics for delivery platforms, with detours into cutting and packing.
For the past several years, I&amp;rsquo;ve worked in real-time optimization for on-demand delivery, scheduling, forecasting, and simulation. I did a MS in Operations Research by night at George Mason University, then a PhD in the same department under the advisement of Karla Hoffman.</description>
      <content:encoded><![CDATA[<p><img loading="lazy" src="/img/ryan.jpg" alt="me"  />
</p>
<p>I build Optimization AI tools.</p>
<p>By day, I am an <a href="https://www.informs.org/Explore/Operations-Research-Analytics">operations research analyst</a>, <a href="https://go.dev">Go</a> coder, and co-founder of <a href="https://nextmv.io">Nextmv</a>. I&rsquo;m interested in hybrid optimization, <a href="https://www.andrew.cmu.edu/user/vanhoeve/mdd/">decision diagrams</a>, and <a href="https://en.wikipedia.org/wiki/Integer_programming">mixed integer programming</a>. My applications skew toward logistics for delivery platforms, with detours into <a href="https://www.euro-online.org/websites/esicup/">cutting and packing</a>.</p>
<p>For the past several years, I&rsquo;ve worked in real-time optimization for on-demand delivery, scheduling, forecasting, and simulation. I did a MS in Operations Research by night at <a href="https://seor.gmu.edu/">George Mason University</a>, then a PhD in the same department under the advisement of <a href="https://seor.vse.gmu.edu/~khoffman/">Karla Hoffman</a>.</p>
<p>By night, I am am amateur cellist, and a cat and early music enthusiast.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>üßë‚ÄçüíªÔ∏è Some code</title>
      <link>https://ryanjoneil.github.io/coding/</link>
      <pubDate>Wed, 06 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://ryanjoneil.github.io/coding/</guid>
      <description>Most of my work is proprietary, but some of it is open. Here are a few projects I&amp;rsquo;ve built or made significant contributions. I&amp;rsquo;ve also made significant contributions to projects such as PuLP, MIPLIBing, and MDRPlib.
Active projects The Ruby Algebraic Modeling System is a simple modeling tool for formulating and solving MILPs in Ruby.
ap.cpp is an incremental primal-dual assignment problem solver written in C++. It can vastly improve propagation in hybrid optimization models that use AP relaxations.</description>
      <content:encoded><![CDATA[<p>Most of my work is proprietary, but some of it is open. Here are a few projects I&rsquo;ve built or made significant contributions. I&rsquo;ve also made significant contributions to projects such as <a href="https://coin-or.github.io/pulp/">PuLP</a>, <a href="https://github.com/thserra/MIPLIBing">MIPLIBing</a>, and <a href="https://github.com/grubhub/mdrplib">MDRPlib</a>.</p>
<h2 id="active-projects">Active projects</h2>
<p>The <a href="https://github.com/ryanjoneil/rams">Ruby Algebraic Modeling System</a> is a simple modeling tool for formulating and solving MILPs in Ruby.</p>
<p><a href="https://github.com/ryanjoneil/ap.cpp">ap.cpp</a> is an incremental primal-dual assignment problem solver written in C++. It can vastly improve propagation in hybrid optimization models that use AP relaxations. I use it within custom propagators in <a href="https://www.gecode.org/">Gecode</a> and in Decision Diagrams for solving the Traveling Salesman Problem with side constraints.</p>
<p><a href="https://github.com/ryanjoneil/ap">ap</a> is a Go version of ap.cpp.</p>
<p><a href="https://github.com/ryanjoneil/tsppd-hybrid">TSPPD Hybrid Optimization Code</a> and <a href="https://github.com/ryanjoneil/tsppd-dd">TSPPD Decision Diagram Code</a> are both used in my dissertation. The former contains C++14 code for hybrid CP and MIP models for solving TSPPDs. The latter uses a hybridized Decision Diagram implementation with an Assignment Problem inference dual inside a branch-and-bound.</p>
<p><a href="https://github.com/grubhub/tsppdlib">TSPPDlib</a> is a standard test set for TSPPDs. The instances are based on observed meal delivery data at Grubhub.</p>
<h2 id="defunct-projects">Defunct projects</h2>
<p><a href="https://pythonhosted.org/python-zibopt/">python-zibopt</a> was a Python interface to the <a href="https://www.scipopt.org/">SCIP Optimization Suite</a>. This was no longer necessary once <a href="https://github.com/scipopt/PySCIPOpt">PySCIPOpt</a> emerged.</p>
<p><a href="https://github.com/ryanjoneil/chute">Chute</a> was a simple, lightweight tool for running discrete event simulations in Python.</p>
<p><a href="https://code.google.com/archive/p/pygep/">PyGEP</a> was a simple library suitable for academic study of GEP (Gene Expression Programming) in Python 2.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>üê™ Reformed JAPHs: Ridiculous Anagram</title>
      <link>https://ryanjoneil.github.io/posts/2011-04-03-reformed-japhs-ridiculous-anagram/</link>
      <pubDate>Sun, 03 Apr 2011 00:00:00 +0000</pubDate>
      
      <guid>https://ryanjoneil.github.io/posts/2011-04-03-reformed-japhs-ridiculous-anagram/</guid>
      <description>Python obfuscation using anagrams</description>
      <content:encoded><![CDATA[<p>Here&rsquo;s the second in my reformed JAPH series. It takes an anagram of <code>'just another python hacker'</code> and converts it prior to printing. It sorts the anagram by the indices of another string, in order of their associated characters. This is sort of like a pre-digested <a href="https://en.wikipedia.org/wiki/Schwartzian_transform">Schwartzian transform</a>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>x <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;upjohn tehran hectors katy&#39;</span>
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;1D0HG6JFO9P5ICKAM87B24NL3E&#39;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;&#39;</span><span style="color:#f92672">.</span>join(x[i] <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> sorted(range(len(x)), key<span style="color:#f92672">=</span><span style="color:#66d9ef">lambda</span> p: y[p])))
</span></span></code></pre></div><p>Obfuscation consists mostly of using silly machinations to construct the string we use to sort the anagram.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;&#39;</span><span style="color:#f92672">.</span>join(<span style="color:#e6db74">&#39;&#39;&#39;upjohn tehran hectors katy&#39;&#39;&#39;</span>[_]<span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> sorted(range
</span></span><span style="display:flex;"><span>(<span style="color:#ae81ff">26</span>),key<span style="color:#f92672">=</span><span style="color:#66d9ef">lambda</span> p:(hex(<span style="color:#ae81ff">29</span>)[<span style="color:#ae81ff">2</span>:]<span style="color:#f92672">.</span>upper()<span style="color:#f92672">+</span>str(<span style="color:#ae81ff">3</span><span style="color:#f92672">*</span><span style="color:#ae81ff">3</span><span style="color:#f92672">*</span><span style="color:#ae81ff">3</span><span style="color:#f92672">*</span><span style="color:#ae81ff">3</span><span style="color:#f92672">-</span><span style="color:#ae81ff">3</span><span style="color:#f92672">**</span><span style="color:#ae81ff">4</span>)<span style="color:#f92672">+</span><span style="color:#e6db74">&#39;HG&#39;</span><span style="color:#f92672">+</span>str(sum(
</span></span><span style="display:flex;"><span>range(<span style="color:#ae81ff">4</span>)))<span style="color:#f92672">+</span><span style="color:#e6db74">&#39;JFO&#39;</span><span style="color:#f92672">+</span>str((<span style="color:#ae81ff">1</span><span style="color:#f92672">+</span><span style="color:#ae81ff">2</span>)<span style="color:#f92672">**</span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>))<span style="color:#f92672">+</span><span style="color:#e6db74">&#39;P&#39;</span><span style="color:#f92672">+</span>str(<span style="color:#ae81ff">35</span><span style="color:#f92672">/</span><span style="color:#ae81ff">7</span>)[:<span style="color:#ae81ff">1</span>]<span style="color:#f92672">+</span><span style="color:#e6db74">&#39;i.c.k.&#39;</span><span style="color:#f92672">.</span>replace(
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;.&#39;</span>,<span style="color:#e6db74">&#39;&#39;</span>)<span style="color:#f92672">.</span>upper()<span style="color:#f92672">+</span><span style="color:#e6db74">&#39;AM&#39;</span><span style="color:#f92672">+</span>str(<span style="color:#ae81ff">3</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>sum(range(<span style="color:#ae81ff">5</span>))<span style="color:#f92672">-</span><span style="color:#ae81ff">3</span>)<span style="color:#f92672">+</span>hex(<span style="color:#ae81ff">0o5444</span>)[<span style="color:#ae81ff">2</span>:]<span style="color:#f92672">.</span>replace
</span></span><span style="display:flex;"><span>(<span style="color:#f92672">*</span><span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\x62</span><span style="color:#e6db74">|</span><span style="color:#ae81ff">\x42</span><span style="color:#e6db74">&#39;</span><span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#39;|&#39;</span>))<span style="color:#f92672">+</span><span style="color:#e6db74">&#39;NL&#39;</span><span style="color:#f92672">+</span>hex(<span style="color:#ae81ff">0o076</span>)<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#39;x&#39;</span>)[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>upper())[p])))
</span></span></code></pre></div>]]></content:encoded>
    </item>
    
    <item>
      <title>üê™ Reformed JAPHs: Alphabetic Indexing</title>
      <link>https://ryanjoneil.github.io/posts/2011-04-01-reformed-japhs-alphabetic-indexing/</link>
      <pubDate>Fri, 01 Apr 2011 00:00:00 +0000</pubDate>
      
      <guid>https://ryanjoneil.github.io/posts/2011-04-01-reformed-japhs-alphabetic-indexing/</guid>
      <description>Python obfuscation</description>
      <content:encoded><![CDATA[<p><em>Note: This post was edited for clarity.</em></p>
<p>Many years ago, I was a Perl programmer. Then one day I became disillusioned at the progress of Perl 6 and decided to <a href="https://www.python.org/dev/peps/pep-0020/">import this</a>.</p>
<p>This seems to be a fairly common story for Perl to Python converts. While I haven&rsquo;t looked back much, there are a number of things I really miss about <code>perl</code> <em>(lower case intentional)</em>. I miss having value types in a dynamic language, magical and ill-advised use of <a href="https://www.foo.be/docs/tpj/issues/vol3_1/tpj0301-0003.html">cryptocontext</a>, and sometimes even <a href="https://web.archive.org/web/20040712204117/https://perldesignpatterns.com/?PseudoHash">pseudohashes</a> because they were inexcusably weird. A language that supports so many ideas out of the box enables an extended learning curve that lasts for <a href="https://web.archive.org/web/20020607034341/https://silver.sucs.org/~manic/humour/languages/perlhacker.htm">many years</a>. &ldquo;Perl itself is the game.&rdquo;</p>
<p>Most of all I think I miss writing Perl <a href="https://www.perlmonks.org/?node=Perl%20Poetry">poetry</a> and <a href="https://en.wikipedia.org/wiki/Just_another_Perl_hacker">JAPHs</a>. Sadly, I didn&rsquo;t keep any of those I wrote, and I&rsquo;m not competent enough with the language anymore to write interesting ones. At the time I was intentionally distancing myself from a model that was largely implicit and based on archaic systems internals and moving to one that was (supposedly) explicit and simple.</p>
<p>After switching to Python as my primary language, I used the following email signature in a nod to this change in orientation <em>(intended for Python 2)</em>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>print <span style="color:#e6db74">&#39;just another python hacker&#39;</span>
</span></span></code></pre></div><p>Recently I&rsquo;ve been experimenting with writing JAPHs in Python. I think of these as &ldquo;reformed JAPHs.&rdquo; They accomplish the same purpose as programming exercises but in a more restricted context. In some ways they are more challenging. Creativity can be difficult in a narrowly defined landscape.</p>
<p>I have written a small series of reformed JAPHs which increase monotonically in complexity. Here is the first one, written in plain understandable Python 3.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> string
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>letters <span style="color:#f92672">=</span> string<span style="color:#f92672">.</span>ascii_lowercase <span style="color:#f92672">+</span> <span style="color:#e6db74">&#39; &#39;</span>
</span></span><span style="display:flex;"><span>indices <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>     <span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">20</span>, <span style="color:#ae81ff">18</span>, <span style="color:#ae81ff">19</span>, <span style="color:#ae81ff">26</span>,  <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">13</span>, <span style="color:#ae81ff">14</span>, <span style="color:#ae81ff">19</span>, <span style="color:#ae81ff">7</span>,  <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">17</span>, <span style="color:#ae81ff">26</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#ae81ff">15</span>, <span style="color:#ae81ff">24</span>, <span style="color:#ae81ff">19</span>,  <span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">14</span>, <span style="color:#ae81ff">13</span>, <span style="color:#ae81ff">26</span>,  <span style="color:#ae81ff">7</span>,  <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">10</span>,  <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">17</span>
</span></span><span style="display:flex;"><span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;&#39;</span><span style="color:#f92672">.</span>join(letters[i] <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> indices))
</span></span></code></pre></div><p>This is fairly simple. Instead of explicitly embedding the string <code>'just another python hacker'</code> in the program, we assemble it using the index of its letters in the string <code>'abcdefghijklmnopqrstuvwxyz '</code>. We then obfuscate through a series of minor measures:</p>
<ul>
<li>Instead of calling the print function, we <code>import sys</code> and make a call to <code>sys.stdout.write</code>.</li>
<li>We assemble <code>string.lowercase + ' '</code> by joining together the character versions of its respective ordinal values (97 to 123 and 32).</li>
<li>We join together the integer indices using <code>'l'</code> and split that into a list.</li>
<li>We apply <code>'''</code> liberally and rely on the fact that <code>python</code> concatenates adjacent strings.</li>
</ul>
<p>Here&rsquo;s the obfuscated version:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>eval(<span style="color:#e6db74">&#34;__import__(&#39;&#39;&#39;</span><span style="color:#ae81ff">\x73</span><span style="color:#e6db74">&#39;&#39;&#39;&#39;&#39;&#39;</span><span style="color:#ae81ff">\x79</span><span style="color:#e6db74">&#39;&#39;&#39;&#39;&#39;&#39;</span><span style="color:#ae81ff">\x73</span><span style="color:#e6db74">&#39;&#39;&#39;).sTdOuT&#34;</span><span style="color:#f92672">.</span>lower()
</span></span><span style="display:flex;"><span>)<span style="color:#f92672">.</span>write(<span style="color:#e6db74">&#39;&#39;</span><span style="color:#f92672">.</span>join(map(<span style="color:#66d9ef">lambda</span> _:(list(map(chr,range(<span style="color:#ae81ff">97</span>,<span style="color:#ae81ff">123</span>)))<span style="color:#f92672">+</span>[chr(
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">32</span>)])[int(_)],(<span style="color:#e6db74">&#39;&#39;&#39;9l20l18l19&#39;&#39;&#39;&#39;&#39;&#39;l26l0l13l14l19l7l4l17l26l15&#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;&#39;&#39;l24l19l7l14l1&#39;&#39;&#39;&#39;&#39;&#39;3l26l7l0l2l10l4l17&#39;&#39;&#39;</span>)<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#39;l&#39;</span>)))<span style="color:#f92672">+</span><span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>,)
</span></span></code></pre></div><p>We could certainly do more, but that&rsquo;s where I left this one. Stay tuned for the next JAPH.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>üìà Simulating GDP Growth</title>
      <link>https://ryanjoneil.github.io/posts/2011-02-23-simulating-gdp-growth/</link>
      <pubDate>Wed, 23 Feb 2011 00:00:00 +0000</pubDate>
      
      <guid>https://ryanjoneil.github.io/posts/2011-02-23-simulating-gdp-growth/</guid>
      <description>Writing and interpreting simulations about GDP growth in R</description>
      <content:encoded><![CDATA[<p>I hope you saw <a href="https://www.washingtonpost.com/wp-srv/special/business/china-growth/">&ldquo;China‚Äôs way to the top&rdquo;</a> on the Post&rsquo;s website recently. It&rsquo;s a very clear presentation of their statement and is certainly worth a look.<!-- raw HTML omitted --></p>
<p>So say you&rsquo;re an economist and you actually do need to produce a realistic estimate of when China&rsquo;s GDP surpasses that of the USA. Can you use such an approach? Not really.  There are several simplifying assumptions the Post made that are perfectly reasonable.  However, if the goal is an analytical output from a highly random system such as GDP growth, one should not assume the inputs are fixed. <em>(I&rsquo;m not saying I have any gripe with their interactive. This post has a different purpose.)</em></p>
<p>Why is this? The short answer is that randomness in any system can change its output drastically from one run to the next. Even if the mean from a deterministic analysis is correct, it tells us nothing about the variance of our output. We really need a confidence interval of years when China is likely to overtake the USA.</p>
<p>We&rsquo;ll move in the great tradition of all simulation studies. First we pepare our input. A CSV of GDP in current US dollars for both countries from 1960 to 2009 is available from the World Bank <a href="https://data.worldbank.org/country/china">data</a> <a href="https://data.worldbank.org/country/usa">files</a>. We read this into a data frame and calculate their growth rates year over year. Note that the first value for growth has to be NA.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>gdp <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">read.csv</span>(<span style="color:#e6db74">&#39;gdp.csv&#39;</span>)
</span></span><span style="display:flex;"><span>gdp<span style="color:#f92672">$</span>USA.growth <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">rep</span>(<span style="color:#66d9ef">NA</span>, <span style="color:#a6e22e">length</span>(gdp<span style="color:#f92672">$</span>USA))
</span></span><span style="display:flex;"><span>gdp<span style="color:#f92672">$</span>China.growth <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">rep</span>(<span style="color:#66d9ef">NA</span>, <span style="color:#a6e22e">length</span>(gdp<span style="color:#f92672">$</span>China))
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">for </span>(i in <span style="color:#ae81ff">2</span><span style="color:#f92672">:</span><span style="color:#a6e22e">length</span>(gdp<span style="color:#f92672">$</span>USA)) {
</span></span><span style="display:flex;"><span>  gdp<span style="color:#f92672">$</span>USA.growth[i] <span style="color:#f92672">&lt;-</span> <span style="color:#ae81ff">100</span> <span style="color:#f92672">*</span> (gdp<span style="color:#f92672">$</span>USA[i] <span style="color:#f92672">-</span> gdp<span style="color:#f92672">$</span>USA[i<span style="color:#ae81ff">-1</span>]) <span style="color:#f92672">/</span> gdp<span style="color:#f92672">$</span>USA[i<span style="color:#ae81ff">-1</span>]
</span></span><span style="display:flex;"><span>  gdp<span style="color:#f92672">$</span>China.growth[i] <span style="color:#f92672">&lt;-</span> <span style="color:#ae81ff">100</span> <span style="color:#f92672">*</span> (gdp<span style="color:#f92672">$</span>China[i] <span style="color:#f92672">-</span> gdp<span style="color:#f92672">$</span>China[i<span style="color:#ae81ff">-1</span>]) <span style="color:#f92672">/</span> gdp<span style="color:#f92672">$</span>China[i<span style="color:#ae81ff">-1</span>]
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>We now analyze our inputs and assign probability distributions to the annual growth rates. In a full study this would involve comparing a number of different distributions and choosing the one that fits the input data best, but that&rsquo;s well beyond the scope of this post. Instead, we&rsquo;ll use the poor man&rsquo;s way out: plot histograms and visually verify what we hope to be true, that the distributions are normal.</p>
<p><img loading="lazy" src="/files/2011-02-23-simulating-gdp-growth/us-gdp-percent-growth-histogram.png" alt="GDP growth histogram for the USA"  />
</p>
<p><img loading="lazy" src="/files/2011-02-23-simulating-gdp-growth/china-gdp-percent-growth-histogram.png" alt="GDP growth histogram for China"  />
</p>
<p>And they pretty much are. That&rsquo;s good enough for our purposes. Now all we need are the distribution parameters, which are mean and standard deviation for normal distributions.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span><span style="color:#f92672">&gt;</span> <span style="color:#a6e22e">mean</span>(gdp<span style="color:#f92672">$</span>USA.growth[<span style="color:#f92672">!</span><span style="color:#a6e22e">is.na</span>(gdp<span style="color:#f92672">$</span>USA.growth)])
</span></span><span style="display:flex;"><span>[1] <span style="color:#ae81ff">7.00594</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">&gt;</span> <span style="color:#a6e22e">sd</span>(gdp<span style="color:#f92672">$</span>USA.growth[<span style="color:#f92672">!</span><span style="color:#a6e22e">is.na</span>(gdp<span style="color:#f92672">$</span>USA.growth)])
</span></span><span style="display:flex;"><span>[1] <span style="color:#ae81ff">2.889808</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">&gt;</span> <span style="color:#a6e22e">mean</span>(gdp<span style="color:#f92672">$</span>China.growth[<span style="color:#f92672">!</span><span style="color:#a6e22e">is.na</span>(gdp<span style="color:#f92672">$</span>China.growth)])
</span></span><span style="display:flex;"><span>[1] <span style="color:#ae81ff">9.90896</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">&gt;</span> <span style="color:#a6e22e">sd</span>(gdp<span style="color:#f92672">$</span>China.growth[<span style="color:#f92672">!</span><span style="color:#a6e22e">is.na</span>(gdp<span style="color:#f92672">$</span>China.growth)])
</span></span><span style="display:flex;"><span>[1] <span style="color:#ae81ff">10.5712</span><span style="color:#f92672">&lt;/</span>code<span style="color:#f92672">&gt;&lt;/</span>pre<span style="color:#f92672">&gt;</span>
</span></span></code></pre></div><p>Now our input analysis is done. These are the inputs:</p>
<p>$$
\begin{align*}
\text{USA Growth} &amp;\sim \mathcal{N}(7.00594, 2.889808^2)\\
\text{China Growth} &amp;\sim \mathcal{N}(9.90896, 10.5712^2)
\end{align*}
$$</p>
<p>This should make the advantage of such an approach much more obvious. Compare the standard deviations for the two countries. China is a lot more likely to have negative GDP growth in any given year. They&rsquo;re also more likely to have astronomical growth.</p>
<p>We now build and run our simulation study. The more times we run the simulation the tighter we can make our confidence interval <em>(to a point)</em>, so we&rsquo;ll pick a pretty big number somewhat arbitrarily. If we want to, we can be fairly scientific about determining how many iterations are necessary after we&rsquo;ve done some runs, but we have to start somewhere.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>repetitions <span style="color:#f92672">&lt;-</span> <span style="color:#ae81ff">10000</span>
</span></span></code></pre></div><p>This is the code for our simulation. For each iteration, it starts both countries at their 2009 GDPs. It then iterates, changing GDP randomly until China&rsquo;s GDP is at least the same value as the USA&rsquo;s. When that happens, it records the current year.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>results <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">rep</span>(<span style="color:#66d9ef">NA</span>, repetitions)
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">for </span>(i in <span style="color:#ae81ff">1</span><span style="color:#f92672">:</span>repetitions) {
</span></span><span style="display:flex;"><span>  usa <span style="color:#f92672">&lt;-</span> gdp<span style="color:#f92672">$</span>USA<span style="color:#a6e22e">[length</span>(gdp<span style="color:#f92672">$</span>USA)]
</span></span><span style="display:flex;"><span>  china <span style="color:#f92672">&lt;-</span> gdp<span style="color:#f92672">$</span>China<span style="color:#a6e22e">[length</span>(gdp<span style="color:#f92672">$</span>China)]
</span></span><span style="display:flex;"><span>  year <span style="color:#f92672">&lt;-</span> gdp<span style="color:#f92672">$</span>Year<span style="color:#a6e22e">[length</span>(gdp<span style="color:#f92672">$</span>Year)]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">while </span>(<span style="color:#66d9ef">TRUE</span>) {
</span></span><span style="display:flex;"><span>    year <span style="color:#f92672">&lt;-</span> year <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    usa.growth <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">rnorm</span>(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">7.00594</span>, <span style="color:#ae81ff">2.889808</span>)
</span></span><span style="display:flex;"><span>    china.growth <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">rnorm</span>(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">9.90896</span>, <span style="color:#ae81ff">10.5712</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    usa <span style="color:#f92672">&lt;-</span> usa <span style="color:#f92672">*</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> (usa.growth <span style="color:#f92672">/</span> <span style="color:#ae81ff">100</span>))
</span></span><span style="display:flex;"><span>    china <span style="color:#f92672">&lt;-</span> china <span style="color:#f92672">*</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> (china.growth <span style="color:#f92672">/</span> <span style="color:#ae81ff">100</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#a6e22e">if </span>(china <span style="color:#f92672">&gt;=</span> usa) {
</span></span><span style="display:flex;"><span>      results[i] <span style="color:#f92672">&lt;-</span> year
</span></span><span style="display:flex;"><span>      break
</span></span><span style="display:flex;"><span>     }
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>From the results vector we see that, given the data and assumptions for this model, China should surpass the USA in 2058. We also see that we can be 95% confident that the mean year this will happen is between 2057 and 2059. This is not quite the same as saying we are confident this will actually happen between those years. The result of our simulation is a probability distribution and we are discovering information about it.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span><span style="color:#f92672">&gt;</span> <span style="color:#a6e22e">mean</span>(results)
</span></span><span style="display:flex;"><span>[1] <span style="color:#ae81ff">2058.494</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">&gt;</span> <span style="color:#a6e22e">mean</span>(results) <span style="color:#f92672">+</span> (<span style="color:#a6e22e">sd</span>(results) <span style="color:#f92672">/</span> <span style="color:#a6e22e">sqrt</span>(<span style="color:#a6e22e">length</span>(results)) <span style="color:#f92672">*</span> <span style="color:#a6e22e">qnorm</span>(<span style="color:#ae81ff">0.025</span>))
</span></span><span style="display:flex;"><span>[1] <span style="color:#ae81ff">2057.873</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">&gt;</span> <span style="color:#a6e22e">mean</span>(results) <span style="color:#f92672">+</span> (<span style="color:#a6e22e">sd</span>(results) <span style="color:#f92672">/</span> <span style="color:#a6e22e">sqrt</span>(<span style="color:#a6e22e">length</span>(results)) <span style="color:#f92672">*</span> <span style="color:#a6e22e">qnorm</span>(<span style="color:#ae81ff">0.975</span>))
</span></span><span style="display:flex;"><span>[1] <span style="color:#ae81ff">2059.114</span><span style="color:#f92672">&lt;/</span>code<span style="color:#f92672">&gt;&lt;/</span>pre<span style="color:#f92672">&gt;</span>
</span></span></code></pre></div><p>So what&rsquo;s wrong with this model? Well, we had to make a number of assumptions:</p>
<ul>
<li>We assume we actually used the right data set. This was more of a how-to than a proper analysis, so that wasn&rsquo;t too much of a concern.</li>
<li>We assume future growth for the next 40-50 years resembles past growth from 1960-2009.  This is a bit ridiculous, of course, but that&rsquo;s the problem with forecasting.</li>
<li>*We assume growth is normally distributed and that we don&rsquo;t encounter heavy-tailed behaviors in our distributions. We assume each year&rsquo;s growth is independent of the year before it. See the last exercise.</li>
</ul>
<p>Here are some good simulation exercises if you&rsquo;re looking to do more:</p>
<ul>
<li>Note how the outputs are quite a bit different from the Post graphic. I expect that&rsquo;s largely due to the inclusion of data back to 1960. Try running the simulation for yourself using just the past 10, 20, and 30 years and see how that changes the result.&lt;</li>
<li>Write a simulation to determine the probability China&rsquo;s GDP surpasses the USA&rsquo;s in the next 25 years. Now plot the mean GDP and 95% confidence intervals for each country per year.</li>
<li>Assume that there are actually two distributions for growth for each country: one when the previous year had positive growth and another when it was negative. How does that change the output?</li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>üßê Data Fitting 2a - Very, Very Simple Linear Regression in R</title>
      <link>https://ryanjoneil.github.io/posts/2011-02-16-data-fitting-2a-very-very-simple-linear-regression-in-r/</link>
      <pubDate>Wed, 16 Feb 2011 00:00:00 +0000</pubDate>
      
      <guid>https://ryanjoneil.github.io/posts/2011-02-16-data-fitting-2a-very-very-simple-linear-regression-in-r/</guid>
      <description>Predict how much people like cats and dogs based on their ice cream preferences. Also, R.</description>
      <content:encoded><![CDATA[<p><em>Note: This post was updated to include an example data file.</em></p>
<p>I thought it might be useful to follow up the <a href="../2011-02-15-data-fitting-2-very-very-simple-linear-regression-in-python/">last post</a> with another one showing the same examples in R.</p>
<p>R provides a function called <code>lm</code>, which is similar in spirit to <a href="https://numpy.org/">NumPy</a>&rsquo;s <code>linalg.lstsq</code>. As you&rsquo;ll see, <code>lm</code>&rsquo;s interface is a bit more tuned to the concepts of modeling.<!-- raw HTML omitted --></p>
<p>We begin by reading in the <a href="/files/2011-02-16-data-fitting-2a-very-very-simple-linear-regression-in-r/example_data.csv">example CSV</a> into a data frame:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>responses <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">read.csv</span>(<span style="color:#e6db74">&#39;example_data.csv&#39;</span>)
</span></span><span style="display:flex;"><span>responses
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-txt" data-lang="txt"><span style="display:flex;"><span>  respondent vanilla.love strawberry.love chocolate.love dog.love cat.love
</span></span><span style="display:flex;"><span>1     Aylssa            9               4              9        9        9
</span></span><span style="display:flex;"><span>2       Ben8            8               6              4       10        4
</span></span><span style="display:flex;"><span>3         Cy            9               4              8        2        6
</span></span><span style="display:flex;"><span>4        Eva            3               7              9        4        6
</span></span><span style="display:flex;"><span>5        Lem            6               8              5        2        5
</span></span><span style="display:flex;"><span>6      Louis            4               5              3       10        3
</span></span></code></pre></div><p>A data frame is sort of like a matrix, but with named columns. That is, we can refer to entire columns using the dollar sign. We are now ready to run least squares. We&rsquo;ll create the model for predicting &ldquo;dog love.&rdquo;  To create the &ldquo;cat love&rdquo; model, simply use that column name instead:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>fit1 <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">lm</span>(
</span></span><span style="display:flex;"><span>  responses<span style="color:#f92672">$</span>dog.love <span style="color:#f92672">~</span> responses<span style="color:#f92672">$</span>vanilla.love <span style="color:#f92672">+</span>
</span></span><span style="display:flex;"><span>                       responses<span style="color:#f92672">$</span>strawberry.love <span style="color:#f92672">+</span> 
</span></span><span style="display:flex;"><span>                       responses<span style="color:#f92672">$</span>chocolate.love
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>The syntax for lm is a little off-putting at first.  This call tells it to create a model for &ldquo;dog love&rdquo; with respect to <em>(the ~)</em> a function of the form <em>offset + x1 * vanilla love + x2 * strawberry love + x3 * chocolate love</em>. Note that the offset is conveniently implied when using <code>lm</code>, so this is the same as the second model we created in Python. Now that we&rsquo;ve computed the coefficients for our &ldquo;dog love&rdquo; model, we can ask R about it:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span><span style="color:#a6e22e">summary</span>(fit1)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-txt" data-lang="txt"><span style="display:flex;"><span>Call:
</span></span><span style="display:flex;"><span>lm(formula = responses$dog.love ~ responses$vanilla.love + responses$strawberry.love + 
</span></span><span style="display:flex;"><span>    responses$chocolate.love)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Residuals:
</span></span><span style="display:flex;"><span>      1       2       3       4       5       6 
</span></span><span style="display:flex;"><span> 3.1827  2.9436 -4.5820  0.8069 -1.9856 -0.3657 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Coefficients:
</span></span><span style="display:flex;"><span>                          Estimate Std. Error t value Pr(&gt;|t|)
</span></span><span style="display:flex;"><span>(Intercept)                20.9298    15.0654   1.389    0.299
</span></span><span style="display:flex;"><span>responses$vanilla.love     -0.2783     0.9934  -0.280    0.806
</span></span><span style="display:flex;"><span>responses$strawberry.love  -1.4314     1.5905  -0.900    0.463
</span></span><span style="display:flex;"><span>responses$chocolate.love   -0.7647     0.8214  -0.931    0.450
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Residual standard error: 4.718 on 2 degrees of freedom
</span></span><span style="display:flex;"><span>Multiple R-squared:  0.4206,	Adjusted R-squared:  -0.4485 
</span></span><span style="display:flex;"><span>F-statistic: 0.484 on 3 and 2 DF,  p-value: 0.7272
</span></span></code></pre></div><p>This gives us quite a bit of information, including the coefficients for our &ldquo;dog love&rdquo; model and various error metrics. You can find the offset and coefficients under the Estimate column above. We quickly verify this using R&rsquo;s vectorized arithmetic:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span><span style="color:#ae81ff">20.9298</span> <span style="color:#f92672">-</span> 
</span></span><span style="display:flex;"><span>  <span style="color:#ae81ff">0.2783</span> <span style="color:#f92672">*</span> responses<span style="color:#f92672">$</span>vanilla.love <span style="color:#f92672">-</span>
</span></span><span style="display:flex;"><span>  <span style="color:#ae81ff">1.4314</span> <span style="color:#f92672">*</span> responses<span style="color:#f92672">$</span>strawberry.love <span style="color:#f92672">-</span>
</span></span><span style="display:flex;"><span>  <span style="color:#ae81ff">0.7647</span> <span style="color:#f92672">*</span> responses<span style="color:#f92672">$</span>chocolate.love
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-txt" data-lang="txt"><span style="display:flex;"><span>[1]  5.8172  7.0562  6.5819  3.1928  3.9853 10.3655
</span></span></code></pre></div><p>You&rsquo;ll notice the model is essentially the same as the one we got from NumPy. Our next step is to add in the squared inputs. We do this by adding extra terms to the modeling formula. The <code>I()</code> function allows us to easily add additional operators to columns. That&rsquo;s how we accomplish the squaring. We could alternatively add squared input values to the data frame, but using <code>I()</code> is more convenient and natural.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>fit2 <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">lm</span>(responses<span style="color:#f92672">$</span>dog.love <span style="color:#f92672">~</span> responses<span style="color:#f92672">$</span>vanilla.love <span style="color:#f92672">+</span>
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">I</span>(responses<span style="color:#f92672">$</span>vanilla.love^2) <span style="color:#f92672">+</span> responses<span style="color:#f92672">$</span>strawberry.love <span style="color:#f92672">+</span>
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">I</span>(responses<span style="color:#f92672">$</span>strawberry.love^2) <span style="color:#f92672">+</span> responses<span style="color:#f92672">$</span>chocolate.love <span style="color:#f92672">+</span>
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">I</span>(responses<span style="color:#f92672">$</span>chocolate.love^2))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">summary</span>(fit2)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-txt" data-lang="txt"><span style="display:flex;"><span>Call:
</span></span><span style="display:flex;"><span>lm(formula = responses$dog.love ~ responses$vanilla.love + I(responses$vanilla.love^2) + 
</span></span><span style="display:flex;"><span>    responses$strawberry.love + I(responses$strawberry.love^2) + 
</span></span><span style="display:flex;"><span>    responses$chocolate.love + I(responses$chocolate.love^2))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Residuals:
</span></span><span style="display:flex;"><span>ALL 6 residuals are 0: no residual degrees of freedom!
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Coefficients: (1 not defined because of singularities)
</span></span><span style="display:flex;"><span>                               Estimate Std. Error t value Pr(&gt;|t|)
</span></span><span style="display:flex;"><span>(Intercept)                    -357.444        NaN     NaN      NaN
</span></span><span style="display:flex;"><span>responses$vanilla.love           72.444        NaN     NaN      NaN
</span></span><span style="display:flex;"><span>I(responses$vanilla.love^2)      -6.111        NaN     NaN      NaN
</span></span><span style="display:flex;"><span>responses$strawberry.love        59.500        NaN     NaN      NaN
</span></span><span style="display:flex;"><span>I(responses$strawberry.love^2)   -5.722        NaN     NaN      NaN
</span></span><span style="display:flex;"><span>responses$chocolate.love          7.000        NaN     NaN      NaN
</span></span><span style="display:flex;"><span>I(responses$chocolate.love^2)        NA         NA      NA       NA
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Residual standard error: NaN on 0 degrees of freedom
</span></span><span style="display:flex;"><span>Multiple R-squared:      1,	Adjusted R-squared:    NaN 
</span></span><span style="display:flex;"><span>F-statistic:   NaN on 5 and 0 DF,  p-value: NA
</span></span></code></pre></div><p>We can see that we get the same &ldquo;dog love&rdquo; model as produced by the third Python version of the last post. Again, we quickly verify that the output is the same (minus some rounding errors):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span><span style="color:#ae81ff">-357.444</span> <span style="color:#f92672">+</span> 
</span></span><span style="display:flex;"><span>  <span style="color:#ae81ff">72.444</span> <span style="color:#f92672">*</span> responses<span style="color:#f92672">$</span>vanilla.love <span style="color:#f92672">-</span>
</span></span><span style="display:flex;"><span>  <span style="color:#ae81ff">6.111</span> <span style="color:#f92672">*</span> responses<span style="color:#f92672">$</span>vanilla.love^2 <span style="color:#f92672">+</span>
</span></span><span style="display:flex;"><span>  <span style="color:#ae81ff">59.5</span> <span style="color:#f92672">*</span> responses<span style="color:#f92672">$</span>strawberry.love <span style="color:#f92672">-</span>
</span></span><span style="display:flex;"><span>  <span style="color:#ae81ff">5.722</span> <span style="color:#f92672">*</span> responses<span style="color:#f92672">$</span>strawberry.love^2 <span style="color:#f92672">+</span>
</span></span><span style="display:flex;"><span>  <span style="color:#ae81ff">7</span> <span style="color:#f92672">*</span> responses<span style="color:#f92672">$</span>chocolate.love
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-txt" data-lang="txt"><span style="display:flex;"><span>[1]  9.009 10.012  2.009  4.011  2.016 10.006
</span></span></code></pre></div>]]></content:encoded>
    </item>
    
    <item>
      <title>üßê Data Fitting 2 - Very, Very Simple Linear Regression in Python</title>
      <link>https://ryanjoneil.github.io/posts/2011-02-15-data-fitting-2-very-very-simple-linear-regression-in-python/</link>
      <pubDate>Tue, 15 Feb 2011 00:00:00 +0000</pubDate>
      
      <guid>https://ryanjoneil.github.io/posts/2011-02-15-data-fitting-2-very-very-simple-linear-regression-in-python/</guid>
      <description>Predict how much people like cats and dogs based on their ice cream preferences. Also, Python and numpy.</description>
      <content:encoded><![CDATA[<p>This post is based on a memo I sent to some former colleagues at the Post. I&rsquo;ve edited it for use here since it fits well as the second in a series on simple data fitting techniques. If you&rsquo;re among the many enlightened individuals already using regression analysis, then this post is probably not for you. If you aren&rsquo;t, then hopefully this provides everything you need to develop rudimentary predictive models that yield surprising levels of accuracy.</p>
<h2 id="data">Data</h2>
<p>For purposes of a simple working example, we have collected six records of input data over three dimensions with the goal of predicting two outputs. The input data are:</p>
<p>$$
\begin{align*}
x_1 &amp;= \text{How much a respondent likes vanilla [0-10]}\\
x_2 &amp;= \text{How much a respondent likes strawberry [0-10]}\\
x_3 &amp;= \text{How much a respondent likes chocolate [0-10]}
\end{align*}
$$</p>
<p>Output data consist of:</p>
<p>$$
\begin{align*}
b_1 &amp;= \text{How much a respondent likes dogs [0-10]}\\
b_2 &amp;= \text{How much a respondent likes cats [0-10]}
\end{align*}
$$</p>
<p>Below are anonymous data collected from a random sample of people.</p>
<table>
<thead>
<tr>
<th>respondent</th>
<th style="text-align:right">vanilla ‚ù§Ô∏è</th>
<th style="text-align:right">strawberry ‚ù§Ô∏è</th>
<th style="text-align:right">chocolate ‚ù§Ô∏è</th>
<th style="text-align:right">dog ‚ù§Ô∏è</th>
<th style="text-align:right">cat ‚ù§Ô∏è</th>
</tr>
</thead>
<tbody>
<tr>
<td>Alyssa P Hacker</td>
<td style="text-align:right">9</td>
<td style="text-align:right">4</td>
<td style="text-align:right">9</td>
<td style="text-align:right">9</td>
<td style="text-align:right">8</td>
</tr>
<tr>
<td>Ben Bitdiddle</td>
<td style="text-align:right">8</td>
<td style="text-align:right">6</td>
<td style="text-align:right">4</td>
<td style="text-align:right">10</td>
<td style="text-align:right">4</td>
</tr>
<tr>
<td>Cy D. Fect</td>
<td style="text-align:right">9</td>
<td style="text-align:right">4</td>
<td style="text-align:right">8</td>
<td style="text-align:right">2</td>
<td style="text-align:right">6</td>
</tr>
<tr>
<td>Eva Lu Ator</td>
<td style="text-align:right">3</td>
<td style="text-align:right">7</td>
<td style="text-align:right">9</td>
<td style="text-align:right">4</td>
<td style="text-align:right">6</td>
</tr>
<tr>
<td>Lem E. Tweakit</td>
<td style="text-align:right">6</td>
<td style="text-align:right">8</td>
<td style="text-align:right">5</td>
<td style="text-align:right">2</td>
<td style="text-align:right">5</td>
</tr>
<tr>
<td>Louis Reasoner</td>
<td style="text-align:right">4</td>
<td style="text-align:right">5</td>
<td style="text-align:right">3</td>
<td style="text-align:right">10</td>
<td style="text-align:right">3</td>
</tr>
</tbody>
</table>
<p>Our input is in three dimensions. Each output requires its own model, so we&rsquo;ll have one for dogs and one for cats. We&rsquo;re looking for functions, <code>dog(x)</code> and <code>cat(x)</code>, that can predict $b_1$ and $b_2$ based on given values of $x_1$, $x_2$, and $x_3$.</p>
<h2 id="model-1">Model 1</h2>
<p>For both models we want to find parameters that minimize their squared residuals (read: errors). There&rsquo;s a number of names for this. Optimization folks like to think of it as unconstrained quadratic optimization, but it&rsquo;s more common to call it least squares or linear regression. It&rsquo;s not necessary to entirely understand why for our purposes, but the function that minimizes these errors is:</p>
<p>$$\beta = ({A^t}A)^{-1}{A^t}b$$</p>
<p>This is implemented for you in the <code>numpy.linalg</code> Python package, which we&rsquo;ll use for examples. Much more information than you probably want can be found <a href="http://en.wikipedia.org/wiki/Least_squares">here</a>.</p>
<p>Below is a first stab at a Python version. It runs least squares against our input and output data exactly as they are. You can see the matrix $A$ and outputs $b_1$ and $b_2$ (dog and cat love, respectively) are represented just as they are in the table.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Version 1: No offset, no squared inputs</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>A <span style="color:#f92672">=</span> numpy<span style="color:#f92672">.</span>vstack([
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">9</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">4</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">8</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">9</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">5</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">3</span>]
</span></span><span style="display:flex;"><span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>b1 <span style="color:#f92672">=</span> numpy<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">10</span>])
</span></span><span style="display:flex;"><span>b2 <span style="color:#f92672">=</span> numpy<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">3</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;dog ‚ù§Ô∏è:&#39;</span>, numpy<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>lstsq(A, b1, rcond<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>)[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;cat ‚ù§Ô∏è:&#39;</span>, numpy<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>lstsq(A, b2, rcond<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>)[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Output:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># dog ‚ù§Ô∏è: [0.72548294      0.53045642     -0.29952361]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># cat ‚ù§Ô∏è: [2.36110929e-01  2.61934385e-05  6.26892476e-01]</span>
</span></span></code></pre></div><p>The resulting model is:</p>
<pre tabindex="0"><code>dog(x) = 0.72548294 * x1 + 0.53045642 * x2 - 0.29952361 * x3
cat(x) = 2.36110929e-01 * x1 + 2.61934385e-05 * x2 + 6.26892476e-01 * x3
</code></pre><p>The coefficients before our variables correspond to beta in the formula above. Errors between observed and predicted data, shown below, are calculated and summed. For these six records, <code>dog(x)</code> has a total error of 20.76 and <code>cat(x)</code> has 3.74. Not great.</p>
<table>
<thead>
<tr>
<th>respondent</th>
<th style="text-align:right">predicted b1</th>
<th style="text-align:right">b1 error</th>
<th style="text-align:right">predicted b2</th>
<th style="text-align:right">b2 error</th>
</tr>
</thead>
<tbody>
<tr>
<td>Alyssa P Hacker</td>
<td style="text-align:right">5.96</td>
<td style="text-align:right">3.04</td>
<td style="text-align:right">7.77</td>
<td style="text-align:right">1.23</td>
</tr>
<tr>
<td>Ben Bitdiddle</td>
<td style="text-align:right">7.79</td>
<td style="text-align:right">2.21</td>
<td style="text-align:right">4.40</td>
<td style="text-align:right">0.40</td>
</tr>
<tr>
<td>Cy D. Fect</td>
<td style="text-align:right">6.25</td>
<td style="text-align:right">4.25</td>
<td style="text-align:right">7.14</td>
<td style="text-align:right">1.14</td>
</tr>
<tr>
<td>Eva Lu Ator</td>
<td style="text-align:right">3.19</td>
<td style="text-align:right">0.81</td>
<td style="text-align:right">6.35</td>
<td style="text-align:right">0.35</td>
</tr>
<tr>
<td>Lem E. Tweakit</td>
<td style="text-align:right">7.10</td>
<td style="text-align:right">5.10</td>
<td style="text-align:right">4.55</td>
<td style="text-align:right">0.45</td>
</tr>
<tr>
<td>Louis Reasoner</td>
<td style="text-align:right">4.66</td>
<td style="text-align:right">5.34</td>
<td style="text-align:right">2.83</td>
<td style="text-align:right">0.17</td>
</tr>
<tr>
<td>Total error:</td>
<td style="text-align:right"></td>
<td style="text-align:right">20.76</td>
<td style="text-align:right"></td>
<td style="text-align:right">3.74</td>
</tr>
</tbody>
</table>
<h2 id="model-2">Model 2</h2>
<p>One problem with this model is that <code>dog(x)</code> and <code>cat(x)</code> are forced to pass through the origin. <em>(Why is that?)</em> We can improve it somewhat if we add an offset. This amounts to prepending 1 to every row in $A$ and adding a constant to the resulting functions. You can see the very slight difference between the code for this model and that of the previous:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Version 2: Offset, no squared inputs</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>A <span style="color:#f92672">=</span> numpy<span style="color:#f92672">.</span>vstack([
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">9</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">4</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">8</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">9</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">5</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">3</span>]
</span></span><span style="display:flex;"><span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;dog ‚ù§Ô∏è:&#39;</span>, numpy<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>lstsq(A, b1, rcond<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>)[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;cat ‚ù§Ô∏è:&#39;</span>, numpy<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>lstsq(A, b2, rcond<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>)[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Output:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># dog ‚ù§Ô∏è: [20.92975427  -0.27831197  -1.43135684  -0.76469017]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># cat ‚ù§Ô∏è: [-0.31744124   0.25133547   0.02978098   0.63394765]</span>
</span></span></code></pre></div><p>This yields the seconds version of our models:</p>
<pre tabindex="0"><code>dog(x) = 20.92975427 - 0.27831197 * x1 - 1.43135684 * x2 - 0.76469017 * x3
cat(x) = -0.31744124 + 0.25133547 * x1 + 0.02978098 * x2 + 0.63394765 * x3
</code></pre><p>These models provide errors of 13.87 and 3.79.  A little better on the dog side, but still not quite usable.</p>
<table>
<thead>
<tr>
<th>respondent</th>
<th style="text-align:right">predicted b1</th>
<th style="text-align:right">b1 error</th>
<th style="text-align:right">predicted b2</th>
<th style="text-align:right">b2 error</th>
</tr>
</thead>
<tbody>
<tr>
<td>Alyssa P Hacker</td>
<td style="text-align:right">5.82</td>
<td style="text-align:right">3.18</td>
<td style="text-align:right">7.77</td>
<td style="text-align:right">1.23</td>
</tr>
<tr>
<td>Ben Bitdiddle</td>
<td style="text-align:right">7.06</td>
<td style="text-align:right">2.94</td>
<td style="text-align:right">4.41</td>
<td style="text-align:right">0.41</td>
</tr>
<tr>
<td>Cy D. Fect</td>
<td style="text-align:right">6.58</td>
<td style="text-align:right">4.58</td>
<td style="text-align:right">7.14</td>
<td style="text-align:right">1.14</td>
</tr>
<tr>
<td>Eva Lu Ator</td>
<td style="text-align:right">3.19</td>
<td style="text-align:right">0.81</td>
<td style="text-align:right">6.35</td>
<td style="text-align:right">0.35</td>
</tr>
<tr>
<td>Lem E. Tweakit</td>
<td style="text-align:right">3.99</td>
<td style="text-align:right">1.99</td>
<td style="text-align:right">4.60</td>
<td style="text-align:right">0.40</td>
</tr>
<tr>
<td>Louis Reasoner</td>
<td style="text-align:right">10.37</td>
<td style="text-align:right">0.37</td>
<td style="text-align:right">2.74</td>
<td style="text-align:right">0.26</td>
</tr>
<tr>
<td>Total error:</td>
<td style="text-align:right"></td>
<td style="text-align:right">13.87</td>
<td style="text-align:right"></td>
<td style="text-align:right">3.79</td>
</tr>
</tbody>
</table>
<h2 id="model-3">Model 3</h2>
<p>The problem is that <code>dog(x)</code> and <code>cat(x)</code> are linear functions. Most observed data don&rsquo;t conform to straight lines. Take a moment and draw the line $f(x) = x$ and the curve $f(x) = x^2$. The former makes a poor approximation of the latter.</p>
<p>Most of the time, people just use squares of the input data to add curvature to their models. We do this in our next version of the code by just adding squares of the input row values to our $A$ matrix. Everything else is the same. (In reality, you can add any function of the input data you feel best models the data, if you understand it well enough.)</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Version 3: Offset with squared inputs</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>A <span style="color:#f92672">=</span> numpy<span style="color:#f92672">.</span>vstack([
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">9</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">4</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">9</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">8</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">6</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">4</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">9</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">4</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">8</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">7</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">9</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">6</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">8</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">4</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>]
</span></span><span style="display:flex;"><span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>b1 <span style="color:#f92672">=</span> numpy<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">10</span>])
</span></span><span style="display:flex;"><span>b2 <span style="color:#f92672">=</span> numpy<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">3</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;dog ‚ù§Ô∏è:&#39;</span>, numpy<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>lstsq(A, b1, rcond<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>)[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;cat ‚ù§Ô∏è:&#39;</span>, numpy<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>lstsq(A, b2, rcond<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>)[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># dog ‚ù§Ô∏è: [1.29368307  7.03633306  -0.44795498  9.98093332</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#  -0.75689575  -19.00757486  1.52985734]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># cat ‚ù§Ô∏è: [0.47945896  5.30866067  -0.39644128 -1.28704188</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   0.12634295   -4.32392606  0.43081918]</span>
</span></span></code></pre></div><p>This gives us our final version of the model:</p>
<pre tabindex="0"><code>dog(x) = 1.29368307 + 7.03633306 * x1 - 0.44795498 * x1**2 + 9.98093332 * x2 - 0.75689575 * x2**2 - 19.00757486 * x3 + 1.52985734 * x3**2
cat(x) = 0.47945896 + 5.30866067 * x1 - 0.39644128 * x1**2 - 1.28704188 * x2 + 0.12634295 * x2**2 - 4.32392606 * x3 + 0.43081918 * x3**2
</code></pre><p>Adding curvature to our model eliminates all perceived error, at least within 1e-16. This may seem unbelievable, but when you consider that we only have six input records, it isn&rsquo;t really.</p>
<table>
<thead>
<tr>
<th>respondent</th>
<th style="text-align:right">predicted b1</th>
<th style="text-align:right">b1 error</th>
<th style="text-align:right">predicted b2</th>
<th style="text-align:right">b2 error</th>
</tr>
</thead>
<tbody>
<tr>
<td>Alyssa P Hacker</td>
<td style="text-align:right">9</td>
<td style="text-align:right">0</td>
<td style="text-align:right">9</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td>Ben Bitdiddle</td>
<td style="text-align:right">10</td>
<td style="text-align:right">0</td>
<td style="text-align:right">4</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td>Cy D. Fect</td>
<td style="text-align:right">2</td>
<td style="text-align:right">0</td>
<td style="text-align:right">6</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td>Eva Lu Ator</td>
<td style="text-align:right">4</td>
<td style="text-align:right">0</td>
<td style="text-align:right">6</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td>Lem E. Tweakit</td>
<td style="text-align:right">2</td>
<td style="text-align:right">0</td>
<td style="text-align:right">5</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td>Louis Reasoner</td>
<td style="text-align:right">10</td>
<td style="text-align:right">0</td>
<td style="text-align:right">3</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td>Total error:</td>
<td style="text-align:right"></td>
<td style="text-align:right">0</td>
<td style="text-align:right"></td>
<td style="text-align:right">0</td>
</tr>
</tbody>
</table>
<p>It should be fairly obvious how one can take this and extrapolate to much larger models. I hope this is useful and that least squares becomes an important part of your lives.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>üó≥ Off the Cuff Voter Fraud Detection</title>
      <link>https://ryanjoneil.github.io/posts/2010-11-30-off-the-cuff-voter-fraud-detection/</link>
      <pubDate>Tue, 30 Nov 2010 00:00:00 +0000</pubDate>
      
      <guid>https://ryanjoneil.github.io/posts/2010-11-30-off-the-cuff-voter-fraud-detection/</guid>
      <description>Using the exponential distribution to interpret votes in a web survey</description>
      <content:encoded><![CDATA[<p>Consider this scenario: You run a contest that accepts votes from the general Internet population. In order to encourage user engagement, you record any and all votes into a database over several days, storing nothing more than the competitor voted for, when each vote is cast, and a cookie set on the voter&rsquo;s computer along with their apparent IP addresses. If a voter already has a recorded cookie set they are denied subsequent votes. This way you can avoid requiring site registration, a huge turnoff for your users. Simple enough.</p>
<p>Unfortunately, some of the competitors are wily and attached to the idea of winning. They go so far as programming or hiring bots to cast thousands of votes for them. Your manager wants to know which votes are real and which ones are fake Right Now. Given very limited time, and ignoring actions that you <em>could</em> have taken to avoid the problem, how can you tell apart sets of good votes from those that shouldn&rsquo;t be counted?</p>
<p>One quick-and-dirty option involves comparing histograms of <a href="http://www.ehow.com/how_5417319_calculate-interarrival-time.html">interarrival times</a> for sets of votes. Say you&rsquo;re concerned that all the votes during a particular period of time or from a given IP address might be fraudulent. Put all the vote times you&rsquo;re concerned about into a list, sort them, and compute their differences:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># times is a list of datetime instances from vote records</span>
</span></span><span style="display:flex;"><span>times<span style="color:#f92672">.</span>sort(reversed<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>interarrivals <span style="color:#f92672">=</span> [y<span style="color:#f92672">-</span>x <span style="color:#66d9ef">for</span> x, y <span style="color:#f92672">in</span> zip(times, times[<span style="color:#ae81ff">1</span>:]]
</span></span></code></pre></div><p>Now use matplotlib to <a href="https://matplotlib.org/2.0.2/users/pyplot_tutorial.html#working-with-text">display a histogram</a> of these. Votes that occur naturally are likely to resemble an <a href="http://en.wikipedia.org/wiki/Exponential_distribution">exponential distribution</a> in their interarrival times. For instance, here are interarrival times for all votes received in a contest:</p>
<p><img loading="lazy" src="/files/2010-11-30-off-the-cuff-voter-fraud-detection/all-votes.png" alt="Interarrival times for all submissions"  />
</p>
<p>This subset of votes is clearly fraudulent, due to the near determinism of their interarrival times. This is most likely caused by the voting bot not taking random sleep intervals during voting. It casts a vote, receives a response, clears its cookies, and repeats:</p>
<p><img loading="lazy" src="/files/2010-11-30-off-the-cuff-voter-fraud-detection/fraud-plot.png" alt="Interarrival times for clearly fraudulent votes"  />
</p>
<p>These votes, on the other hand, are most likely legitimate. They exhibit a nice <!-- raw HTML omitted -->Erlang<!-- raw HTML omitted --> shape and appear to have natural interarrival times that one would expect:</p>
<p><img loading="lazy" src="/files/2010-11-30-off-the-cuff-voter-fraud-detection/not-fraud.png" alt="Proper-looking interarrival times"  />
</p>
<p>Of course this method is woefully inadequate for rigorous detection of voting fraud. Ideally one would find a method to compute the probability that a set of votes is generated by a bot. This is enough to inform quick, ad hoc decisions though.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>üßê Data Fitting 1 - Linear Data Fitting</title>
      <link>https://ryanjoneil.github.io/posts/2010-11-23-data-fitting-1-linear-data-fitting/</link>
      <pubDate>Tue, 23 Nov 2010 00:00:00 +0000</pubDate>
      
      <guid>https://ryanjoneil.github.io/posts/2010-11-23-data-fitting-1-linear-data-fitting/</guid>
      <description>An introduction to data fitting and classification using linear optimization in Python</description>
      <content:encoded><![CDATA[<p><em>Note: This post was updated to work with Python 3 and <a href="https://github.com/scipopt/PySCIPOpt">PySCIPOpt</a>. The original version used Python 2 and <a href="https://pythonhosted.org/python-zibopt/">python-zibopt</a>.</em></p>
<p>Data fitting is one of those tasks that everyone should have at least some exposure to. Certainly developers and analysts will benefit from a working knowledge of its fundamentals and their implementations. However, in my own reading I&rsquo;ve found it difficult to locate good examples that are simple enough to pick up quickly and come with accompanying source code.</p>
<p>This article commences an ongoing series introducing basic data fitting techniques. With any luck they won&rsquo;t be overly complex, while still being useful enough to get the point across with a real example and real data. We&rsquo;ll start with a binary classification problem: presented with a series of records, each containing a set number of input values describing it, determine whether or not each record exhibits some property.</p>
<h2 id="model">Model</h2>
<p>We&rsquo;ll use the <code>cancer1.dt</code> data from the <code>proben1</code> set of test cases, which you can download <a href="/files/2010-11-23-data-fitting-1-linear-data-fitting/cancer1.dt">here</a>. Each record starts with 9 data points containing physical characteristics of a tumor. The second to last data point contains 1 if a tumor is benign and 0 if it is malignant. We seek to find a linear function we can run on an arbitrary record that will return a value greater than zero if that record&rsquo;s tumor is predicted to be benign and less than zero if it is predicted to be malignant. We will train our linear model on the first 350 records, and test it for accuracy on the remaining rows.</p>
<p>This is similar to the data fitting problem found in <a href="https://www.thriftbooks.com/w/linear-programming-series-of-books-in-the-mathematical-sciences_vasek-chvatal/249798/#edition=2416723&amp;idiq=15706498">Chvatal</a>. Our inputs consist of a matrix of observed data, $A$, and a vector of classifications, $b$. In order to classify a record, we require another vector $x$ such that the dot product of $x$ and that record will be either greater or less than zero depending on its predicted classification.</p>
<p>A couple points to note before we start:</p>
<ul>
<li>
<p>Most observed data are noisy. This means it may be impossible to locate a hyperplane that cleanly separates given records of one type from another. In this case, we must resort to finding a function that minimizes our predictive error. For the purposes of this example, we&rsquo;ll minimize the sum of the absolute values of the observed and predicted values. That is, we seek $x$ such that we find $min \sum_i{|a_i^T x-b_i|}$.</p>
</li>
<li>
<p>The <a href="https://www.purplemath.com/modules/strtlneq.htm">slope-intercept</a> form of a line, $f(x)=m^T x+b$, contains an offset. It should be obvious that this is necessary in our model so that our function isn&rsquo;t required to pass through the origin. Thus, we&rsquo;ll be adding an extra variable with the coefficient of 1 to represent our offset value.</p>
</li>
<li>
<p>In order to model this, we use two linear constraints for each absolute value. We minimize the sum of these. Our Linear Programming model thus looks like:</p>
</li>
</ul>
<p>$$
\begin{align*}
\min\quad       &amp; z = x_0 + \sum_i{v_i}\\
\text{s.t.}\quad&amp; v_i \geq x_0 + a_i^\intercal x - 1    &amp;\quad\forall&amp;\quad\text{benign tumors}\\
&amp; v_i \geq 1 - x_0 - a_i^\intercal x    &amp;\quad\forall&amp;\quad\text{benign tumors}\\
&amp; v_i \geq x_0 + a_i^\intercal x - (-1) &amp;\quad\forall&amp;\quad\text{malignant tumors}\\
&amp; v_i \geq -1 - x_0 - a_i^\intercal x   &amp;\quad\forall&amp;\quad\text{malignant tumors}
\end{align*}
$$</p>
<h2 id="code">Code</h2>
<p>In order to do this in Python, we use <a href="https://www.scipopt.org/">SCIP</a> and <a href="https://soplex.zib.de/">SoPlex</a>. We start by setting constants for benign and malignant outputs and providing a function to read in the training and testing data sets.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Preferred output values for tumor categories</span>
</span></span><span style="display:flex;"><span>BENIGN <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>MALIGNANT <span style="color:#f92672">=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">read_proben1_cancer_data</span>(filename, train_size):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;&#39;&#39;Loads a proben1 cancer file into train &amp; test sets&#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Number of input data points per record</span>
</span></span><span style="display:flex;"><span>    DATA_POINTS <span style="color:#f92672">=</span> <span style="color:#ae81ff">9</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    train_data <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    test_data <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> open(filename) <span style="color:#66d9ef">as</span> infile:
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Read in the first train_size lines to a training data list, and the</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># others to testing data. This allows us to test how general our model</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># is on something other than the input data.</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> line <span style="color:#f92672">in</span> infile<span style="color:#f92672">.</span>readlines()[<span style="color:#ae81ff">7</span>:]: <span style="color:#75715e"># skip header</span>
</span></span><span style="display:flex;"><span>            line <span style="color:#f92672">=</span> line<span style="color:#f92672">.</span>split()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Records = offset (x0) + remaining data points</span>
</span></span><span style="display:flex;"><span>            input <span style="color:#f92672">=</span> [float(x) <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> line[:DATA_POINTS]]
</span></span><span style="display:flex;"><span>            output <span style="color:#f92672">=</span> BENIGN <span style="color:#66d9ef">if</span> line[<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>] <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;1&#39;</span> <span style="color:#66d9ef">else</span> MALIGNANT
</span></span><span style="display:flex;"><span>            record <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#39;input&#39;</span>: input, <span style="color:#e6db74">&#39;output&#39;</span>: output}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Determine what data set to put this in</span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> len(train_data) <span style="color:#f92672">&gt;=</span> train_size:
</span></span><span style="display:flex;"><span>                test_data<span style="color:#f92672">.</span>append(record)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>                train_data<span style="color:#f92672">.</span>append(record)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> train_data, test_data
</span></span></code></pre></div><p>The next function implements the LP model described above using SoPlex and SCIP. It minimizes the sum of residuals for each training record. This amounts to summing the absolute value of the difference between predicted and observed output data. The following function takes in input and observed output data and returns a list of coefficients. Our resulting model consists of taking the <a href="https://en.wikipedia.org/wiki/Dot_product">dot product</a> of an input record and these coefficients. If the result is greater than or equal to zero, that record is predicted to be a benign tumor, otherwise it is predicted to be malignant.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> pyscipopt <span style="color:#f92672">import</span> Model
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">train_linear_model</span>(train_data):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;&#39;&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Accepts a set of input training data with known output
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    values.  Returns a list of coefficients to apply to
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    arbitrary records for purposes of binary categorization.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Make sure we have at least one training record.</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">assert</span> len(train_data) <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>    num_variables <span style="color:#f92672">=</span> len(train_data[<span style="color:#ae81ff">0</span>][<span style="color:#e6db74">&#39;input&#39;</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Variables are coefficients in front of the data points. It is important</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># that these be unrestricted in sign so they can take negative values.</span>
</span></span><span style="display:flex;"><span>    m <span style="color:#f92672">=</span> Model()
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> [m<span style="color:#f92672">.</span>addVar(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;x</span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>, lb<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>) <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(num_variables)]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Residual for each data row</span>
</span></span><span style="display:flex;"><span>    residuals <span style="color:#f92672">=</span> [m<span style="color:#f92672">.</span>addVar(lb<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>, ub<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>) <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> train_data]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> r, d <span style="color:#f92672">in</span> zip(residuals, train_data):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># r will be the absolute value of the difference between observed and</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># predicted values. We can model absolute values such as r &gt;= |foo| as:</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">#   r &gt;=  foo</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">#   r &gt;= -foo</span>
</span></span><span style="display:flex;"><span>        m<span style="color:#f92672">.</span>addCons(sum(x <span style="color:#f92672">*</span> y <span style="color:#66d9ef">for</span> x, y <span style="color:#f92672">in</span> zip(x, d[<span style="color:#e6db74">&#39;input&#39;</span>])) <span style="color:#f92672">+</span> r <span style="color:#f92672">&gt;=</span> d[<span style="color:#e6db74">&#39;output&#39;</span>])
</span></span><span style="display:flex;"><span>        m<span style="color:#f92672">.</span>addCons(sum(x <span style="color:#f92672">*</span> y <span style="color:#66d9ef">for</span> x, y <span style="color:#f92672">in</span> zip(x, d[<span style="color:#e6db74">&#39;input&#39;</span>])) <span style="color:#f92672">-</span> r <span style="color:#f92672">&lt;=</span> d[<span style="color:#e6db74">&#39;output&#39;</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Find and return coefficients that min sum of residuals.</span>
</span></span><span style="display:flex;"><span>    m<span style="color:#f92672">.</span>setObjective(sum(residuals))
</span></span><span style="display:flex;"><span>    m<span style="color:#f92672">.</span>setMinimize()
</span></span><span style="display:flex;"><span>    m<span style="color:#f92672">.</span>optimize()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    solution <span style="color:#f92672">=</span> m<span style="color:#f92672">.</span>getBestSol()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> [solution[xi] <span style="color:#66d9ef">for</span> xi <span style="color:#f92672">in</span> x]
</span></span></code></pre></div><p>We also provide a convenience function for counting the number of correct predictions by our resulting model against either the test or training data sets.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">count_correct</span>(data_set, coefficients):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;&#39;&#39;Returns the number of correct predictions.&#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>    correct <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> d <span style="color:#f92672">in</span> data_set:
</span></span><span style="display:flex;"><span>        result <span style="color:#f92672">=</span> sum(x<span style="color:#f92672">*</span>y <span style="color:#66d9ef">for</span> x, y <span style="color:#f92672">in</span> zip(coefficients, d[<span style="color:#e6db74">&#39;input&#39;</span>]))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Do we predict the same as the output?</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> (result <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">0</span>) <span style="color:#f92672">==</span> (d[<span style="color:#e6db74">&#39;output&#39;</span>] <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">0</span>):
</span></span><span style="display:flex;"><span>            correct <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> correct
</span></span></code></pre></div><p>Finally we write a main method to read in the data, build our linear model, and test its efficacy.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> pprint <span style="color:#f92672">import</span> pprint
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;__main__&#39;</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Specs for this input file</span>
</span></span><span style="display:flex;"><span>    INPUT_FILE_NAME <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;cancer1.dt&#39;</span>
</span></span><span style="display:flex;"><span>    TRAIN_SIZE <span style="color:#f92672">=</span> <span style="color:#ae81ff">350</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    train_data, test_data <span style="color:#f92672">=</span> read_proben1_cancer_data(
</span></span><span style="display:flex;"><span>        INPUT_FILE_NAME,
</span></span><span style="display:flex;"><span>        TRAIN_SIZE
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Add the offset variable to each of our data records</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> data_set <span style="color:#f92672">in</span> [train_data, test_data]:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> row <span style="color:#f92672">in</span> data_set:
</span></span><span style="display:flex;"><span>            row[<span style="color:#e6db74">&#39;input&#39;</span>] <span style="color:#f92672">=</span> [<span style="color:#ae81ff">1</span>] <span style="color:#f92672">+</span> row[<span style="color:#e6db74">&#39;input&#39;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    coefficients <span style="color:#f92672">=</span> train_linear_model(train_data)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#39;coefficients:&#39;</span>)
</span></span><span style="display:flex;"><span>    pprint(coefficients)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Print % of correct predictions for each data set</span>
</span></span><span style="display:flex;"><span>    correct <span style="color:#f92672">=</span> count_correct(train_data, coefficients)
</span></span><span style="display:flex;"><span>    print(
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#39;</span><span style="color:#e6db74">%s</span><span style="color:#e6db74"> / </span><span style="color:#e6db74">%s</span><span style="color:#e6db74"> = </span><span style="color:#e6db74">%.02f%%</span><span style="color:#e6db74"> correct on training set&#39;</span> <span style="color:#f92672">%</span> (
</span></span><span style="display:flex;"><span>            correct, len(train_data),
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">100</span> <span style="color:#f92672">*</span> float(correct) <span style="color:#f92672">/</span> len(train_data)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    correct <span style="color:#f92672">=</span> count_correct(test_data, coefficients)
</span></span><span style="display:flex;"><span>    print(
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#39;</span><span style="color:#e6db74">%s</span><span style="color:#e6db74"> / </span><span style="color:#e6db74">%s</span><span style="color:#e6db74"> = </span><span style="color:#e6db74">%.02f%%</span><span style="color:#e6db74"> correct on testing set&#39;</span> <span style="color:#f92672">%</span> (
</span></span><span style="display:flex;"><span>            correct, len(test_data),
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">100</span> <span style="color:#f92672">*</span> float(correct) <span style="color:#f92672">/</span> len(test_data)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>    )
</span></span></code></pre></div><h2 id="results">Results</h2>
<p>The result of running this model against the <code>cancer1.dt</code> data set is:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-txt" data-lang="txt"><span style="display:flex;"><span>coefficients:
</span></span><span style="display:flex;"><span>[1.4072882449702786,
</span></span><span style="display:flex;"><span> -0.14014055927954652,
</span></span><span style="display:flex;"><span> -0.6239513714263405,
</span></span><span style="display:flex;"><span> -0.26727681774258882,
</span></span><span style="display:flex;"><span> 0.067107753841131157,
</span></span><span style="display:flex;"><span> -0.28300216102808429,
</span></span><span style="display:flex;"><span> -1.0355594670918404,
</span></span><span style="display:flex;"><span> -0.22774451038152174,
</span></span><span style="display:flex;"><span> -0.69871243677663608,
</span></span><span style="display:flex;"><span> -0.072575089848659444]
</span></span><span style="display:flex;"><span>328 / 350 = 93.71% correct on training set
</span></span><span style="display:flex;"><span>336 / 349 = 96.28% correct on testing set
</span></span></code></pre></div><p>The accuracy is pretty good here against the both the training and testing sets, so this particular model generalizes well.  This is about the simplest model we can implement for data fitting, and we&rsquo;ll get to more complicated ones later, but it&rsquo;s nice to see we can do so well so quickly.  The coefficients correspond to using a function of this form, rounding off to three decimal places:</p>
<p>$$
\begin{align*}
f(x) =\ &amp;1.407 - 0.140 x_1 - 0.624 x_2 - 0.267 x_3 + 0.067 x_4 - \\
&amp;0.283 x_5 - 1.037 x_6 - 0.228 x_7 - 0.699 x_8 - 0.073 x_9
\end{align*}
$$</p>
<h2 id="resources">Resources</h2>
<ul>
<li><a href="/files/2010-11-23-data-fitting-1-linear-data-fitting/cancer1.dt"><code>cancer1.dt</code></a> data file from <code>proben1</code></li>
<li>Full <a href="/files/2010-11-23-data-fitting-1-linear-data-fitting/fit-linear.py">source listing</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>üêç Monte Carlo Simulation in Python</title>
      <link>https://ryanjoneil.github.io/posts/2009-10-08-monte-carlo-simulation-in-python/</link>
      <pubDate>Thu, 08 Oct 2009 00:00:00 +0000</pubDate>
      
      <guid>https://ryanjoneil.github.io/posts/2009-10-08-monte-carlo-simulation-in-python/</guid>
      <description>A quick introduction to writing and interpreting Monte Carlo simulations in Python</description>
      <content:encoded><![CDATA[<p><em>Note: This post was updated to work with Python 3.</em></p>
<p>One of the most useful tools one learns in an Operations Research curriculum is <a href="https://en.wikipedia.org/wiki/Monte_Carlo_method">Monte Carlo Simulation</a>. Its utility lies in its simplicity: one can learn vital information about nearly any process, be it deterministic or stochastic, without wading through the grunt work of finding an analytical solution. It can be used for off-the-cuff estimates or as a proper scientific tool. All one needs to know is how to simulate a given process and its appropriate probability distributions and parameters if that process is stochastic.</p>
<p>Here&rsquo;s how it works:</p>
<ul>
<li>Construct a simulation that, given input values, returns a value of interest. This could be a pure quantity, like time spent waiting for a bus, or a boolean indicating whether or not a particular event occurs.</li>
<li>Run the simulation a, usually large, number of times, each time with randomly generated input variables. Record its output values.</li>
<li>Compute sample mean and variance of the output values.</li>
</ul>
<p>In the case of time spent waiting for a bus, the sample mean and variance are estimators of mean and variance for one&rsquo;s wait time. In the boolean case, these represent probability that the given event will occur.</p>
<p>One can think of Monte Carlo Simulation as throwing darts. Say you want to find the area under a curve without integrating. All you must do is draw the curve on a wall and throw darts at it randomly. After you&rsquo;ve thrown enough darts, the area under the curve can be approximated using the percentage of darts that end up under the curve times the total area.</p>
<p>This technique is often performed using a spreadsheet, but that can be a bit clunky and may make more complex simulations difficult. I&rsquo;d like to spend a minute showing how it can be done in Python. Consider the following scenario:</p>
<p>Passengers for a train arrive according to a Poisson process with a mean of 100 per hour. The next train arrives exponentially with a rate of 5 per hour. How many passers will be aboard the train?</p>
<p>We can simulate this using the fact that a Poisson process can be represented as a string of events occurring with exponential inter-arrival times. We use the <code>sim()</code> function below to generate the number of passengers for random instances of the problem. We then compute sample mean and variance for these values.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> random
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>PASSENGERS <span style="color:#f92672">=</span> <span style="color:#ae81ff">100.0</span>
</span></span><span style="display:flex;"><span>TRAINS     <span style="color:#f92672">=</span>   <span style="color:#ae81ff">5.0</span>
</span></span><span style="display:flex;"><span>ITERATIONS <span style="color:#f92672">=</span> <span style="color:#ae81ff">10000</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sim</span>():
</span></span><span style="display:flex;"><span>    passengers <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Determine when the train arrives</span>
</span></span><span style="display:flex;"><span>    train <span style="color:#f92672">=</span> random<span style="color:#f92672">.</span>expovariate(TRAINS)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Count the number of passenger arrivals before the train</span>
</span></span><span style="display:flex;"><span>    now <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> <span style="color:#66d9ef">True</span>:
</span></span><span style="display:flex;"><span>        now <span style="color:#f92672">+=</span> random<span style="color:#f92672">.</span>expovariate(PASSENGERS)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> now <span style="color:#f92672">&gt;=</span> train:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">break</span>
</span></span><span style="display:flex;"><span>        passengers <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1.0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> passengers
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;__main__&#39;</span>:
</span></span><span style="display:flex;"><span>    output <span style="color:#f92672">=</span> [sim() <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(ITERATIONS)]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    total <span style="color:#f92672">=</span> sum(output)
</span></span><span style="display:flex;"><span>    mean <span style="color:#f92672">=</span> total <span style="color:#f92672">/</span> len(output)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    sum_sqrs <span style="color:#f92672">=</span> sum(x<span style="color:#f92672">*</span>x <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> output)
</span></span><span style="display:flex;"><span>    variance <span style="color:#f92672">=</span> (sum_sqrs <span style="color:#f92672">-</span> total <span style="color:#f92672">*</span> mean) <span style="color:#f92672">/</span> (len(output) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#39;E[X] = </span><span style="color:#e6db74">%.02f</span><span style="color:#e6db74">&#39;</span> <span style="color:#f92672">%</span> mean)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#39;Var(X) = </span><span style="color:#e6db74">%.02f</span><span style="color:#e6db74">&#39;</span> <span style="color:#f92672">%</span> variance)
</span></span></code></pre></div>]]></content:encoded>
    </item>
    
    <item>
      <title>‚ö°Ô∏è On the Beauty of Power Sets</title>
      <link>https://ryanjoneil.github.io/posts/2009-02-27-on-the-beauty-of-power-sets/</link>
      <pubDate>Fri, 27 Feb 2009 00:00:00 +0000</pubDate>
      
      <guid>https://ryanjoneil.github.io/posts/2009-02-27-on-the-beauty-of-power-sets/</guid>
      <description>Using power sets in algebraic modeling languages for formulating the Traveling Salesman Problem</description>
      <content:encoded><![CDATA[<p>One of the difficulties we encounter in solving the <a href="https://www.math.uwaterloo.ca/tsp/">Traveling Salesman Problem</a> (TSP) is that, for even a small numer of cities, a complete description of the problem requires a factorial number of constraints. This is apparent in the standard formulation used to teach the TSP to OR students. Consider a set of $n$ cities with the distance from city $i$ to city $j$ denoted $d_{ij}$.  We attempt to minimize the total distance of a tour entering and leaving each city exactly once. $x_{ij} = 1$ if the edge from city $i$ to city $j$ is included in the tour, $0$ otherwise:</p>
<p>$$
\small
\begin{align*}
\min\quad       &amp; z = \sum_i \sum_{j\ne i} d_{ij} x_{ij}\\
\text{s.t.}\quad&amp; \sum_{j\ne i} x_{ij} = 1 &amp;\quad\forall&amp;\ i &amp; \text{leave each city once}\\
&amp; \sum_{i\ne j} x_{ij} = 1 &amp;\quad\forall&amp;\ j &amp; \text{enter each city once}\\
&amp; x_{ij} \in \{0,1\}       &amp;\quad\forall&amp;\ i,j
\end{align*}
$$</p>
<p>This appears like a reasonable formulation until we solve it and see that our solution contains disconnected subtours. Suppose we have four cities, labeled $A$ through $D$. Connecting $A$ to $B$, $B$ to $A$, $C$ to $D$ and $D$ to $C$ provides a feasible solution to our formulation, but does not constitute a cycle. Here is a more concrete example of two disconnected subtours $\{(1,5),(5,1)\}$ and $\{(2,3),(3,4),(4,2)\}$ over five cities:</p>
<pre tabindex="0"><code>ampl: display x;
x [*,*]
:   1   2   3   4   5    :=
1   0   0   0   0   1
2   0   0   1   0   0
3   0   0   0   1   0
4   0   1   0   0   0
5   1   0   0   0   0
;
</code></pre><p>Realizing we just solved the <a href="https://en.wikipedia.org/wiki/Assignment_problem">Assignment Problem</a>, we now add subtour elimination constraints. These require that any proper, non-null subset of our $n$ cities is connected by at most $n-1$ active edges:</p>
<p>$$
\sum_{i \in S} \sum_{j \in S} x_{ij} \leq |S|-1 \quad\forall\ S \subset {1, &hellip;, n}, S \ne O
$$</p>
<p>Indexing subtour elimination constraints over a <a href="https://en.wikipedia.org/wiki/Power_set">power set</a> of the cities completes the formulation. However, this requires an additional $\sum_{k=2}^{n-1} \begin{pmatrix} n \\ k \end{pmatrix}$ rows tacked on the end of our matrix and is clearly infeasible for large $n$. The most current computers can handle using this approach <a href="http://zimpl.zib.de/download/zimpl.pdf">is around 19 cities</a>. It remains an instructive tool for understanding the <a href="https://en.wikipedia.org/wiki/Combinatorial_explosion">combinatorial explosion</a> that occurs in problems like TSP and is worth translating into a modeling language. So how does one model it on a computer?</p>
<p>Unfortunately, <a href="https://ampl.com/">AMPL</a>, the gold standard in mathematical modeling languages, is unable to index over sets. Creating a power set in AMPL requires going through a few contortions.  The following code demonstrates power and index sets over four cities:</p>
<pre tabindex="0"><code>set cities := 1 .. 4 ordered;

param n := card(cities);
set indices := 0 .. (2^n - 1);
set power {i in indices} := {c in cities: (i div 2^(ord(c) - 1)) mod 2 = 1};

display cities;
display n;
display indices;
display power;
</code></pre><p>This yields the following output:</p>
<pre tabindex="0"><code>set cities := 1 2 3 4;

n = 4

set indices := 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15;

set power[0] := ; # empty
set power[1] := 1;
set power[2] := 2;
set power[3] := 1 2;
set power[4] := 3;
set power[5] := 1 3;
set power[6] := 2 3;
set power[7] := 1 2 3;
set power[8] := 4;
set power[9] := 1 4;
set power[10] := 2 4;
set power[11] := 1 2 4;
set power[12] := 3 4;
set power[13] := 1 3 4;
set power[14] := 2 3 4;
set power[15] := 1 2 3 4;
</code></pre><p>Note how the index set contains an index for each row in our power set. We can now generate the subtour elimination constraints:</p>
<pre tabindex="0"><code>var x {cities cross cities} binary;
s.t. subtours {i in indices: card(power[i]) &gt; 1 and card(power[i]) &lt; card(cities)}:
sum {(c,k) in power[i] cross power[i]: k != c} x[c,k] &lt;= card(power[i]) - 1;

expand subtours;

subject to subtours[3]:  x[1,2] + x[2,1] &lt;= 1;
subject to subtours[5]:  x[1,3] + x[3,1] &lt;= 1;
subject to subtours[6]:  x[2,3] + x[3,2] &lt;= 1;
subject to subtours[7]:  x[1,2] + x[1,3] + x[2,1] + x[2,3] + x[3,1] + x[3,2] &lt;= 2;
subject to subtours[9]:  x[1,4] + x[4,1] &lt;= 1;
subject to subtours[10]: x[2,4] + x[4,2] &lt;= 1;
subject to subtours[11]: x[1,2] + x[1,4] + x[2,1] + x[2,4] + x[4,1] + x[4,2] &lt;= 2;
subject to subtours[12]: x[3,4] + x[4,3] &lt;= 1;
subject to subtours[13]: x[1,3] + x[1,4] + x[3,1] + x[3,4] + x[4,1] + x[4,3] &lt;= 2;
subject to subtours[14]: x[2,3] + x[2,4] + x[3,2] + x[3,4] + x[4,2] + x[4,3] &lt;= 2;
</code></pre><p>While this does work, the code for generating the power set looks like <a href="https://en.wikipedia.org/wiki/Voodoo_programming">voodoo</a>. Understanding it required piece-by-piece decomposition, an exercise I suggest you go through yourself if you have a copy of AMPL and 15 minutes to spare:</p>
<pre tabindex="0"><code>set foo {c in cities} := {ord(c)};
set bar {c in cities} := {2^(ord(c) - 1)};
set baz {i in indices} := {c in cities: i div 2^(ord(c) - 1)};
set qux {i in indices} := {c in cities: (i div 2^(ord(c) - 1)) mod 2 = 1};

display foo;
display bar;
display baz;
display qux;
</code></pre><p>This may be an instance where open source leads commercial software. The good folks who produce the <a href="https://scipopt.org/">SCIP Optimization Suite</a> provide an AMPL-like language called <a href="https://zimpl.zib.de/">ZIMPL</a> with a few additional useful features. One of these is power sets. Compared to the code above, doesn&rsquo;t this look refreshing?</p>
<pre tabindex="0"><code>set cities := {1 to 4};

set power[] := powerset(cities);
set indices := indexset(power);
</code></pre>]]></content:encoded>
    </item>
    
    <item>
      <title>üìê Uncapacitated Lot Sizing</title>
      <link>https://ryanjoneil.github.io/posts/2009-02-20-uncapacitated-lot-sizing/</link>
      <pubDate>Fri, 20 Feb 2009 00:00:00 +0000</pubDate>
      
      <guid>https://ryanjoneil.github.io/posts/2009-02-20-uncapacitated-lot-sizing/</guid>
      <description>Formulation and aspects of the Uncapacitated Lot Sizing problem in Integer Programming</description>
      <content:encoded><![CDATA[<p>Uncapacitated Lot Sizing (ULS) is a classic <a href="http://en.wikipedia.org/wiki/Operations_research">OR</a> problem that seeks to minimize the cost of satisfying known demand for a product over time.  Demand is subject to varying costs for production, set-up, and storage of the product.  Technically, it is a mixed binary integer linear program &ndash; the key point separating it from the world of <a href="http://en.wikipedia.org/wiki/Linear_programming">linear optimization</a> being that production cannot occur during any period without paying that period&rsquo;s fixed costs for set-up.  Thus it has linear nonnegative variables for production and storage amounts during each period, and a binary variable for each period that determines whether or not production can actually occur.</p>
<p>For $n$ periods with per-period fixed set-up cost $f_t$, unit production cost $p_t$, unit storage cost $h_t$,and demand $d_t$, we define decision variables related to production and storage quantities:</p>
<p>$$
\small
\begin{align*}
x_t &amp;= \text{units produced in period}\ t\\
s_t &amp;= \text{stock at the end of period}\ t\\
y_t &amp;= 1\ \text{if production occurs in period}\ t, 0\ \text{otherwise}
\end{align*}
$$</p>
<p>One can minimize overall cost for satisfying all demand on time using the following model per <a href="http://www.amazon.com/Integer-Programming-Laurence-Wolsey/dp/0471283665/">Wolsey (1998)</a>, defined slightly differently here:</p>
<p>$$
\small
\begin{align*}
\min\quad       &amp; z = \sum_t{p_t x_t} + \sum_t{h_t s_t} + \sum_t{f_t y_t}\\
\text{s.t.}\quad&amp; s_1 = d_1 + s_1\\
&amp; s_{t-1} + x_t = d_t + s_t &amp;\quad\forall&amp;\ t &gt; 1\\
&amp; x_t \leq M y_t            &amp;\quad\forall&amp;\ t\\
&amp; s_t, x_t \geq 0           &amp;\quad\forall&amp;\ t\\
&amp; y_t \in {0,1}           &amp;\quad\forall&amp;\ t
\end{align*}
$$</p>
<p>According to Wolsey, page 11, given that $s_t = \sum_{i=1}^t (x_i - d_i)$ and defining new constants $K = \sum_{t=1}^n h_t(\sum_{i=1}^t d_i)$ and $c_t = p_t + \sum_{i=t}^n h_i$, the objective function can be rewritten as $z = \sum_t c_t x_t + \sum _t f_t y_t - K$.  The book lacks a proof of this and it seems a bit non-obvious, so I attempt an explanation in somewhat painstaking detail here.</p>
<p>$$
\small
\begin{align*}
&amp;\text{Proof}:\\
&amp; &amp; \sum_t p_t x_t + \sum_t h_t s_t + \sum_t f_t y_t &amp;= \sum_t c_t x_t + \sum _t f_t y_t - K\\
&amp;\text{1. Remove} \sum_t f_t y_t:\\
&amp; &amp; \sum_t p_t x_t + \sum_t h_t s_t &amp;= \sum_t c_t x_t - K\\
&amp;\text{2. Expand}\ K\ \text{and}\ c_t:\\
&amp; &amp; \sum_t p_t x_t + \sum_t h_t s_t &amp;= \sum_t (p_t + \sum_{i=t}^n h_i) x_t - \sum_t h_t (\sum_{i=1}^t d_i)\\
&amp;\text{3. Remove}\ \sum_t p_t x_t:\\
&amp; &amp; \sum_t h_t s_t &amp;= \sum_t x_t (\sum_{i=t}^n h_i) - \sum_t h_t (\sum_{i=1}^t d_i)\\
&amp;\text{4. Expand}\ s_t:\\
&amp; &amp; \sum_t h_t (\sum_{i=1}^t x_i) - \sum_t h_t (\sum_{i=1}^t d_i) &amp;= \sum_t x_t (\sum_{i=t}^n h_i) - \sum_t h_t (\sum_{i=1}^t d_i)\\
&amp;\text{5. Remove}\ \sum_t h_t (\sum_{i=1}^t d_i):\\
&amp; &amp; \sum_t h_t (\sum_{i=1}^t x_i) &amp;= \sum_t x_t (\sum_{i=t}^n h_i)
\end{align*}
$$</p>
<p>The result from step 5 becomes obvious upon expanding its left and right-hand terms:</p>
<p>$$
h_1 x_1 + h_2 (x_1 + x_2) + \cdots + h_n (x_1 + \cdots + x_n) =\\
x_1 (h_1 + \cdots + h_n) + x2 (h_2 + \cdots + h_n) + \cdots + x_n h_n
$$</p>
<p>In matrix notation with $h$ and $x$ as column vectors in $\bf R^n$ and $L$ and $U$ being $n \times n$ lower and upper triangular identity matrices, respectively, this can be written as:</p>
<p>$$
\small
\begin{pmatrix}
h_1 \cdots h_n
\end{pmatrix}
\begin{pmatrix}
1 \cdots 0 \\
\vdots \ddots \vdots \\
1 \cdots 1
\end{pmatrix}
\begin{pmatrix}
x_1 \\
\vdots \\
x_n
\end{pmatrix} =
\begin{pmatrix}
x_1 \cdots x_n
\end{pmatrix}
\begin{pmatrix}
1 \cdots 1 \\
\vdots \ddots \vdots \\
0 \cdots 1
\end{pmatrix}
\begin{pmatrix}
h_1 \\
\vdots \\
h_n
\end{pmatrix}
$$</p>
<p>or $h^T L x = x^T U h$.</p>
]]></content:encoded>
    </item>
    
    
  </channel>
</rss>
