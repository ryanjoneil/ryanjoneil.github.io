<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>modeling on adventures in optimization</title>
    <link>https://ryanjoneil.github.io/tags/modeling/</link>
    <description>Recent content in modeling on adventures in optimization</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 26 Nov 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://ryanjoneil.github.io/tags/modeling/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>üìÖ Reducing Overscheduling</title>
      <link>https://ryanjoneil.github.io/posts/2023-11-26-reducing-overscheduling/</link>
      <pubDate>Sun, 26 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://ryanjoneil.github.io/posts/2023-11-26-reducing-overscheduling/</guid>
      <description>Minimize overbooking while scheduling team meetings</description>
      <content:encoded><![CDATA[<p>At a <a href="https://nextmv.io">Nextmv</a> <a href="https://www.youtube.com/watch?v=XTeit7TAWj4">tech talk</a> a couple weeks ago, I showed a <a href="https://en.wikipedia.org/wiki/Least_absolute_deviations">least absolute deviations</a> (LAD) regression model using OR-Tools. This isn&rsquo;t new &ndash; I pulled the formulation from Rob Vanderbei&rsquo;s &ldquo;<a href="https://vanderbei.princeton.edu/tex/LocalWarming/LocalWarmingSIREVrev.pdf">Local Warming</a>&rdquo; paper, and I&rsquo;ve shown similar models at conference talks in the past using other modeling APIs and solvers.</p>
<p>There are a couple reasons I keep coming back to this problem. One is that it&rsquo;s a great example of how to build a machine learning model using an optimization solver. Unless you have an optimization background, it&rsquo;s probably not obvious you can do this. Building a regression or classification model with a solver directly is a great way to understand the model better. And you can customize it in interesting ways, like adding <a href="https://www.robots.ox.ac.uk/~az/lectures/ml/2011/lect6.pdf">epsilon insensitivity</a>.</p>
<p>Another is that <a href="https://en.wikipedia.org/wiki/Least_squares">least squares</a>, while most commonly used regression form, has a fatal flaw: it isn&rsquo;t robust to outliers in the input data. This is because least squares minimize the <em>sum of squared residuals</em>, as shown in the formulation below. Here, $A$ is an $m \times n$ matrix of feature data, $b$ is a vector of observations to fit, and $x$ is a vector of coefficients the optimizer must find.</p>
<p>$$
\min f(x) = \Vert Ax-b \Vert_2^2
$$</p>
<p>Since the objective function minimizes squared residuals, outliers have a much bigger impact than other data. LAD regression solves this by simply summing the values of the residuals as they are.</p>
<p>$$
\min f(x) = \vert Ax-b \vert
$$</p>
<p>So why isn&rsquo;t this used more? Simple &ndash; least squares has a convenient analytical solution, while LAD requires an algorithm to solve. For instance, you can formulate LAD regression as a linear program, but now you need a solver.</p>
<p>$$
\begin{align*}
\min         \quad &amp; 1&rsquo;z \\
\text{s.t.}\ \quad &amp; z \ge Ax - b \\
&amp; z \ge b - Ax
\end{align*}
$$</p>
<p>While I like using this example, it paints a rather negative picture of squaring. If it does funny things to solvers, is there any good reason to square? Thus I&rsquo;ve been on the lookout for a practical example where squaring a variable or expression makes a model more useful.</p>
<p>Luckily for me, Erwin Kalvelagen recently <a href="https://yetanothermathprogrammingconsultant.blogspot.com/2023/10/scheduling-team-meetings.html">posted</a> about using optimization to schedule team meetings. This is an application where minimizing squared values of <em>overbooking</em> can be beneficial &ndash; it may be worse to be triple booked than double booked.</p>
<p>I won&rsquo;t recreate the reasoning behind Erwin&rsquo;s post here. You can read his blog for that. What we&rsquo;ll do is look at both the formulations in his post, along with a couple extras using <a href="https://julialang.org/">Julia</a> for code, <a href="https://jump.dev/">JuMP</a> for modeling, <a href="https://www.scipopt.org/">SCIP</a> for optimization, and <a href="https://gadflyjl.org/stable/">Gadfly</a> for visualization. All model code and data are linked in the resources section at the end.</p>
<h2 id="maximize-attendance">Maximize attendance</h2>
<p>To start off, I built a new data set, which you can find in the resources section. This differentiates team membership between two types of employees: individual contributors (starting with <code>ic</code> in the data), who attend meetings for 1 or 2 teams, and managers (prefixed with <code>mgr</code>), who attend meetings to coordinate across multiple teams. We schedule meetings for 10 teams (prefix <code>t</code>) into 3 time slots (<code>s</code>).</p>
<p>The first model in Erwin&rsquo;s post maximizes attendance. This means it tries to schedule team members for as many unique time slots as possible. It doesn&rsquo;t consider overbooking.</p>
<p>$$
\begin{align*}
\max\quad       &amp; \sum_{i,s} y_{i,s} \\
\text{s.t.}\quad&amp; \sum_{s} x_{t,s} = 1                  &amp;\quad\forall&amp;\ t   &amp; \text{schedule each team meeting once}\\
&amp; y_{i,s} \le \sum_{t} m_{i,t}\ x_{t,s} &amp;\quad\forall&amp;\ i,s &amp; \text{individuals attend team meetings}\\
&amp; x_{t,s} \in \{0,1\}                 &amp;\quad\forall&amp;\ t,s\\
&amp; y_{i,s} \in \{0,1\}                 &amp;\quad\forall&amp;\ i,s
\end{align*}
$$</p>
<p>This yields the following team schedule, with red representing a scheduled team meeting.</p>
<p><img loading="lazy" src="/files/2023-11-26-reducing-overscheduling/maximize-attendance-teams.svg" alt="Maximize attendance - team schedules"  />
</p>
<p>If we look at the manager schedules, we&rsquo;ll see that every manager is completely booked. This makes sense. That&rsquo;s what managers do, right? Go to meetings?</p>
<p><img loading="lazy" src="/files/2023-11-26-reducing-overscheduling/maximize-attendance-managers.svg" alt="Maximize attendance - manager attendance"  />
</p>
<h2 id="minimize-overbooking">Minimize overbooking</h2>
<p>The model gets more interesting once we account for overbooking. Erwin&rsquo;s post has a model that minimizes overbooking, where overbooking is the number of additional meetings in a time slot. If a team member is double booked, that&rsquo;s 1 overbooking. If they are triple booked, that&rsquo;s 2 overbookings.</p>
<h3 id="sum-of-overbooking">Sum of overbooking</h3>
<p>The second model in Erwin&rsquo;s post minimizes the sum of all overbookings. He does this by adding a continuous <code>c</code> vector that only incurs value once a team member goes over a single meeting in a given time slot.</p>
<p>$$
\begin{align*}
\min\quad       &amp; \sum_{i,s} c_{i,s} \\
\text{s.t.}\quad&amp; \sum_{s} x_{t,s} = 1                      &amp;\quad\forall&amp;\ t   &amp; \text{schedule each team meeting once}\\
&amp; c_{i,s} \ge \sum_{t} m_{i,t}\ x_{t,s} - 1 &amp;\quad\forall&amp;\ i,s &amp; \text{measure overbooking}\\
&amp; x_{t,s} \in \{0,1\}                     &amp;\quad\forall&amp;\ t,s\\
&amp; c_{i,s} \ge 0                             &amp;\quad\forall&amp;\ i,s
\end{align*}
$$</p>
<p>Given our data this results in the following team schedule, which is probably not all that interesting. I&rsquo;ll leave this visualization out from now on.</p>
<p><img loading="lazy" src="/files/2023-11-26-reducing-overscheduling/minimize-overbooking-teams.svg" alt="Minimize overbooking - team schedules"  />
</p>
<p>Where it gets interesting is plotting overbookings for the managers. Here we see that 3 manager time slots are triple booked <em>(red)</em>, while 8 are double booked <em>(gray)</em>.</p>
<p><img loading="lazy" src="/files/2023-11-26-reducing-overscheduling/minimize-overbooking-managers.svg" alt="Minimize overbooking - manager overbooking"  />
</p>
<h3 id="sum-of-squared-overbooking">Sum of squared overbooking</h3>
<p>Let&rsquo;s say it&rsquo;s worse to triple book (or, gasp, <em>quadruple</em> book) than to double book. How can the model account for this? One answer, if you have a MIQP-enabled solver, is to simply square the <code>c</code> values.</p>
<p>$$
\begin{align*}
\min\quad       &amp; \sum_{i,s} c_{i,s}^2 \\
\text{s.t.}\quad&amp; \sum_{s} x_{t,s} = 1                      &amp;\quad\forall&amp;\ t   &amp; \text{schedule each team meeting once}\\
&amp; c_{i,s} \ge \sum_{t} m_{i,t}\ x_{t,s} - 1 &amp;\quad\forall&amp;\ i,s &amp; \text{measure overbooking}\\
&amp; x_{t,s} \in \{0,1\}                     &amp;\quad\forall&amp;\ t,s\\
&amp; c_{i,s} \ge 0                             &amp;\quad\forall&amp;\ i,s
\end{align*}
$$</p>
<p>This completely eliminates triple booking, as shown below. No manager is worse off than being double booked, which seems normal given my experiences.</p>
<p><img loading="lazy" src="/files/2023-11-26-reducing-overscheduling/minimize-overbooking-squared-managers.svg" alt="Minimize squared overbooking - manager overbooking"  />
</p>
<p>The problem with this is that the solver now takes a lot longer. It&rsquo;s not bad for the data in this example, but if you try it with something larger you&rsquo;ll see what I mean. You can find the data generator code in the resources section.</p>
<h3 id="constrained-bottleneck">Constrained bottleneck</h3>
<p>So how can we do something similar without the computational cost? One option is to continue using MILP formulations, but in the context of hierarchical optimization. This means splitting the model into two. First, we try to minimize the maximum overbookings for any team member (the <em>bottleneck</em>, if you will). This involves adding a variable $b$ representing that maximum.</p>
<p>$$ b = \max\Bigl\{\sum_{t} m_{i,t}\ x_{t,s} - 1 : i \in I, s \in S \Bigr\} $$</p>
<p>Now we can simply minimize $b$ using a MILP instead of a MIQP.</p>
<p>$$
\begin{align*}
\min\quad       &amp; b \\
\text{s.t.}\quad&amp; \sum_{s} x_{t,s} = 1                &amp;\quad\forall&amp;\ t   &amp; \text{schedule each team meeting once}\\
&amp; b \ge \sum_{t} m_{i,t}\ x_{t,s} - 1 &amp;\quad\forall&amp;\ i,s &amp; \text{maximum overbooking}\\
&amp; x_{t,s} \in \{0,1\}               &amp;\quad\forall&amp;\ t,s
\end{align*}
$$</p>
<p>Once we solve the first model, we get the minimal value of $b$, which we call $b^*$. We can simply use $b^*$ as an upper bound for overbookings in the second original model.</p>
<p>$$
\begin{align*}
\min\quad       &amp; \sum_{i,s} c_{i,s} \\
\text{s.t.}\quad&amp; \sum_{s} x_{t,s} = 1                      &amp;\quad\forall&amp;\ t   &amp; \text{schedule each team meeting once}\\
&amp; c_{i,s} \ge \sum_{t} m_{i,t}\ x_{t,s} - 1 &amp;\quad\forall&amp;\ i,s &amp; \text{measure overbooking}\\
&amp; x_{t,s} \in \{0,1\}                     &amp;\quad\forall&amp;\ t,s\\
&amp; 0 \le c_{i,s} \le b^*                     &amp;\quad\forall&amp;\ i,s
\end{align*}
$$</p>
<p>As we see below, this model also eliminates triple bookings, and it&rsquo;s quite a bit faster to solve than the MIQP.</p>
<p><img loading="lazy" src="/files/2023-11-26-reducing-overscheduling/minimize-bottleneck-managers.svg" alt="Minimize bottleneck - manager overbooking"  />
</p>
<h2 id="resources">Resources</h2>
<ul>
<li><a href="/files/2023-11-26-reducing-overscheduling/main.go"><code>main.go</code></a> generates input data</li>
<li><a href="/files/2023-11-26-reducing-overscheduling/membership.csv"><code>membership.csv</code></a> contains input data</li>
<li><a href="/files/2023-11-26-reducing-overscheduling/maximize-attendance.jl"><code>maximize-attendance.jl</code></a> MILP model</li>
<li><a href="/files/2023-11-26-reducing-overscheduling/minimize-overbooking.jl"><code>minimize-overbooking.jl</code></a> MILP model</li>
<li><a href="/files/2023-11-26-reducing-overscheduling/minimize-overbooking-squared.jl"><code>minimize-overbooking-squared.jl</code></a> MIQP model</li>
<li><a href="/files/2023-11-26-reducing-overscheduling/minimize-bottleneck.jl"><code>minimize-bottleneck.jl</code></a> hierarchical MILP models</li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>üßê Data Fitting 2a - Very, Very Simple Linear Regression in R</title>
      <link>https://ryanjoneil.github.io/posts/2011-02-16-data-fitting-2a-very-very-simple-linear-regression-in-r/</link>
      <pubDate>Wed, 16 Feb 2011 00:00:00 +0000</pubDate>
      
      <guid>https://ryanjoneil.github.io/posts/2011-02-16-data-fitting-2a-very-very-simple-linear-regression-in-r/</guid>
      <description>Predict how much people like cats and dogs based on their ice cream preferences. Also, R.</description>
      <content:encoded><![CDATA[<p><em>Note: This post was updated to include an example data file.</em></p>
<p>I thought it might be useful to follow up the <a href="../2011-02-15-data-fitting-2-very-very-simple-linear-regression-in-python/">last post</a> with another one showing the same examples in R.</p>
<p>R provides a function called <code>lm</code>, which is similar in spirit to <a href="https://numpy.org/">NumPy</a>&rsquo;s <code>linalg.lstsq</code>. As you&rsquo;ll see, <code>lm</code>&rsquo;s interface is a bit more tuned to the concepts of modeling.<!-- raw HTML omitted --></p>
<p>We begin by reading in the <a href="/files/2011-02-16-data-fitting-2a-very-very-simple-linear-regression-in-r/example_data.csv">example CSV</a> into a data frame:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>responses <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">read.csv</span>(<span style="color:#e6db74">&#39;example_data.csv&#39;</span>)
</span></span><span style="display:flex;"><span>responses
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-txt" data-lang="txt"><span style="display:flex;"><span>  respondent vanilla.love strawberry.love chocolate.love dog.love cat.love
</span></span><span style="display:flex;"><span>1     Aylssa            9               4              9        9        9
</span></span><span style="display:flex;"><span>2       Ben8            8               6              4       10        4
</span></span><span style="display:flex;"><span>3         Cy            9               4              8        2        6
</span></span><span style="display:flex;"><span>4        Eva            3               7              9        4        6
</span></span><span style="display:flex;"><span>5        Lem            6               8              5        2        5
</span></span><span style="display:flex;"><span>6      Louis            4               5              3       10        3
</span></span></code></pre></div><p>A data frame is sort of like a matrix, but with named columns. That is, we can refer to entire columns using the dollar sign. We are now ready to run least squares. We&rsquo;ll create the model for predicting &ldquo;dog love.&rdquo;  To create the &ldquo;cat love&rdquo; model, simply use that column name instead:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>fit1 <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">lm</span>(
</span></span><span style="display:flex;"><span>  responses<span style="color:#f92672">$</span>dog.love <span style="color:#f92672">~</span> responses<span style="color:#f92672">$</span>vanilla.love <span style="color:#f92672">+</span>
</span></span><span style="display:flex;"><span>                       responses<span style="color:#f92672">$</span>strawberry.love <span style="color:#f92672">+</span> 
</span></span><span style="display:flex;"><span>                       responses<span style="color:#f92672">$</span>chocolate.love
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>The syntax for lm is a little off-putting at first.  This call tells it to create a model for &ldquo;dog love&rdquo; with respect to <em>(the ~)</em> a function of the form <em>offset + x1 * vanilla love + x2 * strawberry love + x3 * chocolate love</em>. Note that the offset is conveniently implied when using <code>lm</code>, so this is the same as the second model we created in Python. Now that we&rsquo;ve computed the coefficients for our &ldquo;dog love&rdquo; model, we can ask R about it:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span><span style="color:#a6e22e">summary</span>(fit1)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-txt" data-lang="txt"><span style="display:flex;"><span>Call:
</span></span><span style="display:flex;"><span>lm(formula = responses$dog.love ~ responses$vanilla.love + responses$strawberry.love + 
</span></span><span style="display:flex;"><span>    responses$chocolate.love)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Residuals:
</span></span><span style="display:flex;"><span>      1       2       3       4       5       6 
</span></span><span style="display:flex;"><span> 3.1827  2.9436 -4.5820  0.8069 -1.9856 -0.3657 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Coefficients:
</span></span><span style="display:flex;"><span>                          Estimate Std. Error t value Pr(&gt;|t|)
</span></span><span style="display:flex;"><span>(Intercept)                20.9298    15.0654   1.389    0.299
</span></span><span style="display:flex;"><span>responses$vanilla.love     -0.2783     0.9934  -0.280    0.806
</span></span><span style="display:flex;"><span>responses$strawberry.love  -1.4314     1.5905  -0.900    0.463
</span></span><span style="display:flex;"><span>responses$chocolate.love   -0.7647     0.8214  -0.931    0.450
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Residual standard error: 4.718 on 2 degrees of freedom
</span></span><span style="display:flex;"><span>Multiple R-squared:  0.4206,	Adjusted R-squared:  -0.4485 
</span></span><span style="display:flex;"><span>F-statistic: 0.484 on 3 and 2 DF,  p-value: 0.7272
</span></span></code></pre></div><p>This gives us quite a bit of information, including the coefficients for our &ldquo;dog love&rdquo; model and various error metrics. You can find the offset and coefficients under the Estimate column above. We quickly verify this using R&rsquo;s vectorized arithmetic:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span><span style="color:#ae81ff">20.9298</span> <span style="color:#f92672">-</span> 
</span></span><span style="display:flex;"><span>  <span style="color:#ae81ff">0.2783</span> <span style="color:#f92672">*</span> responses<span style="color:#f92672">$</span>vanilla.love <span style="color:#f92672">-</span>
</span></span><span style="display:flex;"><span>  <span style="color:#ae81ff">1.4314</span> <span style="color:#f92672">*</span> responses<span style="color:#f92672">$</span>strawberry.love <span style="color:#f92672">-</span>
</span></span><span style="display:flex;"><span>  <span style="color:#ae81ff">0.7647</span> <span style="color:#f92672">*</span> responses<span style="color:#f92672">$</span>chocolate.love
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-txt" data-lang="txt"><span style="display:flex;"><span>[1]  5.8172  7.0562  6.5819  3.1928  3.9853 10.3655
</span></span></code></pre></div><p>You&rsquo;ll notice the model is essentially the same as the one we got from NumPy. Our next step is to add in the squared inputs. We do this by adding extra terms to the modeling formula. The <code>I()</code> function allows us to easily add additional operators to columns. That&rsquo;s how we accomplish the squaring. We could alternatively add squared input values to the data frame, but using <code>I()</code> is more convenient and natural.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span>fit2 <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">lm</span>(responses<span style="color:#f92672">$</span>dog.love <span style="color:#f92672">~</span> responses<span style="color:#f92672">$</span>vanilla.love <span style="color:#f92672">+</span>
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">I</span>(responses<span style="color:#f92672">$</span>vanilla.love^2) <span style="color:#f92672">+</span> responses<span style="color:#f92672">$</span>strawberry.love <span style="color:#f92672">+</span>
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">I</span>(responses<span style="color:#f92672">$</span>strawberry.love^2) <span style="color:#f92672">+</span> responses<span style="color:#f92672">$</span>chocolate.love <span style="color:#f92672">+</span>
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">I</span>(responses<span style="color:#f92672">$</span>chocolate.love^2))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">summary</span>(fit2)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-txt" data-lang="txt"><span style="display:flex;"><span>Call:
</span></span><span style="display:flex;"><span>lm(formula = responses$dog.love ~ responses$vanilla.love + I(responses$vanilla.love^2) + 
</span></span><span style="display:flex;"><span>    responses$strawberry.love + I(responses$strawberry.love^2) + 
</span></span><span style="display:flex;"><span>    responses$chocolate.love + I(responses$chocolate.love^2))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Residuals:
</span></span><span style="display:flex;"><span>ALL 6 residuals are 0: no residual degrees of freedom!
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Coefficients: (1 not defined because of singularities)
</span></span><span style="display:flex;"><span>                               Estimate Std. Error t value Pr(&gt;|t|)
</span></span><span style="display:flex;"><span>(Intercept)                    -357.444        NaN     NaN      NaN
</span></span><span style="display:flex;"><span>responses$vanilla.love           72.444        NaN     NaN      NaN
</span></span><span style="display:flex;"><span>I(responses$vanilla.love^2)      -6.111        NaN     NaN      NaN
</span></span><span style="display:flex;"><span>responses$strawberry.love        59.500        NaN     NaN      NaN
</span></span><span style="display:flex;"><span>I(responses$strawberry.love^2)   -5.722        NaN     NaN      NaN
</span></span><span style="display:flex;"><span>responses$chocolate.love          7.000        NaN     NaN      NaN
</span></span><span style="display:flex;"><span>I(responses$chocolate.love^2)        NA         NA      NA       NA
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Residual standard error: NaN on 0 degrees of freedom
</span></span><span style="display:flex;"><span>Multiple R-squared:      1,	Adjusted R-squared:    NaN 
</span></span><span style="display:flex;"><span>F-statistic:   NaN on 5 and 0 DF,  p-value: NA
</span></span></code></pre></div><p>We can see that we get the same &ldquo;dog love&rdquo; model as produced by the third Python version of the last post. Again, we quickly verify that the output is the same (minus some rounding errors):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-r" data-lang="r"><span style="display:flex;"><span><span style="color:#ae81ff">-357.444</span> <span style="color:#f92672">+</span> 
</span></span><span style="display:flex;"><span>  <span style="color:#ae81ff">72.444</span> <span style="color:#f92672">*</span> responses<span style="color:#f92672">$</span>vanilla.love <span style="color:#f92672">-</span>
</span></span><span style="display:flex;"><span>  <span style="color:#ae81ff">6.111</span> <span style="color:#f92672">*</span> responses<span style="color:#f92672">$</span>vanilla.love^2 <span style="color:#f92672">+</span>
</span></span><span style="display:flex;"><span>  <span style="color:#ae81ff">59.5</span> <span style="color:#f92672">*</span> responses<span style="color:#f92672">$</span>strawberry.love <span style="color:#f92672">-</span>
</span></span><span style="display:flex;"><span>  <span style="color:#ae81ff">5.722</span> <span style="color:#f92672">*</span> responses<span style="color:#f92672">$</span>strawberry.love^2 <span style="color:#f92672">+</span>
</span></span><span style="display:flex;"><span>  <span style="color:#ae81ff">7</span> <span style="color:#f92672">*</span> responses<span style="color:#f92672">$</span>chocolate.love
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-txt" data-lang="txt"><span style="display:flex;"><span>[1]  9.009 10.012  2.009  4.011  2.016 10.006
</span></span></code></pre></div>]]></content:encoded>
    </item>
    
    <item>
      <title>üßê Data Fitting 2 - Very, Very Simple Linear Regression in Python</title>
      <link>https://ryanjoneil.github.io/posts/2011-02-15-data-fitting-2-very-very-simple-linear-regression-in-python/</link>
      <pubDate>Tue, 15 Feb 2011 00:00:00 +0000</pubDate>
      
      <guid>https://ryanjoneil.github.io/posts/2011-02-15-data-fitting-2-very-very-simple-linear-regression-in-python/</guid>
      <description>Predict how much people like cats and dogs based on their ice cream preferences. Also, Python and numpy.</description>
      <content:encoded><![CDATA[<p>This post is based on a memo I sent to some former colleagues at the Post. I&rsquo;ve edited it for use here since it fits well as the second in a series on simple data fitting techniques. If you&rsquo;re among the many enlightened individuals already using regression analysis, then this post is probably not for you. If you aren&rsquo;t, then hopefully this provides everything you need to develop rudimentary predictive models that yield surprising levels of accuracy.</p>
<h2 id="data">Data</h2>
<p>For purposes of a simple working example, we have collected six records of input data over three dimensions with the goal of predicting two outputs. The input data are:</p>
<p>$$
\begin{align*}
x_1 &amp;= \text{How much a respondent likes vanilla [0-10]}\\
x_2 &amp;= \text{How much a respondent likes strawberry [0-10]}\\
x_3 &amp;= \text{How much a respondent likes chocolate [0-10]}
\end{align*}
$$</p>
<p>Output data consist of:</p>
<p>$$
\begin{align*}
b_1 &amp;= \text{How much a respondent likes dogs [0-10]}\\
b_2 &amp;= \text{How much a respondent likes cats [0-10]}
\end{align*}
$$</p>
<p>Below are anonymous data collected from a random sample of people.</p>
<table>
<thead>
<tr>
<th>respondent</th>
<th style="text-align:right">vanilla ‚ù§Ô∏è</th>
<th style="text-align:right">strawberry ‚ù§Ô∏è</th>
<th style="text-align:right">chocolate ‚ù§Ô∏è</th>
<th style="text-align:right">dog ‚ù§Ô∏è</th>
<th style="text-align:right">cat ‚ù§Ô∏è</th>
</tr>
</thead>
<tbody>
<tr>
<td>Alyssa P Hacker</td>
<td style="text-align:right">9</td>
<td style="text-align:right">4</td>
<td style="text-align:right">9</td>
<td style="text-align:right">9</td>
<td style="text-align:right">8</td>
</tr>
<tr>
<td>Ben Bitdiddle</td>
<td style="text-align:right">8</td>
<td style="text-align:right">6</td>
<td style="text-align:right">4</td>
<td style="text-align:right">10</td>
<td style="text-align:right">4</td>
</tr>
<tr>
<td>Cy D. Fect</td>
<td style="text-align:right">9</td>
<td style="text-align:right">4</td>
<td style="text-align:right">8</td>
<td style="text-align:right">2</td>
<td style="text-align:right">6</td>
</tr>
<tr>
<td>Eva Lu Ator</td>
<td style="text-align:right">3</td>
<td style="text-align:right">7</td>
<td style="text-align:right">9</td>
<td style="text-align:right">4</td>
<td style="text-align:right">6</td>
</tr>
<tr>
<td>Lem E. Tweakit</td>
<td style="text-align:right">6</td>
<td style="text-align:right">8</td>
<td style="text-align:right">5</td>
<td style="text-align:right">2</td>
<td style="text-align:right">5</td>
</tr>
<tr>
<td>Louis Reasoner</td>
<td style="text-align:right">4</td>
<td style="text-align:right">5</td>
<td style="text-align:right">3</td>
<td style="text-align:right">10</td>
<td style="text-align:right">3</td>
</tr>
</tbody>
</table>
<p>Our input is in three dimensions. Each output requires its own model, so we&rsquo;ll have one for dogs and one for cats. We&rsquo;re looking for functions, <code>dog(x)</code> and <code>cat(x)</code>, that can predict $b_1$ and $b_2$ based on given values of $x_1$, $x_2$, and $x_3$.</p>
<h2 id="model-1">Model 1</h2>
<p>For both models we want to find parameters that minimize their squared residuals (read: errors). There&rsquo;s a number of names for this. Optimization folks like to think of it as unconstrained quadratic optimization, but it&rsquo;s more common to call it least squares or linear regression. It&rsquo;s not necessary to entirely understand why for our purposes, but the function that minimizes these errors is:</p>
<p>$$\beta = ({A^t}A)^{-1}{A^t}b$$</p>
<p>This is implemented for you in the <code>numpy.linalg</code> Python package, which we&rsquo;ll use for examples. Much more information than you probably want can be found <a href="http://en.wikipedia.org/wiki/Least_squares">here</a>.</p>
<p>Below is a first stab at a Python version. It runs least squares against our input and output data exactly as they are. You can see the matrix $A$ and outputs $b_1$ and $b_2$ (dog and cat love, respectively) are represented just as they are in the table.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Version 1: No offset, no squared inputs</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>A <span style="color:#f92672">=</span> numpy<span style="color:#f92672">.</span>vstack([
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">9</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">4</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">8</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">9</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">5</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">3</span>]
</span></span><span style="display:flex;"><span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>b1 <span style="color:#f92672">=</span> numpy<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">10</span>])
</span></span><span style="display:flex;"><span>b2 <span style="color:#f92672">=</span> numpy<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">3</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;dog ‚ù§Ô∏è:&#39;</span>, numpy<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>lstsq(A, b1, rcond<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>)[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;cat ‚ù§Ô∏è:&#39;</span>, numpy<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>lstsq(A, b2, rcond<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>)[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Output:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># dog ‚ù§Ô∏è: [0.72548294      0.53045642     -0.29952361]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># cat ‚ù§Ô∏è: [2.36110929e-01  2.61934385e-05  6.26892476e-01]</span>
</span></span></code></pre></div><p>The resulting model is:</p>
<pre tabindex="0"><code>dog(x) = 0.72548294 * x1 + 0.53045642 * x2 - 0.29952361 * x3
cat(x) = 2.36110929e-01 * x1 + 2.61934385e-05 * x2 + 6.26892476e-01 * x3
</code></pre><p>The coefficients before our variables correspond to beta in the formula above. Errors between observed and predicted data, shown below, are calculated and summed. For these six records, <code>dog(x)</code> has a total error of 20.76 and <code>cat(x)</code> has 3.74. Not great.</p>
<table>
<thead>
<tr>
<th>respondent</th>
<th style="text-align:right">predicted b1</th>
<th style="text-align:right">b1 error</th>
<th style="text-align:right">predicted b2</th>
<th style="text-align:right">b2 error</th>
</tr>
</thead>
<tbody>
<tr>
<td>Alyssa P Hacker</td>
<td style="text-align:right">5.96</td>
<td style="text-align:right">3.04</td>
<td style="text-align:right">7.77</td>
<td style="text-align:right">1.23</td>
</tr>
<tr>
<td>Ben Bitdiddle</td>
<td style="text-align:right">7.79</td>
<td style="text-align:right">2.21</td>
<td style="text-align:right">4.40</td>
<td style="text-align:right">0.40</td>
</tr>
<tr>
<td>Cy D. Fect</td>
<td style="text-align:right">6.25</td>
<td style="text-align:right">4.25</td>
<td style="text-align:right">7.14</td>
<td style="text-align:right">1.14</td>
</tr>
<tr>
<td>Eva Lu Ator</td>
<td style="text-align:right">3.19</td>
<td style="text-align:right">0.81</td>
<td style="text-align:right">6.35</td>
<td style="text-align:right">0.35</td>
</tr>
<tr>
<td>Lem E. Tweakit</td>
<td style="text-align:right">7.10</td>
<td style="text-align:right">5.10</td>
<td style="text-align:right">4.55</td>
<td style="text-align:right">0.45</td>
</tr>
<tr>
<td>Louis Reasoner</td>
<td style="text-align:right">4.66</td>
<td style="text-align:right">5.34</td>
<td style="text-align:right">2.83</td>
<td style="text-align:right">0.17</td>
</tr>
<tr>
<td>Total error:</td>
<td style="text-align:right"></td>
<td style="text-align:right">20.76</td>
<td style="text-align:right"></td>
<td style="text-align:right">3.74</td>
</tr>
</tbody>
</table>
<h2 id="model-2">Model 2</h2>
<p>One problem with this model is that <code>dog(x)</code> and <code>cat(x)</code> are forced to pass through the origin. <em>(Why is that?)</em> We can improve it somewhat if we add an offset. This amounts to prepending 1 to every row in $A$ and adding a constant to the resulting functions. You can see the very slight difference between the code for this model and that of the previous:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Version 2: Offset, no squared inputs</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>A <span style="color:#f92672">=</span> numpy<span style="color:#f92672">.</span>vstack([
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">9</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">4</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">8</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">9</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">5</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">3</span>]
</span></span><span style="display:flex;"><span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;dog ‚ù§Ô∏è:&#39;</span>, numpy<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>lstsq(A, b1, rcond<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>)[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;cat ‚ù§Ô∏è:&#39;</span>, numpy<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>lstsq(A, b2, rcond<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>)[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Output:</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># dog ‚ù§Ô∏è: [20.92975427  -0.27831197  -1.43135684  -0.76469017]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># cat ‚ù§Ô∏è: [-0.31744124   0.25133547   0.02978098   0.63394765]</span>
</span></span></code></pre></div><p>This yields the seconds version of our models:</p>
<pre tabindex="0"><code>dog(x) = 20.92975427 - 0.27831197 * x1 - 1.43135684 * x2 - 0.76469017 * x3
cat(x) = -0.31744124 + 0.25133547 * x1 + 0.02978098 * x2 + 0.63394765 * x3
</code></pre><p>These models provide errors of 13.87 and 3.79.  A little better on the dog side, but still not quite usable.</p>
<table>
<thead>
<tr>
<th>respondent</th>
<th style="text-align:right">predicted b1</th>
<th style="text-align:right">b1 error</th>
<th style="text-align:right">predicted b2</th>
<th style="text-align:right">b2 error</th>
</tr>
</thead>
<tbody>
<tr>
<td>Alyssa P Hacker</td>
<td style="text-align:right">5.82</td>
<td style="text-align:right">3.18</td>
<td style="text-align:right">7.77</td>
<td style="text-align:right">1.23</td>
</tr>
<tr>
<td>Ben Bitdiddle</td>
<td style="text-align:right">7.06</td>
<td style="text-align:right">2.94</td>
<td style="text-align:right">4.41</td>
<td style="text-align:right">0.41</td>
</tr>
<tr>
<td>Cy D. Fect</td>
<td style="text-align:right">6.58</td>
<td style="text-align:right">4.58</td>
<td style="text-align:right">7.14</td>
<td style="text-align:right">1.14</td>
</tr>
<tr>
<td>Eva Lu Ator</td>
<td style="text-align:right">3.19</td>
<td style="text-align:right">0.81</td>
<td style="text-align:right">6.35</td>
<td style="text-align:right">0.35</td>
</tr>
<tr>
<td>Lem E. Tweakit</td>
<td style="text-align:right">3.99</td>
<td style="text-align:right">1.99</td>
<td style="text-align:right">4.60</td>
<td style="text-align:right">0.40</td>
</tr>
<tr>
<td>Louis Reasoner</td>
<td style="text-align:right">10.37</td>
<td style="text-align:right">0.37</td>
<td style="text-align:right">2.74</td>
<td style="text-align:right">0.26</td>
</tr>
<tr>
<td>Total error:</td>
<td style="text-align:right"></td>
<td style="text-align:right">13.87</td>
<td style="text-align:right"></td>
<td style="text-align:right">3.79</td>
</tr>
</tbody>
</table>
<h2 id="model-3">Model 3</h2>
<p>The problem is that <code>dog(x)</code> and <code>cat(x)</code> are linear functions. Most observed data don&rsquo;t conform to straight lines. Take a moment and draw the line $f(x) = x$ and the curve $f(x) = x^2$. The former makes a poor approximation of the latter.</p>
<p>Most of the time, people just use squares of the input data to add curvature to their models. We do this in our next version of the code by just adding squares of the input row values to our $A$ matrix. Everything else is the same. (In reality, you can add any function of the input data you feel best models the data, if you understand it well enough.)</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Version 3: Offset with squared inputs</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>A <span style="color:#f92672">=</span> numpy<span style="color:#f92672">.</span>vstack([
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">9</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">4</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">9</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">8</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">6</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">4</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">9</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">4</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">8</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">7</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">9</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">6</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">8</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">4</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span><span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>]
</span></span><span style="display:flex;"><span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>b1 <span style="color:#f92672">=</span> numpy<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">10</span>])
</span></span><span style="display:flex;"><span>b2 <span style="color:#f92672">=</span> numpy<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">3</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;dog ‚ù§Ô∏è:&#39;</span>, numpy<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>lstsq(A, b1, rcond<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>)[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;cat ‚ù§Ô∏è:&#39;</span>, numpy<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>lstsq(A, b2, rcond<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>)[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># dog ‚ù§Ô∏è: [1.29368307  7.03633306  -0.44795498  9.98093332</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#  -0.75689575  -19.00757486  1.52985734]</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># cat ‚ù§Ô∏è: [0.47945896  5.30866067  -0.39644128 -1.28704188</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#   0.12634295   -4.32392606  0.43081918]</span>
</span></span></code></pre></div><p>This gives us our final version of the model:</p>
<pre tabindex="0"><code>dog(x) = 1.29368307 + 7.03633306 * x1 - 0.44795498 * x1**2 + 9.98093332 * x2 - 0.75689575 * x2**2 - 19.00757486 * x3 + 1.52985734 * x3**2
cat(x) = 0.47945896 + 5.30866067 * x1 - 0.39644128 * x1**2 - 1.28704188 * x2 + 0.12634295 * x2**2 - 4.32392606 * x3 + 0.43081918 * x3**2
</code></pre><p>Adding curvature to our model eliminates all perceived error, at least within 1e-16. This may seem unbelievable, but when you consider that we only have six input records, it isn&rsquo;t really.</p>
<table>
<thead>
<tr>
<th>respondent</th>
<th style="text-align:right">predicted b1</th>
<th style="text-align:right">b1 error</th>
<th style="text-align:right">predicted b2</th>
<th style="text-align:right">b2 error</th>
</tr>
</thead>
<tbody>
<tr>
<td>Alyssa P Hacker</td>
<td style="text-align:right">9</td>
<td style="text-align:right">0</td>
<td style="text-align:right">9</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td>Ben Bitdiddle</td>
<td style="text-align:right">10</td>
<td style="text-align:right">0</td>
<td style="text-align:right">4</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td>Cy D. Fect</td>
<td style="text-align:right">2</td>
<td style="text-align:right">0</td>
<td style="text-align:right">6</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td>Eva Lu Ator</td>
<td style="text-align:right">4</td>
<td style="text-align:right">0</td>
<td style="text-align:right">6</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td>Lem E. Tweakit</td>
<td style="text-align:right">2</td>
<td style="text-align:right">0</td>
<td style="text-align:right">5</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td>Louis Reasoner</td>
<td style="text-align:right">10</td>
<td style="text-align:right">0</td>
<td style="text-align:right">3</td>
<td style="text-align:right">0</td>
</tr>
<tr>
<td>Total error:</td>
<td style="text-align:right"></td>
<td style="text-align:right">0</td>
<td style="text-align:right"></td>
<td style="text-align:right">0</td>
</tr>
</tbody>
</table>
<p>It should be fairly obvious how one can take this and extrapolate to much larger models. I hope this is useful and that least squares becomes an important part of your lives.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>üó≥ Off the Cuff Voter Fraud Detection</title>
      <link>https://ryanjoneil.github.io/posts/2010-11-30-off-the-cuff-voter-fraud-detection/</link>
      <pubDate>Tue, 30 Nov 2010 00:00:00 +0000</pubDate>
      
      <guid>https://ryanjoneil.github.io/posts/2010-11-30-off-the-cuff-voter-fraud-detection/</guid>
      <description>Using the exponential distribution to interpret votes in a web survey</description>
      <content:encoded><![CDATA[<p>Consider this scenario: You run a contest that accepts votes from the general Internet population. In order to encourage user engagement, you record any and all votes into a database over several days, storing nothing more than the competitor voted for, when each vote is cast, and a cookie set on the voter&rsquo;s computer along with their apparent IP addresses. If a voter already has a recorded cookie set they are denied subsequent votes. This way you can avoid requiring site registration, a huge turnoff for your users. Simple enough.</p>
<p>Unfortunately, some of the competitors are wily and attached to the idea of winning. They go so far as programming or hiring bots to cast thousands of votes for them. Your manager wants to know which votes are real and which ones are fake Right Now. Given very limited time, and ignoring actions that you <em>could</em> have taken to avoid the problem, how can you tell apart sets of good votes from those that shouldn&rsquo;t be counted?</p>
<p>One quick-and-dirty option involves comparing histograms of <a href="http://www.ehow.com/how_5417319_calculate-interarrival-time.html">interarrival times</a> for sets of votes. Say you&rsquo;re concerned that all the votes during a particular period of time or from a given IP address might be fraudulent. Put all the vote times you&rsquo;re concerned about into a list, sort them, and compute their differences:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># times is a list of datetime instances from vote records</span>
</span></span><span style="display:flex;"><span>times<span style="color:#f92672">.</span>sort(reversed<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>interarrivals <span style="color:#f92672">=</span> [y<span style="color:#f92672">-</span>x <span style="color:#66d9ef">for</span> x, y <span style="color:#f92672">in</span> zip(times, times[<span style="color:#ae81ff">1</span>:]]
</span></span></code></pre></div><p>Now use matplotlib to <a href="https://matplotlib.org/2.0.2/users/pyplot_tutorial.html#working-with-text">display a histogram</a> of these. Votes that occur naturally are likely to resemble an <a href="http://en.wikipedia.org/wiki/Exponential_distribution">exponential distribution</a> in their interarrival times. For instance, here are interarrival times for all votes received in a contest:</p>
<p><img loading="lazy" src="/files/2010-11-30-off-the-cuff-voter-fraud-detection/all-votes.png" alt="Interarrival times for all submissions"  />
</p>
<p>This subset of votes is clearly fraudulent, due to the near determinism of their interarrival times. This is most likely caused by the voting bot not taking random sleep intervals during voting. It casts a vote, receives a response, clears its cookies, and repeats:</p>
<p><img loading="lazy" src="/files/2010-11-30-off-the-cuff-voter-fraud-detection/fraud-plot.png" alt="Interarrival times for clearly fraudulent votes"  />
</p>
<p>These votes, on the other hand, are most likely legitimate. They exhibit a nice <!-- raw HTML omitted -->Erlang<!-- raw HTML omitted --> shape and appear to have natural interarrival times that one would expect:</p>
<p><img loading="lazy" src="/files/2010-11-30-off-the-cuff-voter-fraud-detection/not-fraud.png" alt="Proper-looking interarrival times"  />
</p>
<p>Of course this method is woefully inadequate for rigorous detection of voting fraud. Ideally one would find a method to compute the probability that a set of votes is generated by a bot. This is enough to inform quick, ad hoc decisions though.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>üßê Data Fitting 1 - Linear Data Fitting</title>
      <link>https://ryanjoneil.github.io/posts/2010-11-23-data-fitting-1-linear-data-fitting/</link>
      <pubDate>Tue, 23 Nov 2010 00:00:00 +0000</pubDate>
      
      <guid>https://ryanjoneil.github.io/posts/2010-11-23-data-fitting-1-linear-data-fitting/</guid>
      <description>An introduction to data fitting and classification using linear optimization in Python</description>
      <content:encoded><![CDATA[<p><em>Note: This post was updated to work with Python 3 and <a href="https://github.com/scipopt/PySCIPOpt">PySCIPOpt</a>. The original version used Python 2 and <a href="https://pythonhosted.org/python-zibopt/">python-zibopt</a>.</em></p>
<p>Data fitting is one of those tasks that everyone should have at least some exposure to. Certainly developers and analysts will benefit from a working knowledge of its fundamentals and their implementations. However, in my own reading I&rsquo;ve found it difficult to locate good examples that are simple enough to pick up quickly and come with accompanying source code.</p>
<p>This article commences an ongoing series introducing basic data fitting techniques. With any luck they won&rsquo;t be overly complex, while still being useful enough to get the point across with a real example and real data. We&rsquo;ll start with a binary classification problem: presented with a series of records, each containing a set number of input values describing it, determine whether or not each record exhibits some property.</p>
<h2 id="model">Model</h2>
<p>We&rsquo;ll use the <code>cancer1.dt</code> data from the <code>proben1</code> set of test cases, which you can download <a href="/files/2010-11-23-data-fitting-1-linear-data-fitting/cancer1.dt">here</a>. Each record starts with 9 data points containing physical characteristics of a tumor. The second to last data point contains 1 if a tumor is benign and 0 if it is malignant. We seek to find a linear function we can run on an arbitrary record that will return a value greater than zero if that record&rsquo;s tumor is predicted to be benign and less than zero if it is predicted to be malignant. We will train our linear model on the first 350 records, and test it for accuracy on the remaining rows.</p>
<p>This is similar to the data fitting problem found in <a href="https://www.thriftbooks.com/w/linear-programming-series-of-books-in-the-mathematical-sciences_vasek-chvatal/249798/#edition=2416723&amp;idiq=15706498">Chvatal</a>. Our inputs consist of a matrix of observed data, $A$, and a vector of classifications, $b$. In order to classify a record, we require another vector $x$ such that the dot product of $x$ and that record will be either greater or less than zero depending on its predicted classification.</p>
<p>A couple points to note before we start:</p>
<ul>
<li>
<p>Most observed data are noisy. This means it may be impossible to locate a hyperplane that cleanly separates given records of one type from another. In this case, we must resort to finding a function that minimizes our predictive error. For the purposes of this example, we&rsquo;ll minimize the sum of the absolute values of the observed and predicted values. That is, we seek $x$ such that we find $min \sum_i{|a_i^T x-b_i|}$.</p>
</li>
<li>
<p>The <a href="https://www.purplemath.com/modules/strtlneq.htm">slope-intercept</a> form of a line, $f(x)=m^T x+b$, contains an offset. It should be obvious that this is necessary in our model so that our function isn&rsquo;t required to pass through the origin. Thus, we&rsquo;ll be adding an extra variable with the coefficient of 1 to represent our offset value.</p>
</li>
<li>
<p>In order to model this, we use two linear constraints for each absolute value. We minimize the sum of these. Our Linear Programming model thus looks like:</p>
</li>
</ul>
<p>$$
\begin{align*}
\min\quad       &amp; z = x_0 + \sum_i{v_i}\\
\text{s.t.}\quad&amp; v_i \geq x_0 + a_i^\intercal x - 1    &amp;\quad\forall&amp;\quad\text{benign tumors}\\
&amp; v_i \geq 1 - x_0 - a_i^\intercal x    &amp;\quad\forall&amp;\quad\text{benign tumors}\\
&amp; v_i \geq x_0 + a_i^\intercal x - (-1) &amp;\quad\forall&amp;\quad\text{malignant tumors}\\
&amp; v_i \geq -1 - x_0 - a_i^\intercal x   &amp;\quad\forall&amp;\quad\text{malignant tumors}
\end{align*}
$$</p>
<h2 id="code">Code</h2>
<p>In order to do this in Python, we use <a href="https://www.scipopt.org/">SCIP</a> and <a href="https://soplex.zib.de/">SoPlex</a>. We start by setting constants for benign and malignant outputs and providing a function to read in the training and testing data sets.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Preferred output values for tumor categories</span>
</span></span><span style="display:flex;"><span>BENIGN <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>MALIGNANT <span style="color:#f92672">=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">read_proben1_cancer_data</span>(filename, train_size):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;&#39;&#39;Loads a proben1 cancer file into train &amp; test sets&#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Number of input data points per record</span>
</span></span><span style="display:flex;"><span>    DATA_POINTS <span style="color:#f92672">=</span> <span style="color:#ae81ff">9</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    train_data <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    test_data <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> open(filename) <span style="color:#66d9ef">as</span> infile:
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Read in the first train_size lines to a training data list, and the</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># others to testing data. This allows us to test how general our model</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># is on something other than the input data.</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> line <span style="color:#f92672">in</span> infile<span style="color:#f92672">.</span>readlines()[<span style="color:#ae81ff">7</span>:]: <span style="color:#75715e"># skip header</span>
</span></span><span style="display:flex;"><span>            line <span style="color:#f92672">=</span> line<span style="color:#f92672">.</span>split()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Records = offset (x0) + remaining data points</span>
</span></span><span style="display:flex;"><span>            input <span style="color:#f92672">=</span> [float(x) <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> line[:DATA_POINTS]]
</span></span><span style="display:flex;"><span>            output <span style="color:#f92672">=</span> BENIGN <span style="color:#66d9ef">if</span> line[<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>] <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;1&#39;</span> <span style="color:#66d9ef">else</span> MALIGNANT
</span></span><span style="display:flex;"><span>            record <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#39;input&#39;</span>: input, <span style="color:#e6db74">&#39;output&#39;</span>: output}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Determine what data set to put this in</span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> len(train_data) <span style="color:#f92672">&gt;=</span> train_size:
</span></span><span style="display:flex;"><span>                test_data<span style="color:#f92672">.</span>append(record)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>                train_data<span style="color:#f92672">.</span>append(record)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> train_data, test_data
</span></span></code></pre></div><p>The next function implements the LP model described above using SoPlex and SCIP. It minimizes the sum of residuals for each training record. This amounts to summing the absolute value of the difference between predicted and observed output data. The following function takes in input and observed output data and returns a list of coefficients. Our resulting model consists of taking the <a href="https://en.wikipedia.org/wiki/Dot_product">dot product</a> of an input record and these coefficients. If the result is greater than or equal to zero, that record is predicted to be a benign tumor, otherwise it is predicted to be malignant.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> pyscipopt <span style="color:#f92672">import</span> Model
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">train_linear_model</span>(train_data):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;&#39;&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Accepts a set of input training data with known output
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    values.  Returns a list of coefficients to apply to
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    arbitrary records for purposes of binary categorization.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Make sure we have at least one training record.</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">assert</span> len(train_data) <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>    num_variables <span style="color:#f92672">=</span> len(train_data[<span style="color:#ae81ff">0</span>][<span style="color:#e6db74">&#39;input&#39;</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Variables are coefficients in front of the data points. It is important</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># that these be unrestricted in sign so they can take negative values.</span>
</span></span><span style="display:flex;"><span>    m <span style="color:#f92672">=</span> Model()
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> [m<span style="color:#f92672">.</span>addVar(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;x</span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>, lb<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>) <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(num_variables)]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Residual for each data row</span>
</span></span><span style="display:flex;"><span>    residuals <span style="color:#f92672">=</span> [m<span style="color:#f92672">.</span>addVar(lb<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>, ub<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>) <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> train_data]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> r, d <span style="color:#f92672">in</span> zip(residuals, train_data):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># r will be the absolute value of the difference between observed and</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># predicted values. We can model absolute values such as r &gt;= |foo| as:</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">#   r &gt;=  foo</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">#   r &gt;= -foo</span>
</span></span><span style="display:flex;"><span>        m<span style="color:#f92672">.</span>addCons(sum(x <span style="color:#f92672">*</span> y <span style="color:#66d9ef">for</span> x, y <span style="color:#f92672">in</span> zip(x, d[<span style="color:#e6db74">&#39;input&#39;</span>])) <span style="color:#f92672">+</span> r <span style="color:#f92672">&gt;=</span> d[<span style="color:#e6db74">&#39;output&#39;</span>])
</span></span><span style="display:flex;"><span>        m<span style="color:#f92672">.</span>addCons(sum(x <span style="color:#f92672">*</span> y <span style="color:#66d9ef">for</span> x, y <span style="color:#f92672">in</span> zip(x, d[<span style="color:#e6db74">&#39;input&#39;</span>])) <span style="color:#f92672">-</span> r <span style="color:#f92672">&lt;=</span> d[<span style="color:#e6db74">&#39;output&#39;</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Find and return coefficients that min sum of residuals.</span>
</span></span><span style="display:flex;"><span>    m<span style="color:#f92672">.</span>setObjective(sum(residuals))
</span></span><span style="display:flex;"><span>    m<span style="color:#f92672">.</span>setMinimize()
</span></span><span style="display:flex;"><span>    m<span style="color:#f92672">.</span>optimize()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    solution <span style="color:#f92672">=</span> m<span style="color:#f92672">.</span>getBestSol()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> [solution[xi] <span style="color:#66d9ef">for</span> xi <span style="color:#f92672">in</span> x]
</span></span></code></pre></div><p>We also provide a convenience function for counting the number of correct predictions by our resulting model against either the test or training data sets.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">count_correct</span>(data_set, coefficients):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;&#39;&#39;Returns the number of correct predictions.&#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>    correct <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> d <span style="color:#f92672">in</span> data_set:
</span></span><span style="display:flex;"><span>        result <span style="color:#f92672">=</span> sum(x<span style="color:#f92672">*</span>y <span style="color:#66d9ef">for</span> x, y <span style="color:#f92672">in</span> zip(coefficients, d[<span style="color:#e6db74">&#39;input&#39;</span>]))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Do we predict the same as the output?</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> (result <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">0</span>) <span style="color:#f92672">==</span> (d[<span style="color:#e6db74">&#39;output&#39;</span>] <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">0</span>):
</span></span><span style="display:flex;"><span>            correct <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> correct
</span></span></code></pre></div><p>Finally we write a main method to read in the data, build our linear model, and test its efficacy.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> pprint <span style="color:#f92672">import</span> pprint
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;__main__&#39;</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Specs for this input file</span>
</span></span><span style="display:flex;"><span>    INPUT_FILE_NAME <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;cancer1.dt&#39;</span>
</span></span><span style="display:flex;"><span>    TRAIN_SIZE <span style="color:#f92672">=</span> <span style="color:#ae81ff">350</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    train_data, test_data <span style="color:#f92672">=</span> read_proben1_cancer_data(
</span></span><span style="display:flex;"><span>        INPUT_FILE_NAME,
</span></span><span style="display:flex;"><span>        TRAIN_SIZE
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Add the offset variable to each of our data records</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> data_set <span style="color:#f92672">in</span> [train_data, test_data]:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> row <span style="color:#f92672">in</span> data_set:
</span></span><span style="display:flex;"><span>            row[<span style="color:#e6db74">&#39;input&#39;</span>] <span style="color:#f92672">=</span> [<span style="color:#ae81ff">1</span>] <span style="color:#f92672">+</span> row[<span style="color:#e6db74">&#39;input&#39;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    coefficients <span style="color:#f92672">=</span> train_linear_model(train_data)
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">&#39;coefficients:&#39;</span>)
</span></span><span style="display:flex;"><span>    pprint(coefficients)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Print % of correct predictions for each data set</span>
</span></span><span style="display:flex;"><span>    correct <span style="color:#f92672">=</span> count_correct(train_data, coefficients)
</span></span><span style="display:flex;"><span>    print(
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#39;</span><span style="color:#e6db74">%s</span><span style="color:#e6db74"> / </span><span style="color:#e6db74">%s</span><span style="color:#e6db74"> = </span><span style="color:#e6db74">%.02f%%</span><span style="color:#e6db74"> correct on training set&#39;</span> <span style="color:#f92672">%</span> (
</span></span><span style="display:flex;"><span>            correct, len(train_data),
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">100</span> <span style="color:#f92672">*</span> float(correct) <span style="color:#f92672">/</span> len(train_data)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    correct <span style="color:#f92672">=</span> count_correct(test_data, coefficients)
</span></span><span style="display:flex;"><span>    print(
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#39;</span><span style="color:#e6db74">%s</span><span style="color:#e6db74"> / </span><span style="color:#e6db74">%s</span><span style="color:#e6db74"> = </span><span style="color:#e6db74">%.02f%%</span><span style="color:#e6db74"> correct on testing set&#39;</span> <span style="color:#f92672">%</span> (
</span></span><span style="display:flex;"><span>            correct, len(test_data),
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">100</span> <span style="color:#f92672">*</span> float(correct) <span style="color:#f92672">/</span> len(test_data)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>    )
</span></span></code></pre></div><h2 id="results">Results</h2>
<p>The result of running this model against the <code>cancer1.dt</code> data set is:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-txt" data-lang="txt"><span style="display:flex;"><span>coefficients:
</span></span><span style="display:flex;"><span>[1.4072882449702786,
</span></span><span style="display:flex;"><span> -0.14014055927954652,
</span></span><span style="display:flex;"><span> -0.6239513714263405,
</span></span><span style="display:flex;"><span> -0.26727681774258882,
</span></span><span style="display:flex;"><span> 0.067107753841131157,
</span></span><span style="display:flex;"><span> -0.28300216102808429,
</span></span><span style="display:flex;"><span> -1.0355594670918404,
</span></span><span style="display:flex;"><span> -0.22774451038152174,
</span></span><span style="display:flex;"><span> -0.69871243677663608,
</span></span><span style="display:flex;"><span> -0.072575089848659444]
</span></span><span style="display:flex;"><span>328 / 350 = 93.71% correct on training set
</span></span><span style="display:flex;"><span>336 / 349 = 96.28% correct on testing set
</span></span></code></pre></div><p>The accuracy is pretty good here against the both the training and testing sets, so this particular model generalizes well.  This is about the simplest model we can implement for data fitting, and we&rsquo;ll get to more complicated ones later, but it&rsquo;s nice to see we can do so well so quickly.  The coefficients correspond to using a function of this form, rounding off to three decimal places:</p>
<p>$$
\begin{align*}
f(x) =\ &amp;1.407 - 0.140 x_1 - 0.624 x_2 - 0.267 x_3 + 0.067 x_4 - \\
&amp;0.283 x_5 - 1.037 x_6 - 0.228 x_7 - 0.699 x_8 - 0.073 x_9
\end{align*}
$$</p>
<h2 id="resources">Resources</h2>
<ul>
<li><a href="/files/2010-11-23-data-fitting-1-linear-data-fitting/cancer1.dt"><code>cancer1.dt</code></a> data file from <code>proben1</code></li>
<li>Full <a href="/files/2010-11-23-data-fitting-1-linear-data-fitting/fit-linear.py">source listing</a></li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>‚ö°Ô∏è On the Beauty of Power Sets</title>
      <link>https://ryanjoneil.github.io/posts/2009-02-27-on-the-beauty-of-power-sets/</link>
      <pubDate>Fri, 27 Feb 2009 00:00:00 +0000</pubDate>
      
      <guid>https://ryanjoneil.github.io/posts/2009-02-27-on-the-beauty-of-power-sets/</guid>
      <description>Using power sets in algebraic modeling languages for formulating the Traveling Salesman Problem</description>
      <content:encoded><![CDATA[<p>One of the difficulties we encounter in solving the <a href="https://www.math.uwaterloo.ca/tsp/">Traveling Salesman Problem</a> (TSP) is that, for even a small numer of cities, a complete description of the problem requires a factorial number of constraints. This is apparent in the standard formulation used to teach the TSP to OR students. Consider a set of $n$ cities with the distance from city $i$ to city $j$ denoted $d_{ij}$.  We attempt to minimize the total distance of a tour entering and leaving each city exactly once. $x_{ij} = 1$ if the edge from city $i$ to city $j$ is included in the tour, $0$ otherwise:</p>
<p>$$
\small
\begin{align*}
\min\quad       &amp; z = \sum_i \sum_{j\ne i} d_{ij} x_{ij}\\
\text{s.t.}\quad&amp; \sum_{j\ne i} x_{ij} = 1 &amp;\quad\forall&amp;\ i &amp; \text{leave each city once}\\
&amp; \sum_{i\ne j} x_{ij} = 1 &amp;\quad\forall&amp;\ j &amp; \text{enter each city once}\\
&amp; x_{ij} \in \{0,1\}       &amp;\quad\forall&amp;\ i,j
\end{align*}
$$</p>
<p>This appears like a reasonable formulation until we solve it and see that our solution contains disconnected subtours. Suppose we have four cities, labeled $A$ through $D$. Connecting $A$ to $B$, $B$ to $A$, $C$ to $D$ and $D$ to $C$ provides a feasible solution to our formulation, but does not constitute a cycle. Here is a more concrete example of two disconnected subtours $\{(1,5),(5,1)\}$ and $\{(2,3),(3,4),(4,2)\}$ over five cities:</p>
<pre tabindex="0"><code>ampl: display x;
x [*,*]
:   1   2   3   4   5    :=
1   0   0   0   0   1
2   0   0   1   0   0
3   0   0   0   1   0
4   0   1   0   0   0
5   1   0   0   0   0
;
</code></pre><p>Realizing we just solved the <a href="https://en.wikipedia.org/wiki/Assignment_problem">Assignment Problem</a>, we now add subtour elimination constraints. These require that any proper, non-null subset of our $n$ cities is connected by at most $n-1$ active edges:</p>
<p>$$
\sum_{i \in S} \sum_{j \in S} x_{ij} \leq |S|-1 \quad\forall\ S \subset {1, &hellip;, n}, S \ne O
$$</p>
<p>Indexing subtour elimination constraints over a <a href="https://en.wikipedia.org/wiki/Power_set">power set</a> of the cities completes the formulation. However, this requires an additional $\sum_{k=2}^{n-1} \begin{pmatrix} n \\ k \end{pmatrix}$ rows tacked on the end of our matrix and is clearly infeasible for large $n$. The most current computers can handle using this approach <a href="http://zimpl.zib.de/download/zimpl.pdf">is around 19 cities</a>. It remains an instructive tool for understanding the <a href="https://en.wikipedia.org/wiki/Combinatorial_explosion">combinatorial explosion</a> that occurs in problems like TSP and is worth translating into a modeling language. So how does one model it on a computer?</p>
<p>Unfortunately, <a href="https://ampl.com/">AMPL</a>, the gold standard in mathematical modeling languages, is unable to index over sets. Creating a power set in AMPL requires going through a few contortions.  The following code demonstrates power and index sets over four cities:</p>
<pre tabindex="0"><code>set cities := 1 .. 4 ordered;

param n := card(cities);
set indices := 0 .. (2^n - 1);
set power {i in indices} := {c in cities: (i div 2^(ord(c) - 1)) mod 2 = 1};

display cities;
display n;
display indices;
display power;
</code></pre><p>This yields the following output:</p>
<pre tabindex="0"><code>set cities := 1 2 3 4;

n = 4

set indices := 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15;

set power[0] := ; # empty
set power[1] := 1;
set power[2] := 2;
set power[3] := 1 2;
set power[4] := 3;
set power[5] := 1 3;
set power[6] := 2 3;
set power[7] := 1 2 3;
set power[8] := 4;
set power[9] := 1 4;
set power[10] := 2 4;
set power[11] := 1 2 4;
set power[12] := 3 4;
set power[13] := 1 3 4;
set power[14] := 2 3 4;
set power[15] := 1 2 3 4;
</code></pre><p>Note how the index set contains an index for each row in our power set. We can now generate the subtour elimination constraints:</p>
<pre tabindex="0"><code>var x {cities cross cities} binary;
s.t. subtours {i in indices: card(power[i]) &gt; 1 and card(power[i]) &lt; card(cities)}:
sum {(c,k) in power[i] cross power[i]: k != c} x[c,k] &lt;= card(power[i]) - 1;

expand subtours;

subject to subtours[3]:  x[1,2] + x[2,1] &lt;= 1;
subject to subtours[5]:  x[1,3] + x[3,1] &lt;= 1;
subject to subtours[6]:  x[2,3] + x[3,2] &lt;= 1;
subject to subtours[7]:  x[1,2] + x[1,3] + x[2,1] + x[2,3] + x[3,1] + x[3,2] &lt;= 2;
subject to subtours[9]:  x[1,4] + x[4,1] &lt;= 1;
subject to subtours[10]: x[2,4] + x[4,2] &lt;= 1;
subject to subtours[11]: x[1,2] + x[1,4] + x[2,1] + x[2,4] + x[4,1] + x[4,2] &lt;= 2;
subject to subtours[12]: x[3,4] + x[4,3] &lt;= 1;
subject to subtours[13]: x[1,3] + x[1,4] + x[3,1] + x[3,4] + x[4,1] + x[4,3] &lt;= 2;
subject to subtours[14]: x[2,3] + x[2,4] + x[3,2] + x[3,4] + x[4,2] + x[4,3] &lt;= 2;
</code></pre><p>While this does work, the code for generating the power set looks like <a href="https://en.wikipedia.org/wiki/Voodoo_programming">voodoo</a>. Understanding it required piece-by-piece decomposition, an exercise I suggest you go through yourself if you have a copy of AMPL and 15 minutes to spare:</p>
<pre tabindex="0"><code>set foo {c in cities} := {ord(c)};
set bar {c in cities} := {2^(ord(c) - 1)};
set baz {i in indices} := {c in cities: i div 2^(ord(c) - 1)};
set qux {i in indices} := {c in cities: (i div 2^(ord(c) - 1)) mod 2 = 1};

display foo;
display bar;
display baz;
display qux;
</code></pre><p>This may be an instance where open source leads commercial software. The good folks who produce the <a href="https://scipopt.org/">SCIP Optimization Suite</a> provide an AMPL-like language called <a href="https://zimpl.zib.de/">ZIMPL</a> with a few additional useful features. One of these is power sets. Compared to the code above, doesn&rsquo;t this look refreshing?</p>
<pre tabindex="0"><code>set cities := {1 to 4};

set power[] := powerset(cities);
set indices := indexset(power);
</code></pre>]]></content:encoded>
    </item>
    
    <item>
      <title>üìê Uncapacitated Lot Sizing</title>
      <link>https://ryanjoneil.github.io/posts/2009-02-20-uncapacitated-lot-sizing/</link>
      <pubDate>Fri, 20 Feb 2009 00:00:00 +0000</pubDate>
      
      <guid>https://ryanjoneil.github.io/posts/2009-02-20-uncapacitated-lot-sizing/</guid>
      <description>Formulation and aspects of the Uncapacitated Lot Sizing problem in Integer Programming</description>
      <content:encoded><![CDATA[<p>Uncapacitated Lot Sizing (ULS) is a classic <a href="http://en.wikipedia.org/wiki/Operations_research">OR</a> problem that seeks to minimize the cost of satisfying known demand for a product over time.  Demand is subject to varying costs for production, set-up, and storage of the product.  Technically, it is a mixed binary integer linear program &ndash; the key point separating it from the world of <a href="http://en.wikipedia.org/wiki/Linear_programming">linear optimization</a> being that production cannot occur during any period without paying that period&rsquo;s fixed costs for set-up.  Thus it has linear nonnegative variables for production and storage amounts during each period, and a binary variable for each period that determines whether or not production can actually occur.</p>
<p>For $n$ periods with per-period fixed set-up cost $f_t$, unit production cost $p_t$, unit storage cost $h_t$,and demand $d_t$, we define decision variables related to production and storage quantities:</p>
<p>$$
\small
\begin{align*}
x_t &amp;= \text{units produced in period}\ t\\
s_t &amp;= \text{stock at the end of period}\ t\\
y_t &amp;= 1\ \text{if production occurs in period}\ t, 0\ \text{otherwise}
\end{align*}
$$</p>
<p>One can minimize overall cost for satisfying all demand on time using the following model per <a href="http://www.amazon.com/Integer-Programming-Laurence-Wolsey/dp/0471283665/">Wolsey (1998)</a>, defined slightly differently here:</p>
<p>$$
\small
\begin{align*}
\min\quad       &amp; z = \sum_t{p_t x_t} + \sum_t{h_t s_t} + \sum_t{f_t y_t}\\
\text{s.t.}\quad&amp; s_1 = d_1 + s_1\\
&amp; s_{t-1} + x_t = d_t + s_t &amp;\quad\forall&amp;\ t &gt; 1\\
&amp; x_t \leq M y_t            &amp;\quad\forall&amp;\ t\\
&amp; s_t, x_t \geq 0           &amp;\quad\forall&amp;\ t\\
&amp; y_t \in {0,1}           &amp;\quad\forall&amp;\ t
\end{align*}
$$</p>
<p>According to Wolsey, page 11, given that $s_t = \sum_{i=1}^t (x_i - d_i)$ and defining new constants $K = \sum_{t=1}^n h_t(\sum_{i=1}^t d_i)$ and $c_t = p_t + \sum_{i=t}^n h_i$, the objective function can be rewritten as $z = \sum_t c_t x_t + \sum _t f_t y_t - K$.  The book lacks a proof of this and it seems a bit non-obvious, so I attempt an explanation in somewhat painstaking detail here.</p>
<p>$$
\small
\begin{align*}
&amp;\text{Proof}:\\
&amp; &amp; \sum_t p_t x_t + \sum_t h_t s_t + \sum_t f_t y_t &amp;= \sum_t c_t x_t + \sum _t f_t y_t - K\\
&amp;\text{1. Remove} \sum_t f_t y_t:\\
&amp; &amp; \sum_t p_t x_t + \sum_t h_t s_t &amp;= \sum_t c_t x_t - K\\
&amp;\text{2. Expand}\ K\ \text{and}\ c_t:\\
&amp; &amp; \sum_t p_t x_t + \sum_t h_t s_t &amp;= \sum_t (p_t + \sum_{i=t}^n h_i) x_t - \sum_t h_t (\sum_{i=1}^t d_i)\\
&amp;\text{3. Remove}\ \sum_t p_t x_t:\\
&amp; &amp; \sum_t h_t s_t &amp;= \sum_t x_t (\sum_{i=t}^n h_i) - \sum_t h_t (\sum_{i=1}^t d_i)\\
&amp;\text{4. Expand}\ s_t:\\
&amp; &amp; \sum_t h_t (\sum_{i=1}^t x_i) - \sum_t h_t (\sum_{i=1}^t d_i) &amp;= \sum_t x_t (\sum_{i=t}^n h_i) - \sum_t h_t (\sum_{i=1}^t d_i)\\
&amp;\text{5. Remove}\ \sum_t h_t (\sum_{i=1}^t d_i):\\
&amp; &amp; \sum_t h_t (\sum_{i=1}^t x_i) &amp;= \sum_t x_t (\sum_{i=t}^n h_i)
\end{align*}
$$</p>
<p>The result from step 5 becomes obvious upon expanding its left and right-hand terms:</p>
<p>$$
h_1 x_1 + h_2 (x_1 + x_2) + \cdots + h_n (x_1 + \cdots + x_n) =\\
x_1 (h_1 + \cdots + h_n) + x2 (h_2 + \cdots + h_n) + \cdots + x_n h_n
$$</p>
<p>In matrix notation with $h$ and $x$ as column vectors in $\bf R^n$ and $L$ and $U$ being $n \times n$ lower and upper triangular identity matrices, respectively, this can be written as:</p>
<p>$$
\small
\begin{pmatrix}
h_1 \cdots h_n
\end{pmatrix}
\begin{pmatrix}
1 \cdots 0 \\
\vdots \ddots \vdots \\
1 \cdots 1
\end{pmatrix}
\begin{pmatrix}
x_1 \\
\vdots \\
x_n
\end{pmatrix} =
\begin{pmatrix}
x_1 \cdots x_n
\end{pmatrix}
\begin{pmatrix}
1 \cdots 1 \\
\vdots \ddots \vdots \\
0 \cdots 1
\end{pmatrix}
\begin{pmatrix}
h_1 \\
\vdots \\
h_n
\end{pmatrix}
$$</p>
<p>or $h^T L x = x^T U h$.</p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
